[{"title":"Variational Autoencoder","url":"/2020/02/16/Variational-Autoencoder/","content":"(变分自动编码机(VAE)理解和实现(Tensorflow))[https://blog.csdn.net/ppp8300885/article/details/80070723]\n# AE\n自动编码机Auto-Encoder (AE)由两部分encoder和decoder组成，\nencoder输入$x$数据，输出潜在变量z，\ndecoder输入$z$然后输出一个$x'$，目的是让$x'$与$x$的分布尽量一致，\n当两者完全一样时，中间的潜在变量z可以看作是x的一种压缩状态，包含了$x$的全部feature特征，此时监督信号就是原数据x本身。\n# VAE\n变分自动编码机VAE是自动编码机的一种扩展，\n它假设输出的潜在变量$z$**服从一种先验分布**，如高斯分布。\n这样，在训练完模型后，我们可以通过采样这种先验分布得到$z'$，这个$z'$可能是训练过程中没有出现过的，但是我们依然能在解码器中通过这个$z'$获得$x'$，从而得到一些符合原数据$x$分布的新样本，具有“生成“新样本的能力。\nVAE是一种生成模型，它的目标是要得到$p(z∣x)$分布，即`给定输入数据$x$的分布，得到潜在变量$x$的分布`，\n与其他的生成模型一样，它计算的是$x$和$z$的联合概率分布$p(x,z)$（如朴素贝叶斯模型通过计算$\\frac{p(x,z)}{p(x)}=p(z∣x)$），当然它不是直接计算这个联合概率分布，而是借助一些公式变换求解。\n","categories":["Deep Learning"]},{"title":"Exploring Personalized Neural Conversational Models","url":"/2020/02/16/Exploring-Personalized-Neural-Conversational-Models/","content":"Exploring Personalized Neural Conversational Models\n# 0.Abstract\n对多种显著影响预测效果的因素做了评估，如预训练pretraining、嵌入训练embedding training、数据清理data cleaning、多样性重排序diversity-based raranking、评估设置evaluation setting(retrieval or generative evaluation)，等等。\n在更大的数据及上pretraining speaker embedding、bootstrapping word and speaker embedding可以显著提升效果（最高3点perplexity），使用MI提升回复多样性对提升ranking metrics(排名指标?)非常有用。\n# 1.Introduction\n# 2.Related Work\n# 3.Models\n介绍的baseline分为不考虑context/不考虑personalization/二者都不考虑\n## 3.1 Notation\n本文模型训练采用的是`有多个对话者参与`的数据集。\n一场对话$C_i$是一个有序集，其中元素是由回合turn $D_i^j$和该回合的说话者speaker $S_i^j$组成的pair。i.e.$C_i={(D_i^1,S_i^1),(D_i^2,S_i^2),...,(D_i^n,S_i^n)}$。每个回合本身是对应说话者所说的words集合，$D_i^j={w^j_{i1},w^j_{i2},...,w^j_{in}}$。为了简洁，在对话中我们无视索引i.所有我们的语言模型基于每个时间步之前的信息预测下一个词的分布。先前不同模型的区别在于使用该回合之前信息的范围(the extent of information)，而我们的工作是比较这些模型。\n## 3.2 GRU\n## 3.3 Baselines\n本节所有语言模型，等号左边是相同的\n### 3.3.1 Encoder-Decoder\n`不考虑说话人的信息`，是dyadic(二元?)的。也就是说，下一个回复只依赖于当前的对话，独立于其他信息。\n语言模型：\n$$ P(w^j_{t+1}|w^j_i,...,w^j_t,D^{-j},S^{-j},S^j) = P(w^j_{t+1}|w^j_1,...,w^j_t,D^{j-1}) $$\n其中，$D^{-j}$指的是直到j的所有turns的集合。即$D^{-j}={D^1,D^2,...,D^{j-1}}$.Speakers $S^{-j}$同理。\n### 3.3.2 Persona-only\n[Li et al., 2016b]\n引入说话人的信息作为encoder和decoder RNN两边的额外输入。\n$$ P(w^j_{t+1}|w^j_i,...,w^j_t,D^{-j},S^{-j},S^j) = P(w^j_{t+1}|w^j_1,...,w^j_t,D^{j-1},S^{j-1},S^j) $$\n。因此，语言模型依赖于当前和之前回合的说话人，在生成新回复时考虑了人格。\n### 3.3.3 Context-only\nHierarchical Recurrent Encoder-Decoder[Serban et al., 2016b]通过一个每回合更新自己隐藏层，且基于encoder-decoder RNNs的high level,context RNN捕捉上下文线索。但是没有包含任何人格信息。更多细节参考[Serban et al., 2015a].\n$$ P(w^j_{t+1}|w^j_i,...,w^j_t,D^{-j},S^{-j},S^j) = P(w^j_{t+1}|w^j_1,...,w^j_t,D^{-j}) $$\n注意下一个词依赖于所有之前的回合，进而通过历史保留上下文信息。但是在实践中，上下文被截断，只能考虑过去几个回合的信息。\n## 3.4 CoPerHED Model\n**Co**ntext-aware, **Per**sona-based **H**ierarchical **E**ncoder-**D**ecoder:混合了persona-based[Li et al., 2016b]和context-aware[Serban et al., 2016b]的神经交流模型。当前回合和之前回合的说话者信息被作为额外信息输入；使用额外的RNN在每个回合水平上总结信息。\n![](/images/200216-1.png)\n### 3.4.1 Encoder\n第$j^{th}$回合的隐状态$h^j_t$由每个时间步输入一个词和当前speaker标记得到。所有words和speakers的reperesentation盒模型联合学习。\n$$ h^j_t = GRU(h^j_{t-1},[w^j_t,s^j]) $$\n本回合说话者人格$s^j$对给定回合的所有时间步固定。\n### 3.4.2 Context RNN\n会话中每个回合的句子编码表示由context RNN处理。这有助于保留前一轮的相关信息，并作为预测response的上下文。与[Serban et al., 2016b]类似，CoPerHED由一组分层RNN构成。context RNN工作在turn-level，而language RNN（本例中为GRU）工作在every turn的word-level。\n$g_j$表示$j^{th}$回合的context RNN的隐状态。\n$$ g_j = GRU(g_{j-1}, h^j_n) $$\n### 3.4.3 Decoder\n输入：当前词语，当前回合speaker，context RNN的隐状态。$\\hat{h}_t$表示decoder的隐状态：\n$$\\hat{h}_t^{j+1} = GRU(\\hat{h}_{t-1}^{j+1},[w_t,s^{j+1},g_j])$$\n# 4.Datasets\nMovie-Dic\nTV-Series\nSubTle\n# 5.Experiments\n## 5.1 Data Preprocessing\n## 5.2 Training\n参数介绍。\n- 网络：Encoder和Decoder都是两层GRU\n- dropout:0.2\n- Adam optimizer\n- learning rate:0.001(1e-3),10个epoch结束时指数衰减到0.0001(1e-4)，之后保持为常量\n- 梯度剪裁[-5.0, 5.0]\n- 结束条件:hold-out验证集的困惑度饱和\n\n最优效果:\n- word embedding size:300\n- speaker embedding size:50\n- encoder/decoder GRU 隐单元:300\n- context GRU隐单元:50\n## 5.3 Initialization\n### 5.3.1 Bootstrap word embeddings\n使用在大数据集上训练好的word2vec\n### 5.3.2 Bootstrap speaker embeddings\n使用人工特征初始化speaker embedding。对所有speaker，从训练数据中构建BOW词袋特征，并使用PCA降维。\n### 5.3.3 Pre-train on SubTle dataset\n在标注数据集上微调之前，先使用大规模的数据集预训练，所有speaker分配为$<UNS>$，随机指派台词（使用的数据集有剧本）用作预训练的对话。\n### 5.3.4 Model Varients\n## 5.4 Evaluation\nGeneration: 困惑度Perplexity\nRetrieval: Recall@k\n## 5.5 Promoting diversity using MI\n[Li et al., 2016a],打分函数从似然likelihood变成互信息。\n# 6.Results\n# 7.Discussion and Conclusion\n实验技巧：\n1. 在更大的数据集上预训练speaker embedding\n2. bootstrap word and speaker embedding\n\n模型:\npersona-based + context aware,即speaker embedding + context RNN","categories":["Dialogue System"]},{"title":"AI-Powered Text Generation for Harmonious HumanMachine Interaction Current State and Future Directions","url":"/2020/02/15/AI-Powered-Text-Generation-for-Harmonious-HumanMachine-Interaction-Current-State-and-Future-Directions/","content":"https://arxiv.org/ftp/arxiv/papers/1905/1905.01984.pdf\n# IV.A. Personalized Dialogue Systems\n### 入门\n[24]《A persona-based neural conversation model》\nJiwei Li 首次引入\n[20]《Exploring Personalized Neural Conversational Models》\n扩展之前的的模型，同样使用用户特征表示向量。\n与分层循环Encoder-Decoder结构结合，可以更好的捕捉上下文相关信息。\n考虑用户人格特征，生成更多高质量对话内容\n\n## 缺乏带有用户个性的对话数据集\n[30]《Multi-task learning for speaker-role adaptation in neural conversation models》\n该论文应用多任务学习机制。先用少量数据训练回复生成模型(reply generation model)，然后用非对话数据训练autoencoder模型。两个模型的参数通过多任务学习机制共享。\n[38]《Personalizing a dialogue system with transfer reinforcement learning》\n[65]《Personalized response generation via domain adaptation》\n迁移学习。训练大规模通用数据及来生成通用回复，然后使用少量个性对话数据来微调模型。\n\n## 考虑到不同用户性格对回复内容的影响\n[46]《Assigning Personality/Profile to a Chatting Machine for Coherent Conversation》\n用监督机制判断何时在回复生成过程中表达合适的用户画像。\n[28]《Content-Oriented User Modeling for Personalized Response Ranking in Chatbot》\n建立两分支神经网络(two-branch neural network)来自动从用户对话中学习用户画像，随后该DNN用来更深入的学习用户query、reply、profile的融合表示\n\n## task-oriented(之前全是chatbot)\n[18]《Personalization in goal-oriented dialog》\n包含用户个性信息的任务向对话系统数据集\n[31]《Learning personalized end-to-end goal-oriented dialog》\n使用RNN memory network的变种。用 Profile Model编码用户个性信息，Preference Model解决不同用户的同一个query的歧义问题。同时保存用户的对话历史。通过结合同一用户的对话历史和用户个性特征信息，生成对不同用户的个性化回复\n","categories":["Dialogue System"]},{"title":"The Design and Implementation of XiaoIce, an Empathetic Social Chatbot","url":"/2020/02/11/The-Design-and-Implementation-of-XiaoIce-an-Empathetic-Social-Chatbot/","content":"小冰被设计成一个具有情感连接的人工智能伴侣，以满足人类交流、情感和社会归属感的需求。我们再系统设计中同时考虑了智商IQ和情商EQ，将人机社交聊天作为对马尔科夫决策过程的决策\n\n小冰的主要设计目标是成为一个人工智能伴侣，与用户建立长期的情感联系。作为一个开放领域的社交机器人，小冰能够与人类用户建立**长期**联系，这不仅是小冰与早期社交聊天机器人的区别，也是小冰与苹果Siri，亚马逊Alexa，谷歌Assistant和微软小娜Cortana等人工智能会话个人助理的区别。\n\n# 2.设计原则 Design Principle\n智商->完成特定任务\n情商->满足用户情感需求\n个性->独特\nIQ和EQ的结合是小冰系统设计的核心\n\n## 2.1 IQ+EQ+个性Personality\n- IQ:知识和记忆建模、图像和自然语言理解、推理、生成和预测。这些是发展对话技能的基础，可以满足用户的特定需求，帮助用户完成特定的任务。\n- EQ:移情empathy与社交技能social skills\n- Personality：人格是指一个人独特性格的行为、认识和情感模式的特征集合。表现出一致的个性。\n\n## 2.2 每次会话的对话轮数CPS(Conversation-turns Per Session)\nCPS越大，社交聊天机器人参与度越好\n注意要优化的是**长期**的参与度\n任务完成技能可以有效完成任务，减少短期CPS，但这更适用于**个人助理**，而非**情感陪伴**\n\n## 2.3 社交聊天->分层决策过程(Hierarchical Decision-Making)\n将人机社交聊天作为一个决策过程，优化小冰的长期用户参与，用期望CPS衡量。e.g.促进对话模式多样性来维持用户兴趣，每个会话模式由处理特定类型会话段的技能管理。\n分层：顶层-总体管理对话，选择不同技能处理不同聊天模式\n底层：具体技能，选择“原语”(primitive)动作(回应)来生成对话段或完成任务。\n\nenvironment:human users\nstate:dialog\n依据分层对话策略可选择:skill(option)或response(primary action)\n--->reward(来自user response)，观测到新的状态，继续该循环，直到对话结束。\nchatbot的设计目标：寻找最优策略和技能，最大化期望的CPS(reward)\n\n# 3.系统架构 System Architecture\n三层：用户体验层<->对话引擎层<->数据层\n\n- 用户体验：两种与用户交流的模式：全双工（基于语音流，用户和小冰可以同时说话）、轮流（基于消息，轮流交谈）。包括处理IO（用户输入、小冰响应）的组件，e.g.语音识别与合成、图像理解、文本规范化。\n- 对话引擎：对话管理器（跟踪对话状态，使用对话策略选择对话技能或核心聊天，来生成响应）；移情计算（理解用户输入的内容content与移情方面empathetic aspects，如情感、意图、对主题的观点、背景和兴趣）。这反映了小冰的EQ，展示小冰的社交技巧，确保产生与小冰人格相匹配的人际反应。智商体现在特殊技能与核心对话。\n- 数据：文字-文字对或文字-图像对的会话数据、用于core chat和技能的非会话数据与知识图谱、用户和小冰的profile\n\n# 4.对话引擎实现 Implementation of Conversation Engine\n## 4.1 Dialogue Manager对话管理器\n对话系统的中心控制器center controller。\n### 4.1.1 Global State Tracker全局状态跟踪\n维护一个working memory来追踪对话状态。在每个session的起初为空，然后保存每轮的用户和小冰的文本对话串、从文本中提取的实体和情感标签。working memery中的信息被编码为对话状态向量$s$\n### 4.1.2 Dialogue Policy对话策略\nHIERARCHICAL分层：顶层管理整个对话，每轮选择core chat或激活技能；低层每个技能一个，用于管理自己的对话段。通过迭代试错优化长期用户参与。\n1. 用户输入是文本：core chat, topic manager\n2. 用户输入是图片或视频：image commenting skill\n3. 特定输入和对话上下文触发task completion, deep engagement, content creation.\n### 4.1.3 主题管理 Topic Manager\n由两部分组成\n- 分类器classifier：决定是否切换。以下两种情况需要切换\n  - core chat无法生成任何有效响应候选，只能使用编辑响应editorial response\n  - 生成的响应只是简单重复用户输入或不含新信息\n  - 用户输入变的乏味e.g. \"OK\",\"I see\",\"go on\"\n- 主题检索引擎topic retrieval engine：推荐新主题。依据以下几点，由boosted tree ranker生成\n  - contextual relevance上下文相关度\n  - freshness新鲜度\n  - 个人兴趣\n  - popularity流行度/受欢迎度\n  - acceptance rate接受度\n\n## 4.2 Empathetic Computing移情计算\n反映小冰的EQ，建模人机对话中的情感方面。\n给定用户输入$Q$,\n考虑其上下文$C$并重写为上下文版本$Q_c$,\n用查询移情向量query empathy vector $e_Q$编码用户感受和对话中的状态，\n用回复移情向量response empathy vector $e_R$指定回复$R$中的移情方面。\n\n对话状态向量$s=(Q_c, C, e_Q, e_R)$：\n- 表示移情计算模块的输出\n- Dialogue Policy部分的输入，用于选择技能；\n- 作为激活技能（如Core Chat）部分的输入，用于生成符合小冰人格的人际回复。\n\n移情计算模块三个组件：\n1. Contextual Query Understanding(CQU)上下文查询理解\n   CQU根据以下步骤，使用$C$中的上下文信息重写$Q$和$Q_c$\n   - 命名实体识别：标记$Q$中提到的entity mention，链指到储存在state tracker中的working memory中的对应实体，或在working memory中保存新的实体。\n   - 共指消解：将代词替换对对应实体名称\n   - 语句补全：若$Q$不是完整的句子，使用上下文信息$C$补全\n2. User Understanding用户理解\n    基于$Q_c$和$C$生成查询移情向量$e_Q$。$e_Q$中包含一个键值对列表，表示用户意图user intents、情感emotions、主题topics、观点opinion、用户性格user persona。这些键值对使用一组分类器生成：\n    - 主题检测 标注用户是否关注同一主题，还是引入了新主题。主题集是预先编纂好的的。\n    - 意图检测 标注$Q_c$，使用一种对话动作e.g.问候greet、请求request、通知inform等\n    - 情感分析监测用户感情e.g.happy sad angry neural、以及感情如何演化e.g.happy->sad\n    - 观点检测：用户对主题的反应，e.g.positive negative neural\n    - 如果user id可用，根据用户画像profile生成用户人格向量，包含在$e_Q$中。e.g.性别、年龄、兴趣、职业、性格等\n\n3. Interpersonal Response Generation人际响应生成\n   生成响应/回复移情向量$e_R$,该向量指明回复中生成的情感方面，且包含了小冰的人格。\n   小冰通过跟踪由Topic Manager决定的同一个主题，共享用户的感受，并按指定的一致consistent和积极的方式进行响应。在$e_R$中，根据指定的意图、情感和观点等的值，以一致consistent和积极的方式作出反应，这些值是根据$e_Q$中的值使用一组启发式计算的。回应必须同时符合小冰的人格，人格的键值对(如年龄、性别、兴趣等)从预编译的小冰的profile中抽取。\n   \n\n## 4.3 Core Chat\nCore Chat是小冰IQ和EQ的非常重要的组成部分，和移情计算一起提供基本的交流能力，接受文本输入，生成人际响应为输出。\n包含两部分：通用聊天General Chat和域聊天Domain Chat。二者仅数据集不同，而引擎相同。以General Chat为例\n\nGeneral Chat是数据驱动的回复生成系统。接纳输入:对话状态$s = (Q_c, C, e_Q, e_R)$，分两阶段输出响应$R$: 响应候选生成、排序。相应候选从数据库中检索得到。数据库由人类生成的对话文本组成的，或由神经生成模型快速生成。(or generated on the fly  by a neural generative model)\n以下介绍三个候选生成器和一个候选排序器\n\n### Retrieval-Based Generator using Paired Data使用成对数据的基于检索的生成器\n成对数据来源：①互联网上的人类交流数据(e.g.社交网络、公共论坛、公告板、新闻评论)②小冰和用户交流产生的人机对话数据\n为了控制数据集质量：\n1. 基于pair抽取来源的对话上下文、网页元数据和网站、用户profile，使用移情计算模块，将pair转换为tuple$(Q_c, R, e_Q, e_R)$\n2. 基于tuples过滤pairs，保留符合小冰人格的移情响应的对话对\n3. 移除包含个人识别信息（PII）、混乱的代码、不适当的内容、拼写错误等的pairs。\n\n过滤得到的pairs使用Lucene索引。运行时，使用$s$中的$Q_c$作为query，使用关键词搜索和基于机器学习表示成对数据集的语义搜索，检索400条响应候选。尽管从配对数据集中检索到的候选响应质量很高，但覆盖率coverage很低，因为数据集中不包括论坛上许多新的或不太常讨论的主题。为了增加覆盖率，我们引入了下面描述的另外两个候选生成器。\n### Neural Response Generator神经响应生成器\n使用配对数据集**训练**来模仿人类交流，可以生成任意主题（包含人类交流数据中没有的）。\n基于神经模型的生成器与基于检索的生成器互补：\nneural-model-based generator: robustness, high coverage\nretrieval-based generator: 对流行主题的high-quality\n小冰使用了 用于对话响应生成的seq2seq框架\n基于GRU-RNN，与Speraker-Addressee相似\n输入$(Q_c, e_Q, e_R)$,期望预测用$e_R$建模的小冰(addressee)如何响应用$e_Q$建模的用户(speaker)产生的查询query $Q_c$\n\n1. 线性组合$e_Q$和$e_R$(query and response empathy vectors)得到一个交互表示$v \\in \\mathbb{R}^d$，尝试建模小冰对用户的交互风格。$W_Q,W_R\\in\\mathbb{R}^{k*d}，\\sigma$表示sigmoid函数。\n   $$v=\\sigma(W_Q^{\\mathrm{ T } }e_Q+W_R^{\\mathrm{ T } }e_R)$$\n2. source RNN编码用户查询$Q_c$到一个隐状态向量序列中，向量随后输入target RNN,逐词生成响应$R$，每条响应以EOS结尾。\n3. 用beam search生成20条候选。\n\n在target RNN一侧，在每个时间步$t$，隐状态$h_t$由①前一个时间步的隐状态$h_{t-1}$、②当前时间步的词嵌入向量$e_t$和③$v$ 组合而成。通过这种方式，在每一个步骤中，移情信息都会被注入到隐藏层中，以帮助在整个生成过程中产生符合小冰人格的人际响应。\n模型的详细说明见附录A。\n### Retrieval-Based Generator using Unaired Data使用非成对数据的基于检索的生成器\n除了上述两个响应生成器使用的会话（或成对）数据之外，还有更高质量和更大数量的非会话（或未配对）数据，可用于提高响应的质量和覆盖范围。e.g.公开演讲、新闻文章和报道中的引用。这些句子被视为候选响应$R$.因为我们知道这些句子的作者，所以我们计算每个句子的移情向量$e_R$。与成对数据类似的数据过滤流水线用于保留且仅保留符合小冰人格的响应$(R，e_R)。$\n这些不成对的数据也用Lucene索引。不同于成对数据集，运行时我们需要将查询query $Q_c$扩展到包含额外主题，以避免检索到简单重复用户说过的话的响应。使用知识图谱KG对查询进行扩展。KG包含$head-relation-tail$的三元组集合$(h,r,t)$，通过加入成对数据和Microsoft Satori（微软的知识图谱）构建而成。当且仅当成对数据集中对话对(conversation pairs)的数量大于预先设定的阈值时，Satori三元组$(h,r,t)$包含到小冰的KG中。成对数据集：$h$出现在$Q_c$中，$t$出现在$R$中。这样的三元组包含了人们经常在同一场交流中提及的两个相关的话题。e.g.(Beijing, Great Wall),(Einstein, Relativity),(Quantum Physics, Schrodinger's cat).\n使用非配对数据及和小冰KG生成候选响应的过程：\n1. 识别上下文用户查询$Q_c$中的话题\n2. 对每一个话题，从KG中检索二十个最相关的话题。这些话题依据相关度relevance，使用用人工标注数据训练的boosted tree ranker打分。\n3. 结合$Q_c$中的主题和KG中得到的相关话题，形成query，使用query从非配对数据集中检索最多400条最相关句子，作为响应候选。\n\n此生成器是对上述其他两个生成器的补充。质量较低、涵盖范围更广。与通常生成**形式**良好但响应较短的神经响应生成器相比，未配对数据的候选对象更长，**内容**更有用。\n### Response Candidate Ranker响应候选排序器\n三个生成器生成的响应使用boosted tree ranker聚合排序。响应从排序分数高于预设阈值的候选中**随机**选择\nGiven 对话状态$s=(Q_c,C,e_Q,e_R)$,基于以下四类特征给每个响应候选$R'$一个排序分数：\n1. 局部耦合特征：好的候选应在语义上与用户输入$Q_c$一致或相关。使用一组在人类交流对human conversation pairs上训练的DSSMs计算$R'$和$Q_c$之间的聚合分数。\n2. 全局耦合特征：好的候选应在语义上与$Q_c$和$C$契合。使用一组用人类对话会话human dialogue sessions训练的DSSMs计算$R'$和$(Q_c,C)$之间的聚合分数coherence socres.因为聚合特征使用了全局上下文信息$C$，当$Q_c$是没有上下文、很难检测主题，平淡乏味bland的query时e.g.\"OK\",\"why\",\"I don't know\"，格外有用。\n3. 移情匹配特征：好的候选应该是符合小冰**人格**的**移情**回复。假设小冰选择$R'$来响应给定的上下文$(Q_c,C)$,使用移情计算模块为$R'$计算响应移情向量response empathy vector，$e_{R'}$，然后通过比较 $e_{R'}$和给定的、编码了**期望**响应expected response的移情特征的$e_R$，计算一组移情匹配特征empathy matching features。\n*移情计算模块：将$R'$当做query，$(Q_c,C)$当做上下文，使用上下文查询理解contextual query understand和用户理解user understanding组件来计算查询移情向量query empathy vector $e_{R'}$*\n4. 检索匹配特征:这些特征额仅应用于成对数据集生成的候选。计算$Q_c$和检索查询-响应对(retrived query-response pairs)的query端的匹配分数，在词语级(例如BM25和TFIDF分数)和语义级(例如DSSM分数)均要计算。\n排序器在对话-状态-响应对dialogue-state-pairs$(s,R)$上训练。pair有三种标记等级：\n- 0：响应没有移情或与query不是很相关。可能导致对话终止。\n- 1：响应可接受，与query相关。可能帮助对话继续。\n- 2：响应是移情的、人际的interpersonal，使用户感到快乐兴奋。可能驱动对话。\n*DSSM：深度结构语义模型Deep Structured Semantic Models/深度语义相似模型Deep Semantic Similarity Model.是用于衡量输入对$(x,y)$语义相似度的深度学习模型。在本文中，$(x,y)$是查询-候选-响应对query-candidate-response pair$(Q_c,C)$*\n### Editorial Response编辑响应\n如果候选生成器和响应排序器由于各种原因（例如不在索引中not-in-index、模型失败model failure、执行超时或输入查询包含不正确内容），导致未能生成任何有效响应，则选择编辑响应。为了让对话继续下去，\n编辑响应应该：有同理心empathetic；不应该：安全但温和safe but bland\ne.g.“嗯，很难说。你怎么想 Hmmm, difficult to say. What do you think？”√\n“让我们谈点别的 Let us talk about something else.” √\n“我不知道 I don't know.”×\n“我还在学习回答你的问题 I am still learning to answer your question.”×\n\n## 4.4 对话技巧Dialogue Skills\n### 4.4.1 Image Commenting 图片评论\n图像评论不是为了正确认知描述图像，而是生成反映个人情感、态度、立场的移情评论。\n### 4.4.2 Content Creation 内容创造\n### 4.4.3 Deep Engagement 深度参与\n深度参与技能旨在通过针对特定主题和设置满足用户的特定情感和智能需求，从而提高用户的长期参与度。这些技能可以在两个维度上分为不同系列：IQ 到 EQ，私人一对一 到 小组讨论\n- 满足用户的智能或情感需求：小冰分享自己在不同的IQ主题(e.g.数学、历史、食物、旅游、名人)上的兴趣、经历、知识。\n\n### 4.4.4 Task Completion 任务完成\n# Challenges\n1. 同一模型框架\n2. 目标导向、有充足理由的对话 grounded\n3. 积极主动proactive的个人助理：比传统智能个人助手更准确地认知用户兴趣和意图。\n4. 人类水平智能：有认知有意识的AI。移情计算、知识和记忆建模、可解释机器职能、常识推理、神经符号推理、跨媒体连续流AI、建模反映在人类需求中的感情或内在回报\n5. 道德\n# Appendix A: GRU-RNN Based Response Generator\n与speaker-addressee模型类似。\n给定输入$(Q_c,e_Q,e_R)$，我们希望预测用$e_R$建模小冰(Addressee)如何响应$e_Q$建模的speaker模型产生的$Q_c$。 \n通过线性组合查询移情向量$e_Q$和响应移情向量$e_R$(query and response empathy vectors)，得到一个交互表示$v \\in \\mathbb{R}^d$，尝试建模小冰对用户(speaker)的交互风格。\n   $$v=\\sigma(W_Q^{\\mathrm{ T } }e_Q+W_R^{\\mathrm{ T } }e_R)$$\n其中$W_Q,W_R\\in\\mathbb{R}^{k*d}，\\sigma$表示sigmoid函数。\n使用source RNN将$Q_c$编码到向量表示$h_Q$中。\n在target RNN侧的每一个事件步$t$，隐状态$h_t$由①前一个时间步的隐状态$h_{t-1}$、②当前时间步的词嵌入向量$e_t$和③$v$ 组合而成。通过这种方式，在每一个步骤中，移情信息都会被注入到隐藏层中，以帮助在整个生成过程中产生符合小冰人格的人际响应。\n$u_t$和$z_t$分别表示时间步$t$时，GRU的更新门update和重置门reset.计算$t$时间步的隐状态$h_t$：\n$$u_t=\\sigma(W_u^\\mathrm{ T } [h_{t-1};e_t;v])$$\n$$z_t=\\sigma(W_z^\\mathrm{ T } [h_{t-1};e_t;v])$$\n$$l_t=tanh(W_l^\\mathrm{ T } [z_t\\circ h_{t-1};e_t;v])$$\n$$h_t^Q=(1-u_t)\\circ h_{t-1}+u_t\\circ l_t$$\n其中$W_u,W_z,W_l \\in \\mathbb{R}^{3d*d}$是机器学习得到的矩阵，$\\circ$表示元素乘。\nRNN模型定义了$R$中下一个token的概率，用于使用softmax函数预测。\n$$\n\\begin{align}\np(R|Q_c,e_Q,e_R) &=\\prod_{t=1}^{N_R}p(r_t|Q_c,e_Q,e_R,r_1,r_2,...,r_{t-1})\\\\\n&= \\prod_{t=1}^{N_R}\\frac{exp(f(h_{t-1},e_{r_t},v))}{\\Sigma_{r'}exp(f(h_{t-1},e_{r'},v))}\n\\end{align}\n$$\n其中$f(h_{t-1},e_{r_t},v)$表示$h_{t-1},e_{r_t},v$之间的激活函数，$h_{t-1}$是RNN在$t-1$时间步的输出的表示。每个响应以特殊的表示句子结束的符号EOS结尾。\n响应生成模型参数$\\theta=(W_Q,W_R,W_u,W_z,W_l)$通过在训练数据上最大化对数似然训练，使用随机梯度下降法SGD得到。\n$$\\arg\\mathop{\\max}_{\\theta}\n\\frac{1}{M} \\sum^M_{i=1} log p_{\\theta}(R^{(i)}|Q_c^{(i)},e_Q^{(i)},e_R^{(i)})$$","categories":["Dialogue System"]},{"title":"批标准化Batch Normalization","url":"/2020/02/03/batch-normalization/","content":"https://www.cnblogs.com/guoyaohua/p/8724433.html\n\nIID独立同分布假设:机器学习领域有个很重要的假设,就是假设训练数据和测试数据是满足相同分布的，这是通过训练数据获得的模型能够在测试集获得好的效果的一个基本保障。\nBatchNorm作用:在深度神经网络训练过程中使得每一层神经网络的输入保持相同分布的。","categories":["Machine Learning","Deep Learning"]},{"title":"深度学习常用优化器及Pytorch使用","url":"/2020/01/29/deep-learning-optimizer/","content":"本文参考自[Deep Learning 之 最优化方法](https://blog.csdn.net/bvl10101111/article/details/72615621)和[Pytorch官方文档1.4.0版本](https://pytorch.org/docs/stable/optim.html)，另有[官方文档中文翻译](https://pytorch-cn.readthedocs.io/zh/latest/package_references/torch-optim/)可供参考。\n# SGD\n```python\ntorch.optim.SGD(params, lr=<required parameter>, momentum=0, dampening=0, weight_decay=0, nesterov=False)\n\"\"\"\nparams (iterable) – 待优化参数的iterable或者是定义了参数组的dict\nlr (float) – 学习率\nmomentum (float, 可选) – 动量因子（默认：0）\nweight_decay (float, 可选) – 权重衰减（L2惩罚）（默认：0）\ndampening (float, 可选) – 动量的抑制因子（默认：0）\nnesterov (bool, 可选) – 使用Nesterov动量（默认：False）\n\"\"\"\n# 例\noptimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\noptimizer.zero_grad()\nloss_fn(model(input), target).backward()\noptimizer.step()\n```\n# Momentum\n1. 动量方法主要是为了解决Hessian矩阵病态条件问题（直观上讲就是梯度高度敏感于参数空间的某些方向）的。\n2. 加速学习\n3. 一般将参数设为0.5,0.9，或者0.99，分别表示最大速度2倍，10倍，100倍于SGD的算法。\n4. 通过速度v，来积累了之间梯度指数级衰减的平均，并且继续延该方向移动：\n","categories":["Deep Learning","Pytorch"]},{"title":"pytorch loss","url":"/2020/01/28/pytorch-loss/","content":"https://blog.csdn.net/u011995719/article/details/85107524\nhttps://www.cnblogs.com/wanghui-garcia/p/10862733.html\nhttps://blog.csdn.net/u011995719/article/details/85107524#6KLDivLoss_101\n"},{"title":"python trick","url":"/2020/01/28/python-trick/","content":"```python\nimport warnings\nwarnings.filterwarnings('ignore')\n```\n"},{"title":"pytorch常用代码段","url":"/2020/01/28/pytorch-common-code/","content":"转载自 [[深度学习框架]PyTorch常用代码段](https://zhuanlan.zhihu.com/p/104019160)\n\n# 1. 基本配置\n## 导入包和版本查询\n```python\nimport torch\nimport torch.nn as nn\nimport torchvision\nprint(torch.__version__)\nprint(torch.version.cuda)\nprint(torch.backends.cudnn.version())\nprint(torch.cuda.get_device_name(0))\n```\n## 可复现性 - 随机种子\n在硬件设备（CPU、GPU）不同时，完全的可复现性无法保证，即使随机种子相同。但是，在同一个设备上，应该保证可复现性。具体做法是，在程序开始的时候固定torch的随机种子，同时也把numpy的随机种子固定。\n```python\nnp.random.seed(0)\ntorch.manual_seed(0)\ntorch.cuda.manual_seed_all(0)\n\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n```\n\n## 显卡设置\n如果只需要一张显卡\n```python\n# Device configuration\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n```\n如果需要指定多张显卡，比如0，1号显卡。\n```python\nimport os\nos.environ['CUDA_VISIBLE_DEVICES'] = '0,1'\n```\n也可以在命令行运行代码时设置显卡：\n```bash\nCUDA_VISIBLE_DEVICES=0,1 python train.py\n```\n## 清除显存\n```python\ntorch.cuda.empty_cache()\n```\n也可以使用在命令行重置GPU的指令\n```python\nnvidia-smi --gpu-reset -i [gpu_id]\n```\n# 2. 张量(Tensor)处理\n## 张量的数据类型\nPyTorch有9种CPU张量类型和9种GPU张量类型。\n![](/images/pytorch-data-type.jpg)\n## 张量基本信息\n```python\ntensor = torch.randn(3,4,5)\nprint(tensor.type())  # 数据类型\nprint(tensor.size())  # 张量的shape，是个元组\nprint(tensor.dim())   # 维度的数量\n```\n## 命名张量\n张量命名是一个非常有用的方法，这样可以方便地使用维度的名字来做索引或其他操作，大大提高了可读性、易用性，防止出错。\n```python\n# 在PyTorch 1.3之前，需要使用注释\n# Tensor[N, C, H, W]\nimages = torch.randn(32, 3, 56, 56)\nimages.sum(dim=1)\nimages.select(dim=1, index=0)\n\n# PyTorch 1.3之后\nNCHW = [‘N’, ‘C’, ‘H’, ‘W’]\nimages = torch.randn(32, 3, 56, 56, names=NCHW)\nimages.sum('C')\nimages.select('C', index=0)\n# 也可以这么设置\ntensor = torch.rand(3,4,1,2,names=('C', 'N', 'H', 'W'))\n# 使用align_to可以对维度方便地排序\ntensor = tensor.align_to('N', 'C', 'H', 'W')\n```\n## 数据类型转换\n```python\n# 设置默认类型，pytorch中的FloatTensor远远快于DoubleTensor\ntorch.set_default_tensor_type(torch.FloatTensor)\n\n# 类型转换\ntensor = tensor.cuda()\ntensor = tensor.cpu()\ntensor = tensor.float()\ntensor = tensor.long()\n```\n## torch.Tensor与np.ndarray转换\n除了CharTensor，其他所有CPU上的张量都支持转换为numpy格式然后再转换回来。\n```python\nndarray = tensor.cpu().numpy()\ntensor = torch.from_numpy(ndarray).float()\ntensor = torch.from_numpy(ndarray.copy()).float() # If ndarray has negative stri\n```\n## Torch.tensor与PIL.Image转换\n```python\n# pytorch中的张量默认采用[N, C, H, W]的顺序，并且数据范围在[0,1]，需要进行转置和规范化\n# torch.Tensor -> PIL.Image\nimage = PIL.Image.fromarray(torch.clamp(tensor*255, min=0, max=255).byte().permute(1,2,0).cpu().numpy())\nimage = torchvision.transforms.functional.to_pil_image(tensor)  # Equivalently way\n\n# PIL.Image -> torch.Tensor\npath = r'./figure.jpg'\ntensor = torch.from_numpy(np.asarray(PIL.Image.open(path))).permute(2,0,1).float() / 255\ntensor = torchvision.transforms.functional.to_tensor(PIL.Image.open(path)) # Equivalently way\n```\n## np.ndarray与PIL.Image的转换\n```python\nimage = PIL.Image.fromarray(ndarray.astype(np.uint8))\n\nndarray = np.asarray(PIL.Image.open(path))\n```\n## 从只包含一个元素的张量中提取值\n```python\nvalue = torch.rand(1).item()\n```\n## 张量形变\n```python\n# 在将卷积层输入全连接层的情况下通常需要对张量做形变处理，\n# 相比torch.view，torch.reshape可以自动处理输入张量不连续的情况。\ntensor = torch.rand(2,3,4)\nshape = (6, 4)\ntensor = torch.reshape(tensor, shape)\n```\n## 打乱顺序\n```python\ntensor = tensor[torch.randperm(tensor.size(0))]  # 打乱第一个维度\n```\n## 水平翻转\n```python\n# pytorch不支持tensor[::-1]这样的负步长操作，水平翻转可以通过张量索引实现\n# 假设张量的维度为[N, D, H, W].\ntensor = tensor[:,:,:,torch.arange(tensor.size(3) - 1, -1, -1).long()]\n```\n## 复制张量\n```python\n# Operation                 |  New/Shared memory | Still in computation graph |\ntensor.clone()            # |        New         |          Yes               |\ntensor.detach()           # |      Shared        |          No                |\ntensor.detach.clone()()   # |        New         |          No                |\n```\n## 张量拼接\n```python\n'''\n注意torch.cat和torch.stack的区别在于torch.cat沿着给定的维度拼接，\n而torch.stack会新增一维。例如当参数是3个10x5的张量，torch.cat的结果是30x5的张量，\n而torch.stack的结果是3x10x5的张量。\n'''\ntensor = torch.cat(list_of_tensors, dim=0)\ntensor = torch.stack(list_of_tensors, dim=0)\n```\n## 将整数标签转为one-hot编码\n```python\n# pytorch的标记默认从0开始\ntensor = torch.tensor([0, 2, 1, 3])\nN = tensor.size(0)\nnum_classes = 4\none_hot = torch.zeros(N, num_classes).long()\none_hot.scatter_(dim=1, index=torch.unsqueeze(tensor, dim=1), src=torch.ones(N, num_classes).long())\n```\n## 得到非零元素\n```python\ntorch.nonzero(tensor)               # index of non-zero elements\ntorch.nonzero(tensor==0)            # index of zero elements\ntorch.nonzero(tensor).size(0)       # number of non-zero elements\ntorch.nonzero(tensor == 0).size(0)  # number of zero elements\n```\n## 判断两个张量相等\n```python\ntorch.allclose(tensor1, tensor2)  # float tensor\ntorch.equal(tensor1, tensor2)     # int tensor\n```\n## 张量扩展\n```python\n# Expand tensor of shape 64*512 to shape 64*512*7*7.\ntensor = torch.rand(64,512)\ntorch.reshape(tensor, (64, 512, 1, 1)).expand(64, 512, 7, 7)\n```\n## 矩阵乘法\n```python\n# Matrix multiplcation: (m*n) * (n*p) * -> (m*p).\nresult = torch.mm(tensor1, tensor2)\n\n# Batch matrix multiplication: (b*m*n) * (b*n*p) -> (b*m*p)\nresult = torch.bmm(tensor1, tensor2)\n\n# Element-wise multiplication.\nresult = tensor1 * tensor2\n```\n## 计算两组数据之间的两两欧式距离\n利用broadcast机制\n```python\ndist = torch.sqrt(torch.sum((X1[:,None,:] - X2) ** 2, dim=2))\n```\n# 3. 模型定义和操作\n## 一个简单两层卷积网络的示例\n```python\n\n```\n\n```python\n\n```\n\n```python\n\n```\n\n```python\n\n```\n\n```python\n\n```\n\n```python\n\n```\n\n```python\n\n```\n\n```python\n\n```\n\n```python\n\n```\n\n```python\n\n```\n\n```python\n\n```\n\n```python\n\n```\n\n```python\n\n```\n\n\n\n\n\n\n```python\n\n```","categories":["Pytorch"]},{"title":"动态规划","url":"/2020/01/23/dynamic-programming/","content":"`动态规划`是一种 用来解决一类**最优化问题**的算法思想\nDP 讲一个复杂的问题分解成若干子问题,通过综合子问题的最优解来得到原问题的最优解\nDP会将每个球结果的子问题的解记录下来,在下一次碰到同样的子问题时,可以直接使用之前记录的结果,而不是重复计算.\n\n- `重叠子问题`(Overlapping Subproblems):如果一个问题可以被分解为若干个子问题,且这些子问题会重复出现,那么就称这个问题拥有重叠子问题\n- `最优子结构`(Optimal Substructure):如果一个问题的最优解可以由其子问题的最优解有效的构造出来,那么称这个问题拥有最优子结构\n-  一个问题必须拥有重叠子问题和最优子结构,才能用DP解决\n- `状态转移方程`\n- `状态的无后效性`:当前状态记录了历史信息,一旦当前状态确定,就不会再改变.未来的决策只能在已有的一个或若干个状态的基础上进行,历史信息只能通过已有的状态去影响未来的决策\n- `自底向上`(Bottom-Up):递推写法\n- `自顶向下`(Top-Down):递归写法\n- `多阶段动态规划`:一类动态规划可解问题，可以描述成若干个有序的阶段，且每个阶段的状态只和上一个阶段的状态有关。只要从第一个问题开始，按照阶段顺序解决每个阶段中状态的计算，就可以得到最后一个阶段中状态的解。\n\nDP问题必须设计拥有无后效性的状态以及相应的状态转移方程.如何设计状态和状态转移方程,才是动态规划的核心与难点.\n\n![](/images/dp.png)\n\n### 最大连续子序列和:\n[leetcode 53. 最大子序和](https://leetcode-cn.com/problems/maximum-subarray/)\n$dp[i]$表示以$A[i]$作为结尾的连续序列的最大和.\n- 这个最大和的连续序列只有一个元素,即以$A[i]$开始,以$A[i]$结尾\n- 这个最大和的连续序列有多个元素,即从前面某处 $A[p]$ 开始(p<i),一直到 $A[i]$ 结尾\n$$dp[i] = max{A[i], dp[i-1] + A[i]}$$\n### 最长不下降子序列(Longest Increasing Sequence, LIS)\n$dp[i]$表示以$A[i]$作为结尾的最长不下降子序列的最大和.\n－ 如果存在$A[i]$之前的元素$A[j](j<i)$,使得$A[j]xiaoyA[i]$\n### 最长公共子序列\n### 最长回文子串\n### 背包问题\n01背包 完全背包\n\n","categories":["Algorithms"]},{"title":"实习准备知识点总结","url":"/2020/01/22/2020-winter-interview/","content":"自我介绍\nseq2seq+Attention\n注意力的种类 自注意力 画出来\nTransformer\nself-attention 与之前的区别\nDNN反向传播\n梯度下降优化方法(Adam 动量梯度下降)什么是动量\n如何解决梯度消失和梯度爆炸\n神经网络怎么判断过拟合(dropout 正则) 防止过拟合的方法 L1L2正则区别\n语言模型公式\n为什么n-gram需要平滑 平滑技术\n朴素贝叶斯公式 展开\nCRF是什么 和HMM的区别\n推logistic回归 交叉熵损失函数 优化目标 #http://blog.wangcaimeng.top/2019/03/05/LogisticRegression-%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%EF%BC%89%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E6%8E%A8%E5%AF%BC/\n决策树 随机森林\nGDBT XGBoost\n最熟悉的机器学习算法 做过什么\nLSTM RNN 画结构 能解决什么问题\n预训练与微调\n优化方法\n\"有一个抽奖活动，提前不知道会有多少人抽奖，最后要保证每个人中奖概率一样，且一定有一个人中奖，问我怎么设计抽奖系统\"\n\n讲项目\n\n多轮对话生成安全性答案\n\n二叉树逆时针遍历外围 s形\n二分查找\n\n# 1. seq2seq + Attention\n在⾃然语⾔处理的很多应⽤中，输⼊和输出都可以是不定⻓序列。"},{"title":"linux 常用文件及命令的使用","url":"/2020/01/20/linux-commands/","content":"```bash\ngedit ~/.bashrc\nsudo gedit /etc/hosts\n```\n","categories":["Linux"]},{"title":"python算法与数据结构模板","url":"/2020/01/18/python-algorithm-templetes/","content":"# Quick Sort\n```python\ndef quick_sort(arr):\n    \"\"\"快速排序\"\"\"\n    if len(arr) < 2:\n        return arr\n    # 选取基准，随便选哪个都可以，选中间的便于理解\n    mid = arr[len(arr) // 2]\n    # 定义基准值左右两个数列\n    left, right = [], []\n    # 从原始数组中移除基准值\n    arr.remove(mid)\n    for item in arr:\n        # 大于基准值放右边\n        if item >= mid:\n            right.append(item)\n        else:\n            # 小于基准值放左边\n            left.append(item)\n    # 使用迭代进行比较\n    return quick_sort(left) + [mid] + quick_sort(right)\n```\n","categories":["Algorithms"]},{"title":"博客标签分类总结","url":"/2020/01/18/categories-and-tags/","content":"# categories:\n理论知识类\n- Deep Learning\n- Algorithms\n- Dialogue System\n工具使用类\n- Hexo\n- Linux\n- Python\n- Pytorch\n- Anaconda\n\n# tags:\n","categories":["Hexo","Algorithms"]},{"title":"寒假面试刷题指北","url":"/2020/01/18/2020-winter-leetcode/","content":"[Python 算法与数据结构](https://python-data-structures-and-algorithms.readthedocs.io/zh/latest/)\n\n[数据结构与算法 Python](https://www.ranxiaolang.com/static/python_algorithm/chapter1/index.html)\n\n[CyC2018 刷题目录](https://github.com/CyC2018/CS-Notes/blob/master/notes/Leetcode%20%E9%A2%98%E8%A7%A3%20-%20%E7%9B%AE%E5%BD%95.md)\n\n[azl部分python解](https://github.com/azl397985856/leetcode)\n\n[动画演示](https://github.com/MisterBooo/LeetCodeAnimation)\n\n---\n# 按难度\nAZL EASY\n简单难度\n1. ~~0020.Valid Parentheses~~\n1. ~~0026.remove-duplicates-from-sorted-array~~\n1. 0053.maximum-sum-subarray new\n1. ~~0088.merge-sorted-array~~\n1. ※0104.maximum-depth-of-binary-tree 1.19\n1. 0121.best-time-to-buy-and-sell-stock\n1. 0122.best-time-to-buy-and-sell-stock-ii\n1. ~~0125.valid-palindrome~~\n1. ~~0136.single-number~~ 1.19\n1. 0155.min-stack new\n1. ~~0167.two-sum-ii-input-array-is-sorted~~\n1. ※0172.factorial-trailing-zeroes\n\n    ※2020.1.18\n\n    因为是阶乘后的0,0是由于2\\*5得出一个0,因此便是求2\\*5有多少个.因为2<5,有5一定有2,因此便是求5的个数\n1. ~~0169.majority-element~~\n1. 0190.reverse-bits\n1. 0191.number-of-1-bits\n1. 0198.house-robber\n1. ~~0203.remove-linked-list-elements~~ 1.19\n1. ※0206.reverse-linked-list\n1. ~~0219.contains-duplicate-ii~~ 1.19\n1. 0226.invert-binary-tree\n1. 0232.implement-queue-using-stacks new\n1. 0263.ugly-number\n1. 0283.move-zeroes\n1. 0342.power-of-four\n1. ~~0349.intersection-of-two-arrays~~\n1. 0437.path-sum-iii new\n1. 0371.sum-of-two-integers\n1. 0501.find-mode-in-binary-search-treenew\n1. 0575.distribute-candies\n1. 1260.shift-2d-gridnew\n# 按分类\n链表\n1. 找出两个链表的交点\n2. 链表反转\n3. ~~归并两个有序的链表~~ 1.18\n4. ~~从有序链表中删除重复节点~~ 1.18\n5. 删除链表的倒数第 n 个节点\n6. 交换链表中的相邻结点\n7. 链表求和\n8. 回文链表\n9. 分隔链表\n10. 链表元素按奇偶聚集\n# 按日期\n## 0) 1.18 廿四\n## 1) 1.19 廿五\n1. [20 有效的括号](https://leetcode-cn.com/problems/valid-parentheses/description/) 栈\n2. [206 翻转链表](https://leetcode-cn.com/problems/reverse-linked-list/description/) \n3. [160 相交链表](https://leetcode-cn.com/problems/intersection-of-two-linked-lists/) # TODO\n4. [730 每日温度](https://leetcode-cn.com/problems/daily-temperatures/description/) 贪心 # TODO\n5. [121. 买卖股票的最佳时机](https://leetcode-cn.com/problems/best-time-to-buy-and-sell-stock/description/)# DP动态规划 TODO\n6. [136. 只出现一次的数字](https://leetcode-cn.com/problems/single-number/submissions/) 异或\n7. [263. 丑数](https://leetcode-cn.com/problems/ugly-number/)# TODO 连续判断\n## 2) 1.20 廿六\n1. [1003. 检查替换后的词是否有效](https://leetcode-cn.com/problems/check-if-word-is-valid-after-substitutions/) 还有栈解法\n2. TODO [0015](https://leetcode-cn.com/problems/3sum/)# \n3. TODO [0633. 平方数之和 ](https://leetcode-cn.com/problems/sum-of-square-numbers/)双指针\n4. [0144. 二叉树的前序遍历](https://leetcode-cn.com/problems/binary-tree-preorder-traversal/)迭代 莫里斯遍历#TODO\n5. TODO [0145. 二叉树的后序遍历](https://leetcode-cn.com/problems/binary-tree-postorder-traversal/)# 注意迭代写法与前序区别很大\n6. TODO [1302. 层数最深叶子节点的和](https://leetcode-cn.com/problems/deepest-leaves-sum/)\n## 3) 1.21 廿七\n1. [1315. 祖父节点值为偶数的节点和](https://leetcode-cn.com/problems/sum-of-nodes-with-even-valued-grandparent/)\n2. ※1.19AGAIN [110. 平衡二叉树](https://leetcode-cn.com/problems/balanced-binary-tree/)bottom-up, top-down\n3. [0543. 二叉树的直径](https://leetcode-cn.com/problems/diameter-of-binary-tree/)最长不一定通过root\n4. [0226. 翻转二叉树]()\n5. TODO [0112. 路径总和]()\n6. TODO [0617. 合并二叉树](https://leettodocode-cn.com/problems/merge-two-binary-trees/)\n7. [0563. 二叉树的坡度](https://leetcode-cn.com/problems/binary-tree-tilt/)\n8. [0897. 递增顺序查找树](https://leetcode-cn.com/problems/increasing-order-search-tree/)建立新树 或 利用现有节点\n9. TODO [0669. 修剪二叉搜索树](https://leetcode-cn.com/problems/trim-a-binary-search-tree/) ->1.23\n10. [0098. 验证二叉搜索树](https://leetcode-cn.com/problems/validate-binary-search-tree/) TODO:上下界\n11. [0100. 相同的树](https://leetcode-cn.com/problems/same-tree/)\n12. [0230. 二叉搜索树中第K小的元素](https://leetcode-cn.com/problems/kth-smallest-element-in-a-bst/)\n13. TODO [0235. 二叉搜索树的最近公共祖先](https://leetcode-cn.com/problems/lowest-common-ancestor-of-a-binary-search-tree/) ->1.23\n14. [0108. 将有序数组转换为二叉搜索树](https://leetcode-cn.com/problems/convert-sorted-array-to-binary-search-tree/)\n15. [0501. 二叉搜索树中的众数](https://leetcode-cn.com/problems/find-mode-in-binary-search-tree/)还要考虑中序遍历法\n## 4) 1.22 廿八\n## 5) 1.23 廿九\n1. [0669. 修剪二叉搜索树](https://leetcode-cn.com/problems/trim-a-binary-search-tree/)\n2. [0235. 二叉搜索树的最近公共祖先](https://leetcode-cn.com/problems/lowest-common-ancestor-of-a-binary-search-tree/)\n## 6) 1.24 除夕\n1. TODO[53. 最大子序和](https://leetcode-cn.com/problems/maximum-subarray/)已解决DP,还有分治和贪心\n2. TODO[0300. 最长上升子序列](https://leetcode-cn.com/problems/longest-increasing-subsequence/) TODO复杂度nlogn的情况\n## 7) 1.25 春节\n1. TODO[0005. 最长回文子串](https://leetcode-cn.com/problems/longest-palindromic-substring/)\n2. TODO[0516. 最长回文子序列](https://leetcode-cn.com/problems/longest-palindromic-subsequence/)\n## 8) 1.26 初二\n1. [0070. 爬楼梯](https://leetcode-cn.com/problems/climbing-stairs/)\n2. [0198. 打家劫舍](https://leetcode-cn.com/problems/house-robber/)\n3. [0213. 打家劫舍 II](https://leetcode-cn.com/problems/house-robber-ii/)\n4. [0064. 最小路径和](https://leetcode-cn.com/problems/minimum-path-sum/)\n5. [0062. 不同路径](https://leetcode-cn.com/problems/unique-paths/)\n## 9) 1.27 初三\n## 10) 1.28 初四\n## 11) 1.29 初五\n## 12) 1.30 初六\n## 13) 1.31 初七\n## 14) 2.1 初八\n## 15) 2.2 初九\n优化算法 SGD Momentum Nesterov AdaGrad RMSprop\n## 16) 2.3 初十\n优化算法Adam+总结\n导数 偏导数 方向导数 微分 偏微分 全微分 梯度\n1. [118. 杨辉三角](https://leetcode-cn.com/problems/pascals-triangle/)\n## 17) 2.4 十一\n1. [1304. 和为零的N个唯一整数.py](https://leetcode-cn.com/problems/find-n-unique-integers-sum-up-to-zero/)区分整除//与int(/)\n2. [1252. 奇数值单元格的数目](https://leetcode-cn.com/problems/cells-with-odd-values-in-a-matrix/)注意初始化全零数组的写法\n3. [0561. 数组拆分 I](https://leetcode-cn.com/problems/array-partition-i/)\n4. [0905. 按奇偶排序数组](https://leetcode-cn.com/problems/sort-array-by-parity/)TODO 交换写法 双指针解法 快排 etc\n5. [119. 杨辉三角 II](https://leetcode-cn.com/problems/pascals-triangle-ii/)选择index的时候要使用以后不会用的。如果正序，能用a[i] += a[i + 1]，就不要用a[i - 1]\n6. [120. 三角形最小路径和](https://leetcode-cn.com/problems/triangle/) DP\n## 18) 2.5 十二\n(回溯)\n1. [46. 全排列]\n2. [47. 全排列 II]\n39.组合总和\n\n40. 组合总和 II\n\n47. 全排列 II\n\n78. 子集\n\n90. 子集 II\n\n1. [0102. 二叉树的层次遍历](https://leetcode-cn.com/problems/binary-tree-level-order-traversal/)\n2. [107. 二叉树的层次遍历 II](https://leetcode-cn.com/problems/binary-tree-level-order-traversal-ii/)\n3. [637. 二叉树的层平均值](https://leetcode-cn.com/problems/average-of-levels-in-binary-tree/)\n4. [103. 二叉树的锯齿形层次遍历](https://leetcode-cn.com/problems/binary-tree-zigzag-level-order-traversal/)\n5. [111. 二叉树的最小深度](https://leetcode-cn.com/problems/minimum-depth-of-binary-tree/)\n6. [429. N叉树的层序遍历](https://leetcode-cn.com/problems/n-ary-tree-level-order-traversal/)\n7. []()\n8. []()\n9. []()\n## 19) 2.6 十三\n## 20) 2.7 十四\n## 21) 2.8 十五\n## 22) 2.9 十六\n## 23) 2.10 十七\n## 24) 2.11 十八\n## 25) 2.12 十九\n## 26) 2.13 廿\n## 27) 2.14 廿一\n## 28) 2.15 廿二\n## 29) 2.16 廿三\n## 30) 2.17 廿四\n## 31) 2.18 廿五\n## 32) 2.19 廿六\n## 33) 2.20 廿七\n## 34) 2.21 廿八\n## 35) 2.22 廿九\n## 36) 2.23 二月\n## 37) 2.24 初二\n\n\n1. []()\n2. []()\n3. []()\n4. []()\n5. []()\n6. []()\n7. []()\n8. []()\n9. []()\n10. []()\n11. []()\n12. []()\n13. []()\n14. []()\n15. []()\n16. []()\n17. []()\n18. []()\n19. []()\n20. []()","categories":["Algorithms"]},{"title":"ubuntu合上电脑盖子保持状态不变","url":"/2019/12/04/ubuntu-handle-lid-switch/","content":"如果装的桌面版，在菜单里面找到设置-电源和屏幕 设置通电状态不休眠和不锁屏。\n然后打开终端（ctrl+alt+t）输入：\nsudo gedit /etc/systemd/logind.conf 找到\n```\n#HandleLidSwitch=suspend  //合上笔记本盖后的行为，默认suspend\n```\n改成 去掉“#”号：\n```\nHandleLidSwitch=ignore\n```\n重启服务 service systemd-logind restart"},{"title":"使用numpy进行矩阵操作","url":"/2019/12/02/numpy-linalg/","content":"np.linalg (= **lin**ear + **alg**ebra)\n## 求逆\n```python\nnp.linalg.inv()\n```\n## 求模\n```python\nnp.linalg.norm()\n```\n## QR分解\n```python\nq, r = np.linalg.qr(a, mode)\n```"},{"title":"经典CNN模型 - AlexNet,","url":"/2019/10/18/classic-cnn/","content":"# AlexNet\n5个卷积层，3个全连接层\n特点：\n1. 使用了非线性激活函数：ReLU\n2. 防止过拟合的方法：Dropout，数据扩充（Data augmentation）\n3. 其他：多GPU实现，LRN归一化层的使用\n\n输入input的图像规格：$224×224×3$（RGB图像），实际上经过预处理变为$227×227×3$\n\n- 卷积层C1\n    - 该层的处理流程是： 卷积-->ReLU-->池化-->归一化。\n    - 卷积，输入是$227×227$，使用96个11×11×3的卷积核，步长为4，得到的FeatureMap为55×55×96。`(227-11)/4+1=55`\n    - ReLU，将卷积层输出的FeatureMap输入到ReLU函数中。\n    - 池化，使用3×3步长为2的池化单元（重叠池化，步长小于池化单元的宽度），输出为27×27×96 `(55-3)/2+1=27`\n    - 局部响应归一化，使用k=2, n=5, $\\alpha = 10^{-4}, \\beta = 0.75$进行局部归一化，输出的仍然为$27×27×96$，输出分为两组，每组的大小为$27×28×48$\n- 卷积层C2\n    - 该层的处理流程是：卷积-->ReLU-->池化-->归一化\n    - 卷积，输入是2组27×27×48。使用2组，每组128个尺寸为5×5×48的卷积核，并作了边缘填充padding=2，卷积的步长为1. 则输出的FeatureMap为2组，每组的大小为27×27×128((27+2×2-5)/1+1=27)\n    - ReLU，将卷积层输出的FeatureMap输入到ReLU函数中\n    - 池化运算的尺寸为3×3，步长为2，池化后图像的尺寸为(27-3)/2+1=13，输出为13×13×256\n    - 局部响应归一化，使用k=2, n=5, $\\alpha = 10^{-4}, \\beta = 0.75$进行局部归一化，输出的仍然为$13×13×256$，输出分为2组，每组的大小为13×13×128\n- 卷积层C3\n    - 该层的处理流程是： 卷积-->ReLU\n    - 卷积，输入是，使用2组共384尺寸为的卷积核，做了边缘填充padding=1，卷积的步长为1.则输出的FeatureMap为\n    - ReLU，将卷积层输出的FeatureMap输入到ReLU函数中\n- 卷积层C4\n    - 该层的处理流程是： 卷积-->ReLU\n    - 该层和C3类似。\n    - 卷积，输入是，分为两组，每组为.使用2组，每组192个尺寸为的卷积核，做了边缘填充padding=1，卷积的步长为1.则输出的FeatureMap为，分为两组，每组为\n    - ReLU，将卷积层输出的FeatureMap输入到ReLU函数中\n- 卷积层C5\n    - 该层处理流程为：卷积-->ReLU-->池化\n    - 卷积，输入为，分为两组，每组为。使用2组，每组为128尺寸为的卷积核，做了边缘填充padding=1，卷积的步长为1.则输出的FeatureMap为\n    - ReLU，将卷积层输出的FeatureMap输入到ReLU函数中\n    - 池化，池化运算的尺寸为3×3，步长为2，池化后图像的尺寸为 ,即池化后的输出为\n- 全连接层FC6\n    - 该层的流程为：（卷积）全连接 -->ReLU -->Dropout\n    - 卷积->全连接： 输入为,该层有4096个卷积核，每个卷积核的大小为。由于卷积核的尺寸刚好与待处理特征图（输入）的尺寸相同，即卷积核中的每个系数只与特征图（输入）尺寸的一个像素值相乘，一一对应，因此，该层被称为全连接层。由于卷积核与特征图的尺寸相同，卷积运算后只有一个值，因此，卷积后的像素层尺寸为，即有4096个神经元。\n    - ReLU,这4096个运算结果通过ReLU激活函数生成4096个值\n    - Dropout,抑制过拟合，随机的断开某些神经元的连接或者是不激活某些神经元\n- 全连接层FC7\n    - 流程为：全连接-->ReLU-->Dropout\n    - 全连接，输入为4096的向量\n    - ReLU,这4096个运算结果通过ReLU激活函数生成4096个值\n    - Dropout,抑制过拟合，随机的断开某些神经元的连接或者是不激活某些神经元\n- 输出层\n    - 第七层输出的4096个数据与第八层的1000个神经元进行全连接，经过训练后输出1000个float型的值，这就是预测结果。"},{"title":"在ubuntu上安装并使用vscode编写tex","url":"/2019/10/11/latex-ubuntu-vscode/","content":"参考了官网安装教程[QuickInstall](https://tug.org/texlive/quickinstall.html)\n\n安装部分可以参考https://stone-zeng.github.io/2018-05-13-install-texlive-ubuntu/\n\n[texlive官网](https://tug.org/texlive/)其实说的很清楚啦，小伙伴直接去看就好，那里才是最新的。我这里只是记录下自己的安装过程下。\n# 下载\n下载地址：https://tug.org/texlive/acquire-netinstall.html\n\n因为我是ubuntu，因此只下载`install-tl-unx.tar.gz`即可\n# 安装\n解压后，首先进入install-tl脚本所在目录，运行\n```\n./install-tl \n```\n根据提示输入`i`\n\n运行命令可以带参数，这里只贴了我用到的几条，全部的可以参考上边的网址\n```\n# 使用图形界面安装，可以选择一些选项\ninstall-tl -gui\n\n# 使用已经设置好的配置安装，这里因为我第一次安装中途失败了，所以第二次直接使用上次选择的配置即可\ninstall-tl --profile=profile \n```\n如果中途安装失败，可以先用这两条命令清理一下\n```bash\nrm -rf /usr/local/texlive/2019\nrm -rf ~/.texlive2019\n```\n下好后记得要配置环境变量\n```bash\ngedit ~/.bashrc\n```\n在最后加入这几行：\n```bashrc\nexport PATH=/usr/local/texlive/2019/bin/x86_64-linux:$PATH\nexport MANPATH=/usr/local/texlive/2019/texmf-dist/doc/man:$MANPATH\nexport INFOPATH=/usr/local/texlive/2019/texmf-dist/doc/info:$INFOPATH\n```\n# 配置vscode\n安装插件LaTeX Workshop，扩展商店搜索latex，排在最上、下载数最多的就是了\n","tags":["LaTeX"],"categories":["Linux"]},{"title":"拉格朗日乘子法","url":"/2019/10/10/Lagrange-Multiplier/","content":"在数学中的最优化问题中，拉格朗日乘数法是一种**寻找`多元函数`在其`变量受到一个或多个条件的约束时`的极值**的方法。这种方法可以将一个有`n个变量`与`k个约束条件`的最优化问题转换为一个`解有n + k个变量的方程组的解`的问题。这种方法中引入了一个或一组新的未知数，即拉格朗日乘数，又称拉格朗日乘子，或拉氏乘子，它们是在转换后的方程，即约束方程中作为`梯度（gradient）的线性组合`中各个向量的**系数**。"},{"title":"Fisher线性鉴别","url":"/2019/10/09/Fisher-s-linear-discriminant/","content":"# fishter原理\n费歇（FISHER）判别思想是投影，使多维问题简化为一维问题来处理。选择一个适当的投影轴,使所有的样品点都投影到这个轴上得到一个投影值。对这个投影轴的方向的要求是：使每一类内的投影值所形成的类内离差尽可能小，而不同类间的投影值所形成的类间离差尽可能大。\n\n# 目标\n寻找某一w,使得D={(x_i, y_i}投影后，类别内类距离最小，类别之间距离最大。\n\n严格来说LDA与Fisher判别分析稍有不同，前者假设了各类样本的协方差矩阵相同且满秩。"},{"title":"双系统下让ubuntu开机自动挂载windows磁盘","url":"/2019/10/08/Automatically-mount/","content":"https://zhuanlan.zhihu.com/p/47455239\n","categories":["Linux"]},{"title":"LU Factorization","url":"/2019/10/05/LU-Factorization/","content":"# LU 分解的前提\n并非所有矩阵都能进行LU分解，能够LU分解的矩阵需要满足以下三个条件：\n1. 矩阵是方阵（LU分解主要是针对方阵）；\n2. 矩阵是可逆的，也就是该矩阵是满秩矩阵，每一行都是独立向量；\n3. 消元过程中没有0主元出现，也就是消元过程中不能出现行交换的初等变换。"},{"title":"10 interesting python cool tricks","url":"/2019/10/03/10-interesting-python-cool-tricks/","content":"(封面是二乔～99岁生日快乐呀老头子~\\(≧▽≦)/~)\n转载自 https://www.tutorialspoint.com/10-interesting-python-cool-tricks\n\nWith increase in popularity of python, more and more features are becoming available for python coding. Using this features makes writing the code in fewer lines and cleaner. In this article we will see 10 such python tricks which are very frequently used and most useful.\n\n# 1. Reversing a List 反转列表\nWe can simply reverse a given list by using a reverse() function. It handles both numeric and string data types present in the list.\n**Example**\n```python\nList = [\"Shriya\", \"Lavina\",\"Sampreeti\" ]\nList.reverse()\nprint(List)\n```\n**Output**\nRunning the above code gives us the following result −\n```python\n['Sampreeti', 'Lavina', 'Shriya']\n```\n# 2. Print list elements in any order 以任意顺序打印列表元素\nIf you need to print the values of a list in different orders, you can assign the list to a series of variables and programmatically decide the order in which you want to print the list.\n\n**Example**\n```python\nList = [1,2,3]\nw, v, t = List\nprint(v, w, t )\nprint(t, v, w )\n```\n**Output**\nRunning the above code gives us the following result −\n```python\n(2, 1, 3)\n(3, 2, 1)\n```\n# 3. Using Generators Inside Functions 使用生成器\nWe can use generators directly inside a function to writer shorter and cleaner code. In the below example we find the sum using a generator directly as an argument to the sum function.\n\n**Example**\n```python\nsum(i for i in range(10) )\n```\n**Output**\nRunning the above code gives us the following result −\n```python\n45\n```\n# 4. Using the zip() function \nWhen we need to join many iterator objects like lists to get a single list we can use the zip function. The result shows each item to be grouped with their respective items from the other lists.\n\n**Example**\n```python\nYear = (1999, 2003, 2011, 2017)\nMonth = (\"Mar\", \"Jun\", \"Jan\", \"Dec\")\nDay = (11,21,13,5)\nprint zip(Year,Month,Day)\n```\n**Output**\nRunning the above code gives us the following result −\n```python\n[(1999, 'Mar', 11), (2003, 'Jun', 21), (2011, 'Jan', 13), (2017, 'Dec', 5)]\n```\n# 5. Swap two numbers using a single line of code 交换\nSwapping of numbers usually requires storing of values in temporary variables. But with this python trick we can do that using one line of code and without using any temporary variables.\n\n**Example**\n```python\nx,y = 11, 34\nprint x\nprint y\nx,y = y,x\nprint x\nprint y\n```\n**Output**\nRunning the above code gives us the following result −\n```python\n11\n34\n34\n11\n```\n# 6. Transpose a Matrix 转置矩阵\nTransposing a matrix involves converting columns into rows. In python we can achieve it by designing some loop structure to iterate through the elements in the matrix and change their places or we can use the following script involving zip function in conjunction with the * operator to unzip a list which becomes a transpose of the given matrix.\n\n**Example**\n```python\nx = [[31,17],\n[40 ,51],\n[13 ,12]]\nprint (zip(*x))\n```\n**Output**\nRunning the above code gives us the following result −\n```python\n[(31, 40, 13), (17, 51, 12)]\n```\n*可以理解为“解包”，即将列表中的元素取出来操作\n\n`zip` wants a bunch of arguments to zip together, but what you have is a single argument (a list, whose elements are also lists). The * in a function call \"unpacks\" a list (or other iterable), making each of its elements a separate argument. So without the *, you're doing zip( [[1,2,3],[4,5,6]] ). With the *, you're doing zip([1,2,3], [4,5,6])\n# 7. Print a string N Times 打印一个字符串n次\nThe usual approach in any programming language to print a string multiple times is to design a loop. But python has a simple trick involving a string and a number inside the print function.\n\n**Example**\n```python\nstr =\"Point\";\nprint(str * 3);\n```\n**Output**\nRunning the above code gives us the following result −\n```python\nPointPointPoint\n```\n# 8. Reversing List Elements Using List Slicing 使用切片翻转列表元素\nList slicing is a very powerful technique in python which can also be used to reverse the order of elements in a list.\n\n**Example**\n```python\n#Reversing Strings\nlist1 = [\"a\",\"b\",\"c\",\"d\"]\nprint list1[::-1]\n\n# Reversing Numbers\nlist2 = [1,3,6,4,2]\nprint list2[::-1]\n```\n**Output**\nRunning the above code gives us the following result −\n```python\n['d', 'c', 'b', 'a']\n[2, 4, 6, 3, 1]\n```\n\n切片[开始：结束：步长]\n```python\nprint(a)\n[ 0.64061262  0.8451399   0.965673    0.89256687  0.48518743]\n \nprint(a[-1]) ###取最后一个元素\n[0.48518743]\n \nprint(a[:-1])  ### 除了最后一个取全部\n[ 0.64061262  0.8451399   0.965673    0.89256687]\n \nprint(a[::-1]) ### 取从后向前（相反）的元素\n[ 0.48518743  0.89256687  0.965673    0.8451399   0.64061262]\n \nprint(a[2::-1]) ### 取从下标为2的元素翻转读取\n[ 0.965673  0.8451399   0.64061262]\n```\n# 9. Find the Factors of a Number 找因子\nWhen we are need of the factors of a number, required for some calculation or analysis, we can design a small loop which will check the divisibility of that number with the iteration index.\n\n**Example**\n```python\nf = 32\nprint \"The factors of\",x,\"are:\"\nfor i in range(1, f + 1):\n   if f % i == 0:\nprint(i)\n```\n**Output**\nRunning the above code gives us the following result −\n```python\nThe factors of 32 are:\n1\n2\n4\n8\n16\n32\n```\n# 10. Checking the Usage of Memory 检查存储占用\nWe can check the amount of memory consumed by each variable that we declare by using the getsizeof() function. As you can see below, different string lengths will consume different amount of memory.\n\n**Example**\n```python\nimport sys\na, b, c,d = \"abcde\" ,\"xy\", 2, 15.06\nprint(sys.getsizeof(a))\nprint(sys.getsizeof(b))\nprint(sys.getsizeof(c))\nprint(sys.getsizeof(d))\n```\n**Output**\nRunning the above code gives us the following result −\n```python\n38\n35\n24\n24\n```","tags":["python"],"categories":["basis"]},{"title":"how to be pythonic","url":"/2019/10/02/how-to-be-pythonic/","content":"0. 程序必须先让人读懂，然后才能让计算机执行。“Programs must be written for people to read, and only incidentally for machines to execute.”\n\n1. 交换赋值\n```python\n##不推荐\ntemp = a\na = b\nb = a \n##推荐\na, b = b, a # 先生成一个元组(tuple)对象，然后unpack\n```\n2. Unpacking##不推荐\n```python\nl = ['David', 'Pythonista', '+1-514-555-1234']\nfirst_name = l[0]\nlast_name = l[1]\nphone_number = l[2] \n##推荐\nl = ['David', 'Pythonista', '+1-514-555-1234']\nfirst_name, last_name, phone_number = l\n# Python 3 Only\nfirst, *middle, last = another_list\n```\n3. 使用操作符in\n```python\n##不推荐\nif fruit == \"apple\" or fruit == \"orange\" or fruit == \"berry\":\n# 多次判断 \n##推荐\nif fruit in [\"apple\", \"orange\", \"berry\"]:\n # 使用 in 更加简洁\n ```\n4. 字符串操作\n```python\n##不推荐\ncolors = ['red', 'blue', 'green', 'yellow']\nresult = ''\nfor s in colors:\n result += s # 每次赋值都丢弃以前的字符串对象, 生成一个新对象 \n##推荐\ncolors = ['red', 'blue', 'green', 'yellow']\nresult = ''.join(colors) # 没有额外的内存分配\n```\n5. 字典键值列表\n```python\n##不推荐\nfor key in my_dict.keys():\n # my_dict[key] ... \n##推荐\nfor key in my_dict:\n # my_dict[key] ...\n# 只有当循环中需要更改key值的情况下，我们需要使用 my_dict.keys()\n# 生成静态的键值列表。\n```\n6. 字典键值判断\n```python\n##不推荐\nif my_dict.has_key(key):\n # ...do something with d[key] \n##推荐\nif key in my_dict:\n # ...do something with d[key]\n ```\n7. 字典 get 和 setdefault 方法\n```python\n##不推荐\nnavs = {}\nfor (portfolio, equity, position) in data:\n if portfolio not in navs:\n navs[portfolio] = 0\n navs[portfolio] += position * prices[equity]\n##推荐\nnavs = {}\nfor (portfolio, equity, position) in data:\n # 使用 get 方法\n navs[portfolio] = navs.get(portfolio, 0) + position * prices[equity]\n # 或者使用 setdefault 方法\n navs.setdefault(portfolio, 0)\n navs[portfolio] += position * prices[equity]\n ```\n8. 判断真伪\n```python\n##不推荐\nif x == True:\n # ....\nif len(items) != 0:\n # ...\nif items != []:\n # ... \n##推荐\nif x:\n # ....\nif items:\n # ...\n ```\n9. 遍历列表以及索引\n```python\n##不推荐\nitems = 'zero one two three'.split()\n# method 1\ni = 0\nfor item in items:\n print i, item\n i += 1\n# method 2\nfor i in range(len(items)):\n print i, items[i]\n##推荐\nitems = 'zero one two three'.split()\nfor i, item in enumerate(items):\n print i, item\n ```\n10. 列表推导\n```python\n##不推荐\nnew_list = []\nfor item in a_list:\n if condition(item):\n new_list.append(fn(item)) \n##推荐\nnew_list = [fn(item) for item in a_list if condition(item)]\n```\n11. 列表推导-嵌套\n```python\n##不推荐\nfor sub_list in nested_list:\n if list_condition(sub_list):\n for item in sub_list:\n if item_condition(item):\n # do something... \n##推荐\ngen = (item for sl in nested_list if list_condition(sl) \n for item in sl if item_condition(item))\nfor item in gen:\n # do something...\n ```\n12. 循环嵌套\n```python\n##不推荐\nfor x in x_list:\n for y in y_list:\n for z in z_list:\n # do something for x & y \n##推荐\nfrom itertools import product\nfor x, y, z in product(x_list, y_list, z_list):\n # do something for x, y, z\n ```\n13. 尽量使用生成器代替列表\n```python\n##不推荐\ndef my_range(n):\n i = 0\n result = []\n while i < n:\n result.append(fn(i))\n i += 1\n return result # 返回列表\n##推荐\ndef my_range(n):\n i = 0\n result = []\n while i < n:\n yield fn(i) # 使用生成器代替列表\n i += 1\n# 尽量用生成器代替列表，除非必须用到列表特有的函数。\n```\n14. python中间结果尽量使用imap/ifilter代替map/filter\n```\n##不推荐\nreduce(rf, filter(ff, map(mf, a_list)))\n##推荐\nfrom itertools import ifilter, imap\nreduce(rf, ifilter(ff, imap(mf, a_list)))\n# lazy evaluation 会带来更高的内存使用效率，特别是当处理大数据操作的时候。\n```\n15. 使用any/all函数\n```python\n##不推荐\nfound = False\nfor item in a_list:\n if condition(item):\n found = True\n break\nif found:\n # do something if found... \n##推荐\nif any(condition(item) for item in a_list):\n # do something if found...\n ```\n16. 属性(property)\n```python\n##不推荐\nclass Clock(object):\n def __init__(self):\n self.__hour = 1\n def setHour(self, hour):\n if 25 > hour > 0: self.__hour = hour\n else: raise BadHourException\n def getHour(self):\n return self.__hour\n##推荐\nclass Clock(object):\n def __init__(self):\n self.__hour = 1\n def __setHour(self, hour):\n if 25 > hour > 0: self.__hour = hour\n else: raise BadHourException\n def __getHour(self):\n return self.__hour\n hour = property(__getHour, __setHour)\n ```\n17. 使用 with 处理文件打开\n```python\n##不推荐\nf = open(\"some_file.txt\")\ntry:\n data = f.read()\n # 其他文件操作..\nfinally:\n f.close()\n##推荐\nwith open(\"some_file.txt\") as f:\n data = f.read()\n # 其他文件操作...\n ```\n18. 使用 with 忽视异常(仅限Python 3)\n```python\n##不推荐\ntry:\n os.remove(\"somefile.txt\")\nexcept OSError:\n pass\n##推荐\nfrom contextlib import ignored # Python 3 only\nwith ignored(OSError):\n os.remove(\"somefile.txt\")\n ```\n19. 使用 with 处理加锁\n```python\n##不推荐\nimport threading\nlock = threading.Lock()\nlock.acquire()\ntry:\n # 互斥操作...\nfinally:\n lock.release()\n##推荐\nimport threading\nlock = threading.Lock()\nwith lock:\n # 互斥操作...\n```\n\n作者：python小南瓜\n\n链接：https://zhuanlan.zhihu.com/p/73163063\n","categories":["basis"]},{"title":"在hexo上使用mathjax展示公式(非NexT主题)","url":"/2019/10/02/hexo-with-mathjax/","content":"为了在博客上敲公式真是踩了好多坑...心累_(:з」∠)_\n好在终于完美解决啦\n在vscode上敲公式看预览还美滋滋，上传上来人都傻了hhh，感觉找解决方法。看起来NexT主题已经有很完善的解决方案了，不过我用的[hexo-casper](https://github.com/xzhih/hexo-theme-casper)主题(非常美貌！吹爆！！)，作者为了主题的轻便没有加入mathjax支持，就只能自己动手，丰衣足食啦。\n\nhexo-math, hexo-renderer-mathjax, hexo-renderer-kramed这些我都试过，也根据网上教程改了各种配置。其中只装hexo-math时，独立一行的公式`$$...$$`是可以正常显示的，但是行内公式只能用`\\\\(,\\\\)`如果用`$...$`就不能渲染，这太不符合我平时的使用习惯了...\n\n这里可以参考hexo-math的[issues 26](https://github.com/hexojs/hexo-math/issues/26)，有非常丰富的讨论，也有很多热心的小伙伴给出了可行的解决方案，可惜我不是幸运儿那些方案对我都没用TAT \n\n废话了这么多，还是上最后的解决方法：\n ```JavaScript\n <script type=\"text/x-mathjax-config\">\nMathJax.Hub.Config({\n  tex2jax: {inlineMath: [['$','$'], ['\\\\(','\\\\)']]}\n});\n</script>\n<script type=\"text/javascript\" async src=\"path-to-mathjax/MathJax.js?config=TeX-AMS_CHTML\"></script>\n ```\n这里最后一行path-to-mathjax是指js文件的路径，我用的是cdn地址`https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML`，直接替换上面的src中的内容即可\n ```JavaScript\n <script type=\"text/x-mathjax-config\">\nMathJax.Hub.Config({\n  tex2jax: {inlineMath: [['$','$'], ['\\\\(','\\\\)']]}\n});\n</script>\n<script type=\"text/javascript\" async src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML\"></script>\n ```\n\n 将这一部分追加到theme里配置js的文件中，以hexo-theme-casper为例，是themes/hexo-casper/layout/_layout.swig\n\n 然后 hexo clean, hexo g, hexo s或者d，就可以看到效果啦～\n\n 参考资料：[在Hexo使用Latex写公式](https://geniusoridiot.github.io/2018/05/16/UsingLatexInHexo/)"},{"title":"git代理","url":"/2019/10/01/gitwithsocks/","content":"设置代理\n```shell\ngit config --global http.proxy http://127.0.0.1:1080\ngit config --global https.proxy https://127.0.0.1:1080\ngit config --global http.proxy 'socks5://127.0.0.1:1080' \ngit config --global https.proxy 'socks5://127.0.0.1:1080'\n```\n取消代理\n```shell\ngit config --global --unset http.proxy\ngit config --global --unset https.proxy\n```\n只对github设置代理\n```shell\ngit config --global http.https://github.com.proxy socks5://127.0.0.1:1080 \n```\n取消github代理\n```shell\ngit config --global --unset http.https://github.com.proxy   \n```"},{"title":"Personalized Dialogue Generation with Diversified Traits","url":"/2019/09/26/Personalized-Dialogue-Generation-with-Diversified-Traits/","content":"## 0.Abstract\n*是否可以考虑嵌入隐式特征？*\n\n本文使用的数据库中特征包括年龄、性别、位置、兴趣、标签(Tags)等等\n\n提出：seq2seq框架中的性格感知对话生成模型\n\n显式性格特征用键值对构成，解码过程中用`性格感知注意力 persona-aware attention`和`性格感知偏置 persona-aware bias`来捕捉并处理与特征有关的信息。\n## 1.Introduction\n\n### 总结过去\n性格设置包括年龄、性别、语言、说法风格、知识水平、专业领域甚至合适的口音。\n\n个性化可分为显式和隐式\n\n显式：用户用个人画像（键值对）或文字描述的性格 依赖手工标注的数据或众包对话，因此对于大规模数据集不具备扩展性\n\n隐式：用户用向量表示 简单成功/可解释性差；数据稀疏\n### 自己的工作\n性格可视为多种性格特征的综合\n\n一个个性化的对话agent应该拥有多种特性，并能够决定在不同的语境中应该表现哪种性格。\n\n- 每个说话者的性格用许多性格特征代表，性格特征用键值对的形式给出，特征值相同（例如都是女性）可以共享特征表示（trait representations）\n\n- the use of such persona information can be captured implicitly by data-driven methods that are scalable to large-scale corpora.这些性格信息的使用可以用可扩展的数据驱动的方法隐式捕捉\n\n- 研究性格特征如何在语言表达中表示出来。\n\n1. 每个说话者的特征编码成嵌入向量，不同特征合并来产生性格表示\n2. 产生表示的过程中应用了两种方法：性格感知注意力机制（性格表示用于产生注意力权重来在每一个解码位置获取context vector），性格感知偏置（估计词的生成分布）\n\nWe propose persona-aware models which apply a trait fusion module in the encoder-decoder framework to capture and address personality traits in dialogue generation. We devise a persona-aware attention mechanism and persona-aware bias to incorporate the persona information in the decoding process.  Experiments demonstrate that our model is able to address proper traits in different contexts.我们提出了一种基于特征融合模块的角色感知模型，该模型采用编解码器框架中的特征融合模块来捕获和处理对话生成中的性格特征。我们设计了一种角色感知注意机制和角色感知偏见来将角色信息融入解码过程。实验表明，我们的模型能够在不同的环境下处理合适的特征。\n## 2.Related Works\n*建模电影角色：3，8;隐式+神经模型:20,23,31,45,46,48;显式：26,47*\n\n3 A Movie Dialogue Corpus for Research and Development. ACL 2012\n\n8 Chameleons in imagined conversations: A new approach to understanding coordination of linguistic style in dialogs. ACL 2011\n\n\n20 Exploring Personalized Neural Conversational Models, IJCAI 2017\n\n23 A Persona-Based Neural Conversation Model, Li Jiwei, 2016\n\n31 Addressee and response selection for multiparty\nconversation, EMNLP 2016\n\n45 Group Linguistic Bias Aware Neural Response Generation. 2017 Workshop Ijcnlp\n\n46 Addressee and response selection in multi-party conversations with speaker interaction rnns, arXiv 2017\n\n48 Neural personalized response generation as domain adaptation, WWW 2017\n\n\n26 Training Millions of Personalized Dialogue Agents, 2018 arXiv\n\n47 Personalizing Dialogue Agents: I have a dog, do you have pets too? ACL 2018\n\n## 3.Model\n*关系的亲疏？*\n在seq2seq中配备一个性格特征融合模块 a personality trait fusion module，用它计算一个整体的性格表示$v_p$, 然后令其作用于解码过程。\n两种利用$v_p$解码的方法： \n1. persona-aware attention mechanism 可识别人格的注意力机制\n2. persona-aware bias 可识别人格的偏置机制\n\n### 3.1 Task Definition and Overview\n$$Y* = \\mathop{\\arg\\max}_{Y} P(Y|X,T)$$\nT是特征集，其中每个$t_i$是键值对\n三种用于生成人物角色表示$v_p$的性格特征融合方法，两种将$v_p$引入解码过程的方法。（三种编码方法，两种解码方法）\n解码：\n1. 使用$v_p$在每个解码位置生成注意力权重，使得在每个位置计算的context vector受$v_p$的约束\n2. 直接在估计生成分布时使用能识别人格的bias\n\n### 3.2 seq2seq\n带attention\nencoder:两层双向GRU；decoder:两层GRU\n\n### 3.3 编码 Personality Trait Fusion\n1. 每个特征->representation vector $v_{t_i}$\n2. 所有特征融合成$v_p$。特征表示集合${v_{t_1} ,v_{t_2}, ...,v_{t_N} }$使用`人格特征融合函数`表示成persona representation $v_p$\n我们首先计算一个整体的性格表示$v_p$, 然后令其作用于解码过程。\n$v_p$是特征融合，因此要先表示出每一个特征。$v_p$的构建开始于将每个特点$t_i$使用相应的特征编码器(corresponding trait encoder)映射为一个嵌入表示$v_{t_i}$。本文考虑的特征全部都是单值的single-valued，故可以直接查表look-up tables.\n其它类型特征e.g.一个个人介绍句子：可以用LSTM\n\n使用了三种不同的合并方法：\n1. Traits Attention 权重\n2. Traits Average 平均\n   $$v_p = \\frac{1}{N} \\sum^N_{i=1}v_{t_i}$$\n   是所有特征权重相等的attention特例。\n3. Traits Concatenation 串联\n   $v_p$长度(=$d_p$)要能被$N$整除,每个特征的向量表示$v_{t_i}$是$d_p/N$\n\n### 3.4 解码 Decoding with Persona Representation\n\n#### 3.4.1 Persona-Aware Attention(PAA)\n扩展了decoder中注意力权重的计算，不仅依赖于decoder的状态，还依赖于人格表示$v_p$\n$$\n\\begin{align}\ne_i &= MLP(s_{t-1},h_i,v_p) &= V·tanh(W^1_{\\alpha}s_{t-1}+W^2_{\\alpha}h_i+W^3_{\\alpha}v_p)\n\\end{align}\n$$\n其中$V \\in \\mathbb{R}^{d_s},W_{\\alpha}^1 \\in \\mathbb{R}^{d_s × d_s},W_{\\alpha}^2 \\in \\mathbb{R}^{d_s × d_s},W_{\\alpha}^3 \\in \\mathbb{R}^{d_s × d_p}$是可学习的参数。$e_i$是计算注意力权重时softmax的输入。\n#### 3.4.2 Persona-Aware Bias(PAB)\n原始seq2seq的输出层：\n$$\ny_t = softmax(W^1_os_t+b_{out})\\\\\ns_t=RNN(S_{t-1},c_t,w_{t-1})\n$$\n尝试在decoder输出层包含$v_p$。特别的，将人格偏置包含上面的公式中来获取生成分布。设计一个门gate来平衡原始项和人格偏置项。\n$$\ny_t = softmax(a_t · W_o^1s_t +(1-a_t)·W_o^2v_p + b_{out})\\\\\na_t=\\sigma(V^T_o·s_t)\n$$\n其中$w^1_o \\in \\mathbb{R}^{|V|× d_s},W^2_o \\in \\mathbb{R}^{|V|× d_p},V_o \\in \\mathbb{R}^{d_s},b_{out} \\in \\mathbb{R}^{|V|}$是可学习的参数。\n尽管$v_p$带来的偏置看起来是上下文独立的（可能会在每个解码步独立选择词语），计算得的scalar variable $a_t \\in [0,1]$起到门的作用，控制多少人格相关的特征可以包含在每个时间步里。它可以决定是使用特征相关词语trait related word还是语义相关词语semantic related word，并因此作出一致的回复。\nPAB在影响生成分布时更为直接，且效果通常好于PAA。《Low-Rank RNN Adaptation for Context-Aware Language Modeling》《Emotional chatting machine: Emotional conversation generation with internal and external memory》（ECM）使用了相似的模型结构。\n## 4.`PERSONALDIALOG`数据集\n## 5.\n*joint learning 与（从语句中抽取属性）联合训练*\n*关系会演进，而不是一成不变~感情会变-emotion~人工智能是万能的而人不是（考虑百科词典的例子）——情感何时转换？*","categories":["Dialogue System"]},{"title":"为什么信息熵要定义成log的形式","url":"/2019/09/25/entropy-and-log/","content":"如果我们有俩个不相关的事件x和y，那么我们观察到的俩个事件同时发生时获得的信息应该等于观察到的事件各自发生时获得的信息之和，即：h(x,y) = h(x) + h(y)由于x，y是俩个不相关的事件，那么满足p(x,y) = p(x)*p(y).根据上面推导，我们很容易看出h(x)一定与p(x)的对数有关（因为只有对数形式的真数相乘之后，能够对应对数的相加形式，可以试试）。因此我们有信息量公式如下：\n\n作者：忆臻\n链接：https://www.zhihu.com/question/30828247/answer/160647576\n来源：知乎\n著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。"},{"title":"Target-Guided Open-Domain Conversation","url":"/2019/09/24/Target-Guided-Open-Domain-Conversation/","content":"ACL 2019\n\n## 任务：\n在开放域上自然地聊天，并主动将对话引导到预先设定好的目标话题上\n\n## 方法：\n将整个系统解耦成独立的模块，在不同粒度上分别解决这些挑战。\n\n## 要求：\n1. transition smoothness 转换流畅，引导过程切合当前对话上下文\n\n2. target achievement 达成目标\n\n1. a turn-level keyword transition predictor (section 4.1)\n用于预测下一句回复的关键词，获得其分布\n\n这一部分主要用于生成流畅的对话，而不考虑引导对话主题\n\n文章使用了三种方法:\n- Parawise PMI-based Transition 点互信息\n- 基于神经网络预测\n- 基于核的混合方法 \n第三种效果最佳\n\n2. a discourse-level target-guiding strategy (section 4.2)\n用于完成主动将对话主题引导到目标的任务。即选择下一句生成回复的主题\n\n要求：\n\n- 相关度更高\n- 离目标更近\n\n\n使用基于规则的方法\n\n关键在于the augmentation of interpretable coarse-grained keywords 粗粒度关键词的可解释性的提升\n\n\n3. Keyword-augmented response retriever (section 4.3).\n目标：基于对话历史和预测关键词生成回复\n\n使用检索式方法\n\n用RNN编码输入对话历史和关键词，以及从数据库中得到的候选回复，然后计算候选特征与历史特征、候选特征与关键词特征(feature)之间的element-wise乘积。拼接得到的两个向量，输入到带sigmoid单个单元(single-unit)的全连接层来获取候选回复的匹配概率值\n\n## 数据集\n## 实验\n## 总结\ntbc."},{"title":"GitWithProxy","url":"/2019/09/21/GitWithProxy/","content":"git config --global https.proxy http://127.0.0.1:1080\n\ngit config --global https.proxy https://127.0.0.1:1080\n\ngit config --global --unset http.proxy\n\ngit config --global --unset https.proxy\n"},{"title":"如何读论文","url":"/2019/09/21/HowToReadPapers/","content":"作者：铁头娃转行AI\n链接：https://www.zhihu.com/question/304334959/answer/827275515\n来源：知乎\n著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。\n\n### 读研究论文\n你如何通过阅读研究论文来高效和相对快速地学习？所以，如果你想从学术文献中学习，你应该做什么，无论是你想学习建立一个感兴趣的机器学习系统/项目，还是仅仅停留在事情的顶端，获得更多的知识，成为一个深入学习的人。\n\n以下是清单：\n\n1. **编写一份论文列表**：尝试创建一份研究论文列表、包括你拥有的任何文本或学习资源。\n2. **过一遍列表**：基本上，你应该以一种并行的方式阅读研究论文，意思是一次处理多篇论文。具体地说，试着快速浏览并理解每一篇文章，而不是全部读完，也许你读了每一篇文章的10-20%，也许这足以让你对手头的文章有一个高水平的理解。在那之后，你可能会决定删除其中的一些论文，或者只是浏览一两篇论文，把它们通读一遍。\n\n他还提到，如果你读到：\n\n**5-20篇论文**(在选择的领域，比如语音识别)=>这可能是足够的知识，你可以实现一个语音识别系统，但可能不够研究或让你处于前沿。\n\n**50-100篇论文**=>你可能会对这个领域的应用(语音识别)有很好的理解。\n\n### 如何读论文？\n不要从头读到尾。相反，**需要多次遍历论文**，下面是具体如何做的：\n\n1. **阅读文章标题、摘要和图**：通过阅读文章标题、摘要、关键网络架构图，或许还有实验部分，你将能够对论文的概念有一个大致的了解。在深度学习中，有很多研究论文都是将整篇论文总结成一两个图形，而不需要费力地通读全文。\n2. **读介绍+结论+图+略过其他**：介绍、结论和摘要是作者试图仔细总结自己工作的地方，以便向审稿人阐明为什么他们的论文应该被接受发表。\n\n此外，略过相关的工作部分(如果可能的话)，这部分的目的是突出其他人所做的工作，这些工作在某种程度上与作者的工作有关。因此，阅读它可能是有用的，但如果你不熟悉这个主题，有时很难理解。\n\n3. **通读全文，但跳过数学部分**。\n4. **通读全文，但略过没有意义的部分**：出色的研究意味着我们发表的东西是在我们的知识和理解的边界上。\n\n他还解释说，当你阅读论文时(即使是最有影响力的论文)，你可能也会发现有些部分没什么用，或者没什么意义。因此，如果你读了一篇论文，其中一些内容没有意义(这并不罕见)，那么你可以先略读。除非你想要掌握它，那就花更多的时间。\n\n当你阅读一篇论文时，试着回答以下问题：\n\n1. **作者试图完成什么**\n2. **这个方法的关键要素是什么**\n3. **你自己能做什么**\n4. **你还想要什么其他的参考资料**\n\n如果你能回答这些问题，就很有希望的能反映出你对论文有很好的理解。\n\n事实证明，当你读更多的论文时，通过练习你会变得更快。因为，很多作者在写论文时使用的是通用格式。"},{"title":"Generating Responses with a Specific Emotion in Dialog","url":"/2019/08/13/Generating Responses with a Specific Emotion in Dialog/","content":"\n\n# Generating Responses with a Specific Emotion in Dialog\n## 0 Abstract\n用强烈或含蓄的方式表达感情\n有意义且结构连贯\n## 1 Introduction\nat least two ways to put feelings into words\n两种表达感情的方法\none is to describe emotional states by explicitly using strong emotional words associated with the categories一种是显式的使用强烈的感情词语\nanother is to increase the intensify of the emotional experiences not by using words i emotion lexicon, bu ty implicitly combining neutral words in distinct ways on emotion.\n另一种是含蓄地结合中性词汇表达情感\nseq2seq框架，基于词典的注意力机制拓展，机制鼓励用情感词典中的同义词代替回复中的词\n回复的生成过程由序列级的情感分类器指导，它帮助增强情感表达的强度，识别情感句子中是否包含情感词\n半监督方法 创建情感词典，相对准确地表示情感状态\n\n## 2 \n## 3 Method\n(1) seq2seq framework\n(2) lexicon-based attention mechanism：plug in the desired emotional words 插入需要的情感词汇\n(3) sequence-level emotion classifier：recognize the emotional sentences without any emotional word 识别没有情感词汇的情感句子\n(4) diverse decoding algorithm: foster diversity in response generation 回复生成多样性\n(5) semi-supervised method: produce an emotion lexicon that can properly represent the mental perceptions of the emotional states 产生一个情感词汇，能够恰当地表示对情感状态的心理感知\n\n### 3.1 Problem Definition\n输入：\npost `$X = {x_1,x_2,...,x_M}$`\nemotion category \\\\(e\\\\) \n目标：\n生成response `$ Y = {y_1, y_2,...,y_N} $` ，在意义和情感上都合适\n其中 $$x_i \\in V$$ 和$y_i \\in V$分别是post和response中的词语\n$M,N$分别是post和response的长度，\n$V=V_g \\cup V_e$是词典，包含了通用词典$V_g$和情感和一个情感词语$V_e$。$V_g \\cap V_e = \\emptyset$ \n$V_e$可以进一步分为几个子集$V^z_e$,每一个保存了有关情感分类$z$的词语。\n### 3.2 Dialogue System with Lexicon-based Attention Mechanism 基于词典的注意力机制\n基于seq2seq框架\nlexicon-based attention mechanism: \"plug\" emotional words into the generated texts at the right time steps 在正确的时间步将情感词语插入到生成的文本中\n**encoder**: 双向LSTM，将输入转化为向量表示\n**docoder**: a separate LSTM enhanced with a lexicon-based attention mechanism 带基于lexicon的注意力机制的单个LSTM \n输入： 之前预测的词语$y_{j-1}$和情感向量$e_j$ 来更新隐状态；$y_{j-1}$的嵌入和$e_j$串接(concatenate)\n情感向量$e_j$是对于给定的类别$z$, $V^z_e$中词语的加权和\n（3）\n对于$V^z_e$中的每个情感词语$w^z_k$来说，每个时间步$j$的注意力分数$a_{jk}$由三部分决定：\n- decoder的前一个隐藏状态$s_{j-1}$\n- 输入post的编码表示$h_M$\n- $V^z_e$中第k个词的嵌入$Emb(w^z_k)$\n### 3.3 Emotion Classification\nsequence-level emotion classifier 序列级情感分类器，识别传递特定情感但不包含情感词语的回复\nexpected word embedding 期望词嵌入 每个时间步所有可能的词的加权和\n\n引入的情感分类器不仅能够增加情绪表达的强度，还可以识别情感回复中是否包含情感词语。情感分类器只用在训练过程中，可以作为情感表达的全局指导\n\n### 3.4 Training Objective\n由generation与classification 生成与分类 两部分组成\n生成损失$L_{MCE}$确保decoder可以产生结构连贯有意义的回复\n分类损失$L_{CLA}$确保在产生的回复中特定情感可以合适的传达\n### 3.5 Diverse Decoding Algorithm\n集束搜索beam search生成的结果大多相似\n强制开头的词语of N-candidates必须不同，然后在开头的词语确定后，继续用贪心解码策略生成回复。最后从最好的N个候选中选择感情分数最高的。\n### 3.6 Emotion Lexicon Construction\n如何用半监督方法从包含了句子和对应情感分类的语料库中建立需要的情感词典\nThe meaning of words is rated on a number of different bipolar adjective scales.单词的意思是用一些不同的双极性形容词量表来评定的。\n对一个感情类别，每个词表示为$w=(p_w,n_w)$，其中$p_w$表示指定类别的概率，$n_w$表示另一个，即positiive和negative.\n给定n个词组成的序列的句子s，\n如果句子表示了情感，则标记为二维情感向量$z=(1,0)$，否则则标记为$z=(0,1)$.。每个词用一个小的随机值初始化，并最小化交叉熵___损失来训练，其中m是语料库中句子的数目。\n移除所有的的停止词，映射识别的“digit数字”“E-mail电子邮件”“URL”“data日期”“foreign word外来词”到特殊符号。\n跟在否定词后面的词在生成感情向量前被转化成 $(-p_w;-n_w)$\n如果词语是最高级或比较级修饰的形容词或副词，更新表示时学习率的值会相应变为两倍或三倍\n训练过程可以被分为两个阶段，第一阶段使用标准反向传播；当预测accuracy大于一个预定的阈值(如90%)时，进入第二阶段，开始使用maximum margin learning strategy，直到达到收敛。在训练结束时，计算平均值$v=\\frac{1}{n}\\sum^n_{i=1}(p_w-n_w)$以及方差$\\sigma$值$\\frac{1}{\\sigma}(p_w-n_w-v)$大于特定阈值的词将会被视作情感词语。\n\n## 4.Experiments\n### 4.1 Data Preparation\n目前没有现成的感情的对话数据，所以我们给予Short Text Conversation (STC)数据集建立了自己的实验数据集，我们首先在NLPCC数据集上训练出了一个情感分类器，然后使用这个分类器标注STC数据集。这个分类器使用Bi-LSTM.\nNLPCC包含anger, disgust, contentment, joy, sadness, fear, surprise, neutral，移除不常见的类别fear和surprise，还剩六类，用bilstm分类器标注情感标签。训练、验证、测试集划分为9:0.5:0.5\n### 4.2 Training Details\n### 4.3 Baseline Models\n- Seq2Seq\n- EmoEmb\n- ECM\n- EmoDS-MLE\n- EmoDS-EV 使用外部情感词典，而不是内部的\n- EmoDS-BS 原始集束搜索beam search，而不是我们的多样化解码\n### 4.4 Automatic Evaluation\n- Embedding Score\n- BLEU Score\n- Distinct\n- Emotion Evaluation\n### 5 Conclusion\n情感状态可以用语言来表达，可以明确地使用强烈的情感词，也可以用不同的模式形成中性词，我们提出了一个创新的情感对话系统，可以以任一方式传达想要的情感，并生成有意义且连贯的结构。\nseq2seq框架用基于词典的注意力机制扩展，提高每个时间步中情感词进入文章的概率；情感分类器用于指导回复生成过程，确保特定的感情在生成文本中被合适的表达。"},{"title":"NEW_TEST","url":"/2019/08/07/NEW-TEST/","content":"\n我胡汉三又回来啦！"},{"url":"/2019/08/07/tensorflow001/","content":"** tf.Session()和tf.InteractiveSession()的区别？ **\n\n\n唯一的区别在于：tf.InteractiveSession()加载它自身作为默认构建的session，tensor.eval()和operation.run()取决于默认的session.\n\n换句话说：InteractiveSession 输入的代码少，原因就是它允许变量不需要使用session就可以产生结构。"},{"title":"Hello World","url":"/2019/08/07/hello-world/","content":"Welcome to [Hexo](https://hexo.io/)! This is your very first post. Check [documentation](https://hexo.io/docs/) for more info. If you get any problems when using Hexo, you can find the answer in [troubleshooting](https://hexo.io/docs/troubleshooting.html) or you can ask me on [GitHub](https://github.com/hexojs/hexo/issues).\n\n## Quick Start\n\n### Create a new post\n\n``` bash\n$ hexo new \"My New Post\"\n```\n\nMore info: [Writing](https://hexo.io/docs/writing.html)\n\n### Run server\n\n``` bash\n$ hexo server\n```\n\nMore info: [Server](https://hexo.io/docs/server.html)\n\n### Generate static files\n\n``` bash\n$ hexo generate\n```\n\nMore info: [Generating](https://hexo.io/docs/generating.html)\n\n### Deploy to remote sites\n\n``` bash\n$ hexo deploy\n```\n\nMore info: [Deployment](https://hexo.io/docs/deployment.html)\n"},{"title":"DiscreteMathematicsReview","url":"/2018/03/27/DiscreteMathematicsReview/","content":"参考教材：《离散数学及其应用》(第7版) Kenneth H.Rosen著\n#Chapter1 基础：逻辑和证明\n#Chapter2 基本结构：集合、函数、序列、求和与矩阵\n#Chapter6 计数\n#Chapter6 高级计数技术\n#Chapter9 关系\n#Chapter10 图\n#Chapter11 树\n"},{"title":"《Explicit semantic ranking for academic search via knowledge graph embedding》论文笔记","url":"/2018/03/23/ExplicitSemanticRankingForAcademicSearchViaKnowledgeGraphEmbedding/","content":"This paper introduces Explicit Semantic Ranking (ESR),\na new ranking technique to connect query and documents\nusing semantic information from a knowledge graph. We\nFirst build an academic knowledge graph using S2's corpus\nand Freebase. The knowledge graph includes concept en-\ntities, their descriptions, context correlations, relationships\nwith authors and venues, and embeddings trained from the\ngraph structure. We apply this knowledge graph and em-\nbeddings to our ranking task. Queries and documents are\nrepresented by entities in the knowledge graph, providing\n`smart phrasing' for ranking. Semantic relatedness between\nquery and document entities is computed in the embedding\nspace, which provides a soft matching between related enti-\nties. ESR uses a two-stage pooling to generalize these entity-\nbased matches to query-document ranking features and uses\na learning to rank model to combine them.\n本文引入显式语义排序（Explicit Semantic Ranking，ESR），这是一种利用知识图中的语义信息连接查询和文档的新排序技术。 我们首先使用S2的语料库和Freebase构建学术知识图。 知识图包括概念实体，它们的描述，上下文相关性，与作者和场所的关系以及从图结构训练的嵌入。 我们应用这个知识图并嵌入到我们的排名任务中。 查询和文档由知识图中的实体表示，为排序提供“智能表述”。 在嵌入空间中计算查询和文档实体之间的语义相关性，这提供了相关实体之间的软匹配。 ESR使用两阶段池来将这些基于实体的匹配概括为查询文档排名特征，并使用学习来对模型进行排名以将它们组合。\n#2.Related Work\n之前的工作更多关注于学术图的分析而不是特别的排名。","tags":["Graph","Paper","NLP"]},{"title":"SDNE","url":"/2018/03/19/SDNE/","content":"明天填坑\n"},{"title":"RNN","url":"/2018/03/18/RNN/","content":"明天填坑\n"},{"title":"digui","url":"/2018/03/14/digui/"},{"title":"SkipGram模型的理解","url":"/2018/03/12/SkipGram/","content":"http://blog.csdn.net/yc1203968305/article/details/79103664","tags":["Graph","NLP"]},{"title":"《A Comprehensiv Survey of Graph Embedding Problems Techniques and Applications》笔记","url":"/2018/03/10/AComprehensivSurveyofGraphEmbeddingProblemsTechniquesandApplications/","content":"Questions:\n- auxiliary-node feature(text feature)\n- 方法之间的区别\n- \n\n图嵌入的意义：降低图研究的复杂程度\n# 1.Introduction #\n因为要利用图中隐含的信息，所以要对图进行研究。但是图的规模太大，直接研究计算开销太大，因此要用合理的方式来给图换一种表示方法，同时要尽可能的保留原有的结构信息。\n**Graph embedding converts a graph into a low dimensional space in which the graph information is preserved.**\n区分图表示与图嵌入。graph representation不需要降维，graph embedding需要降维。\n# 2.Problem Formalization\n## 2.1 Notataion and Definition\n- graph\n- homogeneous graph：节点、边的类型只有一种。\n- heterogeneous graph：节点、边的类型不止一种。\n- knowledge graph：（h,r,t）（实体1，关系，实体2）可以看做一种异构图的特例。\n- first-order proximity：\n\t直接相邻的邻居之间，边的权值越大相似度越高\n- second-order proximity\n\t公共邻居越多越相似，由邻居的结构决定。用余弦相似度计算。\n# 3.Problem Settings\n从问题的角度分类，可以根据输入数据的类型，也可以根据需要的输出类型。\n因为携带的数据/需要保留的信息不同，所以说嵌入信息的方法也不同，评价嵌入结果的方法也不同。\n## 3.1 Input\nFixed and given.\n不同的输入类型携带了不同的需要保存的信息。\n### 3.1.1 Homogeneous Graph\n最普通最直观的是无权无向图，近来也有考虑其中之一或者两者都考虑的。\nChallenge:如何保存连接模式的多样性/结构信息（因为只有这个需要保存）\n### 3.1.2 Heterogeneous Graph\n常用于三种应用场景：\n- 社区问答\n- 多媒体网络\n- 知识图谱？\nChallenge:如何在不同的类型之间保持一致与平衡？\n### 3.1.3 Graph with Auxiliaary Information\n点/边/全图除了结构之外，也可能自己带有一些相关信息。主要分类有：\n- Label:打了不同标签的节点在表示时距离应该尽量的远\n- Attribute：属性值既可以是离散的，也可以是连续的。\n- Node feature节点特征：大多数的节点特征是文本特征，可以代表words/documents.\n丰富的无结构信息。使归纳成为可能。\n- Information propagation信息传播：可能是级联的\n- Knowledge base基于知识库。\nChallenge:这些信息往往丰富而无结构。如何体现这样的信息，使结构与附加信息同时保存下来？//对比于Homogeneous，只需要考虑结构。\n### 3.1.4 Graph Constructed from Non-relational Data\nInput data is assumed to lie in a low dimensional maniofold.输入数据来自于低维流形空间\n①Pairwise similarity based.点对之间的相似度。\n不能采用欧氏距离。可以用KNN,Isomap,Anchor等.\n②Node co-occurrence based.基于节点同现\n③etc.\nChallenge:如何计算相似程度\n## 3.2 Output\nTask driven.由解决的任务决定\n首要的挑战就是如何根据不同的问题选择合适的输出粒度(输出类型)。\n### 3.2.1 Node**最常见的一种\n与邻居的相似度\nChallenge：如何定义、编码\"邻近度\"\n### 3.2.2 Edge\n常用于预测类的任务。预测关系是什么（边的意义）？预测是否有边（有关系）？\nChallenge：如何定义边的相似度？不对称的边如何建模？（可能有向）\n### 3.2.3 Hybrid\n//最终产物是substructure\n如何生成子结构？在统一的空间里如何表示不同的子结构？\n### 3.2.4 Whole-Graph\n//substructure只是中间产物，是分析问题的一个步骤\n全图级别的相似度\n分析不同规模的子结构。\nChallenge:①如何归纳全图的信息②分析全图需要的开销很大，如何实现效率与表达能力的平衡。\n# 4.Graph Embedding Techniques\n## 4.1 Matrix Factorization矩阵分解\n多数情况下input from on-relational high dimensional data features,\n输出为node.\n### 4.1.1 Graph Laplacian Eigenmaps图拉普拉斯特征映射\nThe graph property to be preserved can be interpreted as pairwise node similarities,and thus a larger penalty is imposed if two nodes with larger similarity are embedded far apart.\n相似度高的两节点映射到低维空间之后仍然应保持距离相近。\n### 4.1.2 Node Proximity Matrix Factorization\nNode proximity can be approximated in a low-dimensional space using matrix factorization.The objective of preserving node proximity is therefore to minimize the loss during the approximation.\n利用矩阵分解可以在低维空间中近似地逼近节点的邻近度。因此，保持节点邻近度的目标是在逼近过程中尽量减少损失。\n**Summary**\nMatrix factorization is mostly used to embed the graph constructed from non-relational data for node embedding as this is the original setting of the graph Laplacian eigenmaps based problems.In a few work,it is also used to embed homogeneous graphs.//从输入的角度总结\n矩阵分解主要用于嵌入由非关系数据构建的图，用于节点嵌入，因为这是基于问题的图拉普拉斯特征映射的原始设置。在一些工作中，它也被用来嵌入齐次图形。\n## 4.2 Deep Learning\n可以从别的研究方向推广而来。\n输入①边采样paths sampled from a graph②全图the whole graph\n依据**边采样是否采用了随机游走**分类\n鲁棒，高效，已经广泛应用。\n### 4.2.1 DL based Graph Embedding with Random Work\n刻画了网络的局部信息。\n两节点联系越紧密，在随机游走中共现的可能性就越大。如果不连通，就不可能共现。\n基础算法DeepWalk，其他算法在此基础上引申。\n从NLP/语言模型领域推广而来，思想来自于word2vec.\ne.g.DeepWalk的游走方法是随机DFS,node2vec是由参数控制的随机游走，加入了BFS。\n### 4.2.2...without Random Walk\n基于全图.\n- Autoencoder\n- Deep Neural Network\n不同算法的区别是在图上生成类似卷积的操作。\n## 4.3 Edge Reconstruction based Optimization\nThe edges established based on node embedding should be as similar to those in the input graph as possible.\n基于边重建，重建的边应该近似于输入\n### 4.3.1 Maximizing Edge Reconstruction Probability最大化概率\n### 4.3.2 Minimizing Distance-based Loss最小化基于距离的损失\n### 4.3.3 Minimizing Margin-based Ranking Loss最小化基于\n图中一些节点可能与**一组**相关的节点有关\nA node's embedding is more similar to the embedding of relevant nodes than that of any other irrellevant node.\n## 4.4 Graph Kernel\n把握全局特征。针对全图而生\n全图表示。拆分成分解出来的子结构\nThe whole graph structure can be represented as a vector containing the counts of elementary substructures that are decomposed from it.\n子结构分类：\n- Graphlet\n- Subtree Patterns \n- Random Walks\n适合的输入类型：\n- homogeneous同质\n- auxiliary info.辅助信息\n## 4.5 Generative Model\n适用场景： 输出node,edge；输入heterogeneous,auxiliary infomation。\n### 4.5.1 Embed Graph Into The Latent Semantic Space\n嵌入到潜在语义空间\n### 4.5.2 Incorporate Latent Semantics for Graph Embedding\n## 4.6 Hybrid & Others\n\n# 5.Appliacation#\n该节是按照输出类型分类的。\n## 5.1 Node Related##\n\n- Node Classfication\n- Node Clustering\n- Node Recommendation/Retrieval/Ranking推荐、检索、排序\n## 5.2 Edge Related\n- Link Prediction联系预测\n- Triple Classification三元组分类 \n## 5.3 Graph Related Applicaitons\n- Graph Classification图像分类\n- Visualization 可视化\n## 5.4 Other\n- Knowledge graph related\n- Multimedia network\n- Information propagation\n- Socail networks alignment\n- Image\n# 6.Future Directions\n- Computation\n- Problem settings\n动态的，随时间而变的\n- Techniques\n- Applications\n# 7.Conclusion\nGraph embedding的正式定义、基本概念\n两种分类方法，将现有工作分为①四输入四输出，分别的挑战②技术\n总结应用\n四个未来研究方向\n\n\n----------\nDeepWalk:Online Learning of Social Representations\n# 1.Introduction\n# 2.Problem Definition\n# 3.\nSocial representations应有的特征：\n- Adaptability适应性。能够适应社交网络的演化\n- Community aware.\"有社区意识\"，用距离度量相似度，有利于同质网络的泛化。\n- Low dimensional低维\n- Continuous连续\n## 3.1 Random Walks\n可用于相似度的度量。\n容易并行化\n适应小的变化\n## 3.2 Connection:Power Laws\n网络结构和语言结构是相似的。\n## 3.3 Language Model\n# 4.Method\n## 4.1\n## 4.2 Algorithm:DeepWalk\n### 4.2.1 SkipGram\n### 4.2.2 Hierarchical Softmax\n为了提高性能，加速计算。\n### 4.2.3 Optimization\n随机梯度下降","tags":["Graph"]},{"title":"180309","url":"/2018/03/09/180309/"},{"title":"180306","url":"/2018/03/06/180306/"},{"title":"byjl","url":"/2018/03/03/byjl/"},{"title":"remotenj180302","url":"/2018/03/02/remotenj180302/"},{"title":"《正确写作美国大学生数学建模竞赛论文》笔记","url":"/2018/01/30/mcmwriting/","content":"#Ⅰ.简介\n#Ⅱ论文结构\n##1.小节划分\nSummary\n1.Restatement of the Problem重述并澄清赛题\n2.Assumptions\n3.Justification of Our Approach\n4.The Model\n5.Testing the Model\n6.Results\n7.Strengths and Weaknesses\nReferences\n重要的句子，包括首次定义的概念，应该用黑体或斜体书写。\n重要的数学公式应另起新行单独列出。\n能用列表就用列表\n图表加上简单明确的文字说明。\n##2.写好引言\n引言(Introduction)是论文的第一节，尽管其标题不一定叫“引言”，例如，也可以用“问题重述”作为标题。引言应该包括以下内容：对赛题的解读、对现有研究成果的综述与评论以及对解题思路和主要方法的简要介绍。\n##3.论文主体\n在描述模型之前，将模型设计所用的假设条件一一列出并解释清楚。\n**Assumptions and justifications 假设条件与解释**\n**Model designs模型设计**\n应集中精力设计一个模型，或者最终能够导出一个较好模型的一系列子模型，不要分散精力，拼凑出几个很一般的模型\n**Modeling for general problems普遍模型**\n不要只针对赛题给出的参数值设计模型，高水平的论文通常会把赛题看成是一个普遍问题的特例，首先探讨普遍问题的求解，然后再对赛题这一特例给出具体解答。\n##4.如何写结论\n\n"},{"title":"CNN","url":"/2018/01/04/CNN/","content":"CNN是深度学习在图像领域的一个应用。那么它相对于原来的神经网络方法有什么不同？为什么它适用于图像领域？\n原来：全连接神经网络\n\n需要的参数过多，例如1000*1000的图像，则输入层有10^6个节点，若隐藏层也有10^6个节点，则输入层到隐藏层的参数有10^12个。\n\nCNN：局部感知、权值共享\n\nCNN用局部感知和权值共享大大减少了参数，同时还具备其它优点。它们与自然图像自身具有的特性：特征的局部性与重复性完美贴合。\n\n局部感知：\n\n（是什么）即网络部分连通，每个神经元只与上一层的部分神经元相连，只感知局部，而不是整幅图像。（滑窗实现）\n\n（可行性）局部像素关系紧密，较远像素相关性弱。\n\n因此只需要局部感知，在更高层将局部的信息综合起来就得到了全局的信息。\n\n受启发于生物视觉系统：局部敏感；对外界认知从局部到全局。\n\n权值共享：\n\n（是什么）从一个局部区域学习到的信息，应用到图像的其它地方去。即用一个相同的卷积核去卷积整幅图像，相当于对图像做一个全图滤波。一个卷积核对应的特征比如是边缘，那么用该卷积核去对图像做全图滤波，即是将图像各个位置的边缘都滤出来。（帮助实现不变性）。不同的特征靠多个不同的卷积核实现。\n\n（可行性）图像的局部统计特征在整幅图像上具有重复性（即位置无关性）。即如果图像中存在某个基本图形，该基本图形可能出现在任意位置，那么不同位置共享相同权值可实现在数据的不同位置检测相同的模式。比如我们在第一个窗口卷积后得到的特征是边缘，那么这个卷积核对应的就是边缘特征的提取方式，那么我们就可以用这个卷积核去提取其它区域的边缘特征。\n\n \n\nCNN为什么具有仿射不变性（平移、缩放、旋转等线性变换）？\n局部感知、权值共享、池化共同影响形成。\n\n1)局部感知和权值共享，使得CNN能够在浅层学到比较基础的特征，比如点、线、边缘，高层特征是这些基础浅层特征的组合，即使发生仿射变化，这些基础的特征依然不变，因此最后的识别结果不受到影响。\n\n用具体的人脸识别的例子来说，到高层时，CNN将这些浅层基础特征组成成更高级的特征，比如人的眼睛、鼻子等，直到全连接层将这种两个眼睛一个鼻子的组合识别为人脸。因此，即使对图像进行平移、缩放、旋转，在浅层依旧能提取出点、线、边缘等基础特征，到高层就能将它们组合成眼睛鼻子，从而最终依旧能被识别为人脸。（一个说法是，平移旋转后的图像到高层对应的特征图一定程度上也是平移旋转后的。过多的平移旋转，最终可能影响到图像的识别，因此更鼓励用data augmentation来实现对旋转的鲁棒。）\n\n2)池化，比如max pooling，它是取一个区域的最大值。因此当图像发生平移、缩放、旋转等较小的变化时，依然很有可能在同一位置取到最大值，与变化前的响应相同，由此实现了仿射不变性。average pooling同理，发生较小的仿射变化后，均值可能依然不变。\n\n3)但上述对CNN仿射不变性的解释，都建立在“发生较小的仿射变化”这样的约束上，当发生较大的仿射变化后，可能对CNN的识别结果造成影响。因此，鼓励获取更加丰富的训练样本（进行data augmentation）使得CNN在应用时对新样本有较好的鲁棒性。\n\n \n\n参考：\n\nhttp://blog.csdn.net/stdcoutzyx/article/details/41596663/\n\nhttp://blog.csdn.net/zouxy09/article/details/8781543/\n\nhttp://blog.csdn.net/yeeman/article/details/6325693\n\nhttp://dataunion.org/11692.html\n\nhttp://www.voidcn.com/article/p-ahduptmb-dx.html CNN十大问"},{"title":"计算机网络概括","url":"/2017/11/23/NetworkOverview/","content":"计网的目的是通信，是为了连接端到端 所以我们首先要考虑——网怎么设计\n我们有两种网：\n1.分组交换 ；\n2.电路交换（电话）\n在很久很久以前，你记不记着，有个“拨号连接”，有个叫做“猫”的东西？？？ 没错，就是那个，一上网就打不了座机的时代 此时，我们还是电路交换哟\n这样太蠢了！！！ 如果我只是想上网看下小电影的简介，那我打开介绍小电影的网站，就暂时不会再通信了 所以，没必要一直给我连接着啊！\n于是，我们用起了分组交换 分组交换还有两种方式：\n1.虚电路，如ATM（模拟电话线路）；\n2.数据报，如因特网\n-为啥因特网不用虚电路？ 肯定是因为，大多数时候，虚电路没必要啊，而且麻烦不好用啊\n-为啥虚电路没必要&不好用？ 因为大多数时候，互联网没有实时要求啊，&他的面向连接浪费资源啊\n好嘞，现在我们知道了，因特网使用的是，数据报 我们先不管数据报是什么，我们先考虑下如何传输数据报\n-----------------------------------------\n我们的因特网，肯定是基于物理电路的， 因此，我们需要一个，将数据转化为物理信号的层， 于是，物理层诞生啦\n----------------------------------------\n有了处理物理信号的物理层，可我们还得知道，信号发给谁啊 你肯定知道，每个主机都有一个，全球唯一的MAC地址吧 所以，我们可以用MAC地址来寻址啊 恭喜你，链路层诞生啦\n----------------------------------------\n别急，你知道MAC地址，是扁平化的吧 也就是说，MAC地址的空间分布，是无规律的！！！ 如果你有十万台主机，要通过MAC地址来寻址 无F**K可说， 不管你设计什么样的算法，数据量都太大了！！！ 所以，我们需要IP地址啊 <PS,IP里的有趣的东西太多啦，所以我补充在了最后> 有了IP地址，恭喜你，网络层诞生啦\n-----------------------------------------\n然而，一台主机不能只和一台服务器通信啊， 毕竟下小电影，也要同时货比三家啊 那如何实现并行通信呢？ 嘿嘿，我们有端口号啊\n再基于不同需求： 有人想要连得快，不介意数据丢失，比如你的小电影 有人必须要数据可靠，比如发一个电子邮件 于是产生了UDP&TCP 恭喜你，运输层诞生啦\n-----------------------------------------\n别急，你知道的吧，不同应用，有不同的传输需求 比如，请求网页，发送邮件，P2P... 而且，还有DHCP服务器啊 为了方便开发者，我们就对这些常用需求，进行了封装 恭喜你，应用层诞生啦\n至此，自底向上，讲述了计网。 待我考完试，我可以写一部，计算机网络•从下向顶方法 （斜眼笑\n====================\n<细节补充> >来我们思考先一个问题：如果有四台电脑，要互相能通信，咋办？\n>每一台电脑都和另外三台连起来？ 那我要是再来十台电脑，你在电脑上给我再加十个接口？\n>那，把他们连接到一个小盒子上，让小盒子帮着通信？ 哎这个可以有啊，那如果我有一万台电脑，一个小盒子能够用？\n>嘿嘿，那让每一个小盒子连一百台，然后把一百台小盒子再连给一个小盒子\n-----------------------------------------------\n我们可以用“电话线，宽带，和光纤”，把电脑接给小盒子，它们被称作“接入网” 而ISP就像小盒子，帮你在网络里做通信 而ISP的分层，无非就是，终端太多了，没办法不分层\n好了，现在你已经明白了网络的层次化\n你肯定是知道， 为了在辣么多计算机里，找到目标，我们采用了，有规律的IP地址 而路由器，又叫分组交换机，就是帮我们在公网里，做IP寻址的\n最初，IP地址是IPv4 首先，IP地址是分成了五类（ABCDE）\n奈何不够用啊，于是，我们是使用了子网划分 然鹅，手动分配子网IP，会死人的！ 于是，DHCP来了（斜眼笑\nmd还是不好用啊，于是，诞生了无分类编址（CIDR） 奈何，还是不够用啊 于是，NAT出现啦，于是专用网的IP不再占用公网IP\n---------------------------------------\n>首先，啥是专用网啊 1.局域网，比如，公用一个路由器的宿舍啊，家啊 2.部分广域网，比如军队、铁路、交通、电力等部门，拥有自己专用的通信网和计算机网。然鹅，这些网络不对内部外的用户开放。这些网络覆盖的地理范围很广，因此，这些专用网都是广域网。\n保密性质的广域网，通信要扯到VPN，宝宝没学到这里，先埋个坑\n---------------------------------------\n来我们先谈谈局域网内的通信： 如果哈，我们是一个大局域网，比如我们公司有一百台电脑， 首先，路由器没一百个接口让我插！ 其次，如果我不想和公网通信，那我就没必要用路由器！ 所以，链路交换机来了！！！\n链路交换机是基于MAC寻址的，因为局域网没大到必须用IP寻址的地步啊 但更准确的说话，链路交换机采用了，跨越链路层和网络层边界的协议——ARP 毕竟，ARP要做一个IP到MAC的映射，\n-----------------------------------------\n>你问我，为啥ARP要做IP到MAC的映射 因为，你在应用层和运输层里，目的地址都写得是IP, 不把IP转化为MAC，咋寻址啊？\n>你问我，局域网为啥不用路由器，为啥要用链路交换机 交换机功能少，接口多，比路由器划算啊\n>那，局域网和公网怎么通信呢? 所以，NAT来了啊！！！\n分组交换机，也就是路由器，用自己的公网IP，帮你们局域网里的人们，给公网发信息 然后把接受到的信息，再转发给，那个找他帮忙的人 这就是NAT技术啊混蛋！！！\n-------------------------------------------\n这时一群人说，NAT bulabula不好，我们要拒绝NAT,使用IPv6 那么就牵扯到了IPv4和IPv6间的通信（双栈||隧道）\n还有啊，IP地址太丑啦，用户根本记不住 http://xxx.xxx.xxx.xxx 于是乎，域名千呼万唤始出来 顺便带出来了DNS服务器\n","tags":["SDU"]},{"title":"NachosProj3题目要求","url":"/2017/11/10/NachosProj3/","content":"Phase 3: Caching and Virtual Memory\n缓存和虚拟内存\nThe third phase of Nachos is to investigate the interaction between the TLB, the virtual memory system, and the file system. We don't provide any new virtual memory code for this assignment. You will continue to use the stub file system. For this phase, you should run gmake and nachos in the proj3 directory.\nNachos的第三个阶段是研究TLB、虚拟内存系统和文件系统之间的交互。我们没有为这个任务提供任何新的虚拟内存代码。您将继续使用存根文件系统。在这个阶段，您应该在proj3目录中运行gmake和nachos。\n\nTo help you to organize your code better, we provide you with a new package, nachos. vm, with two new classes, VMKernel and VMProcess. VMKernel extends UserKernel, and VMProcess extends UserProcess. VMKernel and VMProcess are the only classes you should have to modify for this project phase.\n为了帮助您更好地组织代码，我们提供了一个新的包nachos。vm，有两个新类:VMKernel和VMProcess。VMKernel扩展了UserKernel，而VMProcess扩展了用户进程。在这个项目阶段，VMKernel和VMProcess是唯一需要修改的类。\n\nThis phase and the next phase of the project involve open-ended design problems. We will expect you to come up with a design that would make sense in a real system, and to defend your choices in the design review. For example, you will have some freedom to choose how to do software address translation on TLB misses, how to represent the swap partition, how to implement paging, etc. In each case, we will expect you to come to the design review armed with a defensible justification as to why your choices are reasonable. You should evaluate your design on all the available criteria: speed of handling a TLB miss, space overhead in memory, minimizing the number of page faults, simplicity, etc. There is no single \"right\" answer.\n这个阶段和下一阶段的项目涉及开放式设计问题。我们希望你能设计出一个在真实系统中有意义的设计，并在设计评论中为你的选择辩护。例如,您将有自由选择如何去做软件在TLB地址转换,如何代表交换分区,如何实现分页,等等。在每种情况下,我们将期待你来设计审查手持一个站得住脚的理由为什么你的选择是合理的。你应该根据所有可用的标准来评估你的设计:处理一个TLB失误的速度，内存中的空间开销，最小化页面错误的数量，简单性等等，没有一个“正确”的答案。\n\nThe first design aspect to consider is the software-managed translation lookaside buffer (TLB). Page tables were used in phase 2 to simplify memory allocation and to isolate failures from one address space from affecting other programs. For this phase, the processor knows nothing about page tables. Instead, the processor only deals with a software-managed cache of page table entries, called the TLB. Given a memory address (an instruction to fetch, or data to load or store), the processor first looks in the TLB to determine if the mapping of the virtual page to a physical page is already known. If the translation is in the TLB, the processor uses it directly. If the translation is not in the TLB (a \"TLB miss\"), the processor causes a trap to the OS kernel. Then it is the kernel's responsibility to load the mapping into the TLB, using page tables, segments, inverted page tables, or whatever other mechanism might be appropriate. In other words, the Nachos MIPS simulator does not have direct access to your page tables; it only knows about the TLB. It is your job to write the code that manages a TLB miss.\n首先要考虑的设计方面是软件管理的翻译lookaside buffer(TLB)。第2阶段使用页表来简化内存分配，并从一个地址空间隔离失败，从而影响其他程序。对于这个阶段，处理器对页表一无所知。相反，处理器只处理页面表条目的软件管理缓存，称为TLB。给定一个内存地址(用于获取或数据到加载或存储的指令)，处理器首先在TLB中查找，以确定虚拟页到物理页面的映射是否已经知道。如果翻译在TLB中，处理器直接使用它。如果翻译不是在TLB(一个“TLB miss”)中，处理器会给OS内核造成一个陷阱。然后，内核负责将映射加载到TLB中，使用页表、段、倒页表，或者其他任何可能合适的机制。换句话说，Nachos MIPS模拟器不直接访问您的页表;它只知道TLB。编写管理TLB失误的代码是您的工作。\n\nThe second design aspect of this project is paging, which allows physical memory pages to be transferred to and from disk to provide the illusion of an (almost) unlimited physical memory. A TLB miss may require a page to be brought in from disk to satisfy the translation. That is, when a TLB miss fault occurs, the kernel should check its own page table. If the page is not in memory, it should read the page in from disk, set the page table entry to point to the new page, install the page table entry, and resume the execution of the user program. Of course, the kernel must first find space in memory for the incoming page, potentially writing some other page back to disk, if it has been modified.\n这个项目的第二个设计方面是分页，它允许将物理内存页面从磁盘传输到磁盘，以提供(几乎)无限物理内存的错觉。一个TLB失误可能需要从磁盘引入一个页面以满足转换。也就是说，当一个TLB错误发生时，内核应该检查它自己的页表。如果页面不在内存中，则应该从磁盘读取页面，设置页面表项以指向新页面，安装页表条目，并恢复用户程序的执行。当然，内核必须首先为传入的页面找到内存空间，如果它被修改的话，可能会将其他页面重新写入磁盘。\n\nPerformance of this mechanism depends greatly on the policy used to decide which pages are kept in memory and which are stored on disk. On a page fault, the kernel must decide which page to replace; ideally, it will throw out a page that will not be referenced for a long time, keeping in memory those pages may be referenced soon. Another consideration is that if the replaced page has been modified, the page must first be saved to disk before the needed page can be brought in. (Of course, if the page has not been modified, it is not necessary to write it back to disk.)\n这种机制的性能在很大程度上取决于用于决定哪些页面保存在内存中，哪些页面存储在磁盘上。在页面错误上，内核必须决定要替换哪个页面;理想情况下，它会抛出一个长时间不会被引用的页面，记住这些页面可能很快就会被引用。另一个考虑是如果替换的页面已经被修改，那么页面必须先保存到磁盘，然后才可以引入需要的页面。(当然，如果页面没有被修改，就没有必要把它写回磁盘。)\n\nTo help you implement virtual memory, each TLB entry contains three status bits: valid, used, and dirty. If the valid bit is set, the virtual page is in memory and the translation can be used directly by the processor. If the valid bit is clear, or if the page translation is not found in the TLB, then the processor traps to the OS to perform the translation. The processor sets the used bit in the TLB entry whenever a page is referenced and sets the the dirty bit whenever the page is modified.\n为了帮助您实现虚拟内存，每个TLB条目包含三个状态位:有效、使用和脏。如果设置了有效位，则虚拟页在内存中，而翻译可以直接由处理器使用。如果有效位是清晰的，或者在TLB中没有找到页面转换，那么处理器就会捕捉到操作系统来执行翻译。当页面被引用时，处理器将在TLB条目中使用的位设置为，并在修改页面时设置脏的位。","tags":["SDU","Nachos","OperatingSystem"],"categories":["Lesson"]},{"title":"多核平台上的并行计算 实验笔记","url":"/2017/11/09/ParallelProgrammingHW/","content":"信号量与互斥量最大的区别在于信号量是没有个体拥有权的。\n信号量不是PThreads线程库的一部分，所以需要在使用信号量的程序开头加头文件。\n条件变量是一个数据对象，允许线程在某个特定条件或事件发生前都处于挂起状态。当时间或条件发生时，另一给线程可以通过信号来唤醒挂起的线程。一个条件变量总是与一个互斥量相关联。","tags":["ParallelProgramming","PThreads"]},{"title":"NachosProject2笔记","url":"/2017/11/06/NachosNote2/","content":"UserKernel 继承自 ThreadedKernel。\n整个需要装入的进程是一个 machine.CoffSection 类型的对象\n对 java 文件操作进行包装，向上提供为 StubFileSystem 的接口。\n\n用户进程是通过UserProcess.load 方法将程序载入内存的。而每个可执行文件用一个 Coff 对象包装。对传入的每个文件名， load 方法先根据文件名读取该文件，然后将文件作为构造方法参数构造一个 Coff 对象，在 Coff 对象中，会对文件进行一个整体的解析，获取代码段\n（ CoffSection）数目、程序入口等信息，并构造代码段。然后 load方法会获取该程序的每段代码段，统计页的数目和参数的数目，参数会占一页内存，进而将代码段和参数按页载入内存。\n\n在 test 目录 stdio.h 中，可以看到 typedef int FILE，说明 c 代码中的 FILE 是该文件的\n标识符。\n对于创建文件、打开文件、关闭文件、读取文件以及删除文件，在 handleSyscall 方\n法中模仿 halt 的写法，依赖 syscall.h 中各个系统调用的参数值，分别书写\nhandleFileCreate、 handleFileOpen、 handleFileClose、 handleFileRead、 handleFileUnlink\n方法，传入对应的参数，通过 ThreadedKernel.fileSystem（第 9 条提示）可以调用到\n系统已经实现的 StubFileSystem，进行各类文件操作。\n对于文件的创建及打开，会返回整型文件描述符，不存在返回-1。可向 open 方法\n中传入一个文件名，检查返回是否是-1 进而确定文件是否存在。文件读写都要传入\n文件标识符、缓冲区数组、读取/写入的字节数，会返回实际读写的字节数。关闭\n和删除传入的参数不同，但都是通过返回 0 来表示操作成功， -1 表示错误发生。\n\n从当前进程中取用FileDesc，从内存中取用FilenamePtr\n\n","tags":["SDU","Nachos","OperatingSystem"],"categories":["Lesson"]},{"title":"碎碎念 - 20171105","url":"/2017/11/05/diary171105/","content":"在苏州的时候一直收不回心来，果然还是要逼自己学习比较好。\n扪心自问，是在宿舍里窝着快乐，还是学到知识的满足感，看见未来曙光的感觉更快乐呢？\n\n总结一下最近要做的事\n- 计算机网络作业 还差两个题\n- nachos\n- 多核平台上的并行计算 ！！！\n- 统计学习方法\n\n这条路很长很寂寞，但是非常值得。\n","tags":["diary"],"categories":["diary"]},{"title":"数据科学导论HW1 - Text Analysis and Entity Resolution","url":"/2017/10/25/DataScienceIntroductionHW1/","content":"Assignment 1: Text Analysis and Entity Resolution\n#### Overview ####\nEntity resolution is a common, yet difficult problem in data cleaning and integration. In this assignment, we will use powerful and scalable text analysis techniques to perform entity resolution across two data sets of commercial products.\n\n#### Entity Resolution ####\nEntity resolution, also known as record deduplication, is the process of identifying rows in one or more data sets that refer to the same real world entity. Take an example. You're on ebay looking for a hip data science accessory, but you're on a budget, so you decide to scrape the ebay listings for a few days to get a feel for the market. Unfortunately, the listings are confusing and you don't know how to aggregate them. Entity resolution to the rescue! You find an authoritative database and map all the ebay listings to it. Now you can comparison shop, get that sweet Keuffel and Esser for way cheap, and impress all the data hipsters.\n\nBut finding matching records is a hard problem in general. A major reason is that the criteria for identifying duplicates are often vague and impossible to encode in rules. In addition, from a purely computational perspective, the problem is quadratic in the size of its inputs: naively, all pairs of records need to be compared to find all the duplicates. In this assignment, we will begin to address both these challenges.\n\n#### Application ####\nYour assignment is to perform entity resolution over two web-scraped data sets of commercial product listings, one from Amazon, and one from Google. The goal is to build a unified database of all products listed on the Internet: a one-stop-shop for all your shopping needs. (Elevator pitch: it's like Kayak.com for e-commerce!)\n\nThe web has made unprecedented amounts of data available publicly, but scraped data frequently needs to be de-duplicated. These data sets are typical examples of what you can collect with some simple scripting. The data is not especially large (just a few thousand records), but even so, you will find that entity resolution is a major challenge (top results with this data are ~50% success rate). Don't get discouraged; the goal is to get acquainted with techniques to tackle the problem, and apply them to a representative example.\n\n#### Files ####\nData files for this assignment can be found at:\n\nhttps://github.com/biddata/datascience-fa14/raw/master/hw1/hw1data.tar.gz\n\nThe zip file includes the following files:\n\nGoogle.csv, the Google Products data set\nAmazon.csv, the Amazon data set\nGoogle_small.csv, 200 records sampled from the Google data\nAmazon_small.csv, 200 records sampled from the Amazon data\nAmazon_Google_perfectMapping.csv, the \"gold standard\" mapping\nstopwords.txt, a list of common English words\nBesides the complete data files, there are \"sample\" data files for each data set. Use these for Part 1. In addition, there is a \"gold standard\" file that contains all of the true mappings between entities in the two data sets. Every row in the gold standard file has a pair of record IDs (one Google, one Amazon) that belong to two record that describe the same thing in the real world. We will use the gold standard to evaluate our algorithms.\n\n#### Deliverables ####\nComplete the all the exercises below and turn in a write up in the form of an IPython notebook, that is, an .ipynb file. The write up should include your code, answers to exercise questions, and plots of results. Complete submission instructions will be posted on Piazza.\n\nYou can use this notebook and fill in answers inline, or if you prefer, do your write up in a separate notebook. In this notebook, we provide code templates for many of the exercises. They are intended to help with code re-use, since the exercises build on each other, and are highly recommended. Don't forget to include answers to questions that ask for natural language responses, i.e., in English, not code!\n\nWe would prefer to test some of your code automatically, so please try to submit a notebook that uses the function names requested by the questions and that can be executed with \"Cell > Run all\".\n\n## Guidelines ##\n#### Code ####\nThis assignment can be done with basic python and matplotlib. Feel free to use PANDAs, too, which you may find well suited to several exercises. As for other libraries, please check with course staff whether they're allowed. In general, we want you to use whatever is comfortable, except for libraries (e.g., NLTK) that include functionality covered in the assignment.\n\nYou're not required to do your coding in IPython, so feel free to use your favorite editor or IDE. But when you're done, remember to put your code into a notebook for your write up.\n\n#### Collaboration ####\nThis assignment is to be done individually. Everyone should be getting a hands on experience in this course. You are free to discuss course material with fellow students, and we encourage you to use Internet resources to aid your understanding, but the work you turn in, including all code and answers, must be your own work.\n\n#### Part 0: Preliminaries ####\nExercise 0\nDownload the data and unzip it. Read each file in from the file system, and store them as lists of lines.\n\nFor each of the data files (\"Google.csv\", \"Amazon.csv\", and the samples), we want to parse the IDs out of each record. The IDs are the first column of the file (they are URLs for Google, and alphanumeric strings for Amazon). Omitting the headers, load these data files into dictionaries mapping ID to a string containing the rest of the record.\n\n`In [ ]:\n*# Please preserve the format of this line so we can use it for automated testing.*\nDATA_PATH = \"\" # Make this the /path/to/the/data\n\n*# TODO Load data files here...*`\nPart 1: ER as Text Similarity¶\nA simple approach to entity resolution is to treat all records as strings and compute their similarity with a string distance function. In this section, we will build some components for bag-of-words text-analysis, and use them to compute record similarity.\n\n1.1 Bags of Words\nBag-of-words is a conceptually simple yet powerful approach to text analysis. The idea is to treat strings, a.k.a. documents, as unordered collections of words, or tokens, i.e., as bags of words.\n\n> Note on terminology: \"token\" is more general than what we ordinarily mean by \"word\" and includes things like numbers, acronyms, and other exotica like word-roots and fixed-length character strings. Bag of words techniques all apply to any sort of token, so when we say \"bag-of-words\" we really mean \"bag-of-tokens,\" strictly speaking.\nTokens become the atomic unit of text comparison. If we want to compare two documents, we count how many tokens they share in common. If we want to search for documents with keyword queries (this is what Google does), then we turn the keywords into tokens and find documents that contain them.\n如果我们希望比较两个文档，我们要计数他们有多少相同的tokens.\n\nThe power of this approach is that it makes string comparisons insensitive to small differences that probably do not affect meaning much, for example, punctuation and word order.\n这种方法的强大之处在于，它使得字符串比较对小的差异不敏感，而这些差异可能不太影响意义，例如，标点和词序\nExercise 1\na. Implement the function simple_tokenize(string) that takes a string and returns a list of tokens in the string. simple_tokenize should split strings using the provided regular expression. Since we want to make token-matching case insensitive, make sure all tokens are lower-case. Give an interpretation, in natural language, of what the regular expression, split_regex, matches.\n参数：一个字符串\n返回值：string类型的列表，内容是tokens\n使用正则表达式分割字符串\n因为我们希望标记匹配对大小写不敏感，所以要保证所有的标记都是小写字母。\n`In [ ]:\nimport re\n\nquickbrownfox = \"A quick brown fox jumps over the lazy dog.\"\nsplit_regex = r'\\W+'\n\n*# TODO Implement this*\ndef simple_tokenize(string):\n    pass\n\nprint simple_tokenize(quickbrownfox) # Should give ['a', 'quick', 'brown', ... ]\n\n*# Simple testcases*\nassert(simple_tokenize(\" \") == [])\nassert(simple_tokenize(\"!!!!123A/456_B/789C.123A\") == [\"123a\",\"456_b\",\"789c\",\"123a\"])\nassert(simple_tokenize(quickbrownfox) == \n        ['a','quick','brown','fox','jumps','over','the','lazy','dog'])\nTODO Answer question a above here (click here, hit esc-Enter to edit, enter your answer, then shift-Enter to exit)`\nb. Stopwords are common words that do not contribute much to the content or meaning of a document (e.g., \"the\", \"a\", \"is\", \"to\", etc.). Stopwords add noise to bag-of-words comparisons, so the are usually excluded. Using the included file \"stopwords.txt\", implement tokenize, an improved tokenizer that does not emit stopwords.\nstopwords是常见但对文档内容贡献不大的词语(e.g., \"the\", \"a\", \"is\", \"to\", etc.)，他们给比较增加了噪声，所以通常被排除。\n`In [ ]:\nstopwords = [] # Load from file\n\n*# TODO Implement this*\ndef tokenize(string):\n    pass\n\nprint tokenize(quickbrownfox) # Should give ['quick', 'brown', ... ]\n\nassert(tokenize(\"Why a the?\") == [])\nassert(tokenize(quickbrownfox) == ['quick','brown','fox','jumps','lazy','dog'])`\nc. Now let's tokenize the two small data sets. **For each one build a dictionary of tokens**, i.e., a dictionary where the record IDs are the keys, and the output of tokenize is the values. Include tokens for the name, description, and manufacturer fields, but not the price field. How many tokens, total, are there in the two data sets? Which Amazon record has the biggest number of tokens?\n\n```In [ ]:\n*# TODO Compute these (dict() or DataFrame OK)*\namazon_rec2tok = {}\ngoogle_rec2tok = {}\n\ntotal_tokens = 0 # TODO Fix me\nprint 'There are %s tokens in the combined data sets' % total_tokens\n\nbiggest_record = \"\" # TODO Fix me\nprint 'The Amazon record with ID \"%s\" has the most tokens' % biggest_record```\n## 1.2 Weighted Bag-of-Words: TF-IDF ##\nBag-of-words comparisons are not very good when all tokens are treated the same: some tokens are more important than others. Weights give us a way to specify which tokens to favor. With weights, when we compare documents, instead of counting common tokens, we sum up the weights of common tokens.\n\nA good heuristic for assigning weights is called \"Term-Frequency/Inverse-Document-Frequency,\" or TF-IDF for short.\n\n\n> TF\nTF rewards tokens that appear many times in the same document. It is computed as the frequency of a token in a document, that is, if document d contains 100 tokens and token t appears in d 5 times, then the TF weight of t in d is 5/100 = 1/20. The intuition for TF is that if a word occurs often in a document, then it is more important to the meaning of the document.\n\n\n> IDF\nIDF rewards tokens that are rare overall in a data set. The intuition is that it is more significant if two documents share a rare word than a common one. IDF weight for a token, t, in a set of documents, U, is computed as follows:\n\nLet N be the total number of documents in U\nFind n(t), the number of documents in U that contain t\nThen IDF(t) = N/n(t).\nNote that n(t)/N is the frequency of t in U, and N/n is the inverse frequency.\n\n\n> Note on terminology: Sometimes token weights depend on the document the token belongs to, that is, the same token may have a different weight when it's found in different documents. We call these weights local weights. TF is an example of a local weight, because it depends on the length of the source. On the other hand, some token weights only depend on the token, and are the same everywhere that token is found. We call these weights global, and IDF is one such weight.\n#### TF-IDF ####\nFinally, to bring it all together, the total TF-IDF weight for a token in a document is the product of its TF and IDF weights.\n\n## Exercise 2 ##\na. Implement tf(tokens) that takes a list of tokens belonging to a single document and returns a dictionary mapping tokens to TF weights.\n\n`In [ ]:\n# TODO Implement this\ndef tf(tokens):\n    pass\n\nprint tf(tokenize(quickbrownfox)) # Should give { 'quick': 0.1666 ... }`\nb. Implement idfs that assigns an IDF weight to every unique token in a collection of data called corpus. You may structure corpus however you want, but idfs should return a dictionary mapping tokens to weights. Use idfs to compute IDF weights for all tokens in the combined small data sets. How many unique tokens are there?\n\nIn [ ]:\n# TODO Implement this\ndef idfs(corpus):\n    pass\n\nidfs_small = {} # Use find_idfs here\n\nunique_tokens = 0 # Fix me\n\nprint \"There are %s unique tokens in the small data sets.\" % unique_tokens\nc. What are the 10 tokens with the smallest IDF in the combined small data set? Do you think they are useful for entity resolution? Why or why not?\n\nIn [ ]:\nsmall_idf_tokens = [] # TODO Compute me\n\nprint small_idf_tokens\nTODO Answer question c here (click, esc-Enter, edit, shift-Enter)\nd. Plot a histogram of IDF values. Be sure to use appropriate scaling and bucketing for the data. What conclusions can you draw from the distribution of weights?\n\nIn [ ]:\nimport pylab\n%matplotlib inline\n\n# TODO Make a plot. HINT: You can use pylab.hist\nTODO Answer question d\ne. Use tf to implement tfidf(tokens, idfs) that takes a list of tokens from a document and a dictionary of idf weights and returns a dictionary mapping tokens to total TF-IDF weight. Use tfidf to compute the weights of Amazon product record 'b000hkgj8k'.\n\nIn [ ]:\n# TODO Implement this\ndef tfidf(tokens, idfs):\n    pass\n\nrec_b000hkgj8k_weights = find_tfidf(None, None) # Fix me\n\nprint \"Amazon record 'b000hkgj8k' has tokens and weights:\\n%s\" % rec_b000hkgj8k_weights\n1.3 Cosine Similarity\nNow we are ready to do text comparisons in a formal way. The metric of string distance we will use is called cosine similarity. We will treat each document as a vector in some high dimensional space. Then, to compare two documents we compute the cosine of the angle between their two document vectors. This is easier than it sounds.\n\nThe first question to answer is how do we represent documents as vectors? The answer is familiar: bag-of-words! We treat each unique token as a dimension, and treat token weights as magnitudes in their respective token dimensions. For example, suppose we use simple counts as weights, and we want to interpret the string \"Hello, world! Goodbye, world!\" as a vector. Then in the \"hello\" and \"goodbye\" dimensions the vector has value 1, in the \"world\" dimension it has value 2, and it is zero in all other dimensions.\n\nNext question is: given two vectors how do we find the cosine of the angle between them? Recall the formula for the dot product of two vectors:\n\na⋅b=∥a∥∥b∥cosθ\na⋅b=‖a‖‖b‖cos⁡θ\nHere a⋅b=∑ni=1aibia⋅b=∑i=1naibi is the ordinary dot product of two vectors, and ∥a∥=∑ni=1a2i−−−−−−√‖a‖=∑i=1nai2 is the norm of aa.\n\nWe can rearrange terms and solve for the cosine to find it is simply the normalized dot product of the vectors. With our vector model, the dot product and norm computations are simple functions of the bag-of-words document representations, so we now have a formal way to compute similarity:\n\nsimilarity=cosθ=a⋅b∥a∥∥b∥=∑ni=1aibi∑ni=1a2i−−−−−−√∑ni=1b2i−−−−−−√\nsimilarity=cos⁡θ=a⋅b‖a‖‖b‖=∑i=1naibi∑i=1nai2∑i=1nbi2\nSetting aside the algebra, the geometric interpretation is more intuitive. The angle between two document vectors is small if they share many tokens in common, because they are pointing in roughly the same direction. Then, the cosine of the angle will be large. Otherwise, if the angle is large (and they have few words in common), the cosine is small. So the cosine scales proportionally with our intuitive sense of similarity.\n\nExercise 3\na. Implement cosine_similarity(string1, string2, idfs) that takes two strings and computes their cosine similarity in the context of some global IDF weights. Use tokenize, tfidf, and the IDF weights from exercise 2b for extracting tokens and assigning them weights.\n\nIn [ ]:\nimport math\n\n# Optional utility\ndef dotprod(a, b):\n    pass\n\n# Optional utility\ndef norm(a):\n    pass\n\n# Optional freebie\ndef cossim(a, b):\n    return dotprod(a, b) / norm(a) / norm(b)\n\ntest_vec1 = {'foo': 2, 'bar': 3, 'baz': 5 }\ntest_vec2 = {'foo': 1, 'bar': 0, 'baz': 20 }\nprint dotprod(test_vec1, test_vec2), norm(test_vec1) # Should be 102 6.16441400297\n\n# TODO Implement this\ndef cosine_similarity(string1, string2, idfs):\n    pass\n\nprint cosine_similarity(\"Adobe Photoshop\",\n                        \"Adobe Illustrator\", \n                        idfs_small) # Should be 0.0577243382163\nb. Now we can finally do some entity resolution! For every product record in the small Google data set, use cosine_similarity to compute its similarity to every record in the small Amazon set. Build a dictionary mapping (Amazon Id, Google Id) tuples to similarity scores between 0 and 1. What is the similarity between Amazon record 'b000o24l3q' and Google record http://www.google.com/base/feeds/snippets/17242822440574356561.\n\nIn [ ]:\n# TODO Compute similarities\nsimilarities = {}\n\nprint 'Requested similarity is %s.' % similarities[('b000o24l3q',\n  'http://www.google.com/base/feeds/snippets/17242822440574356561')]\nc. Use the \"gold standard\" data (loaded from the supplied file) to answer the following questions. How many true duplicate pairs are there in the small data sets? What is the average similarity score for true duplicates? What about for non-duplicates? Based on this, is cosine similarity doing a good job, qualitatively speaking, of identifying duplicates? Why or why not?\n\nIn [ ]:\ngold_standard = [] # Load this if not already loaded\n\ntrue_dups = 0 # Fix me\navg_sim_dups = 0.0 # Fix me\navg_sim_non = 0.0 # Fix me\n\nprint \"There are %s true duplicates.\" % true_dups\nprint \"The average similarity of true duplicates is %s.\" % avg_sim_dups\nprint \"And for non duplicates, it is %s.\" % avg_sim_non\nTODO Answer question c here\nPart 2: Scalable ER\nIn the previous section we built a text similarity function and used it for small scale entity resolution. Our implementation is limited by its quadratic run time complexity, and is not practical for even modestly sized data sets. In this section we will implement a more scalable algorithm and use it to do entity resolution on the full data set.\n\nInverted Indices\nTo improve our ER algorithm from Part 1, we should begin by analyzing its running time. In particular, the algorithm above is quadratic in two ways. First, we did a lot of redundant computation of tokens and weights, since each record was reprocessed every time it was compared. Second, we made qudratically many token comparisons between records.\n\nThe first source of quadratic overhead can be eliminated with precomputation and look-up tables, but the second source is a little more tricky. In the worst case, every token in every record in one data set exists in every record in the other data set, and therefore every token makes a nonzero contribution to the cosine similarity. In this case, token comparison is unavoidably quadratic.\n\nBut in reality most records have nothing (or very little) in common. Moreover, it is typical for a record in one data set to have at most one duplicate record in the other data set (this is the case assuming each data set has been de-duplicated against itself). In this case, the output is linear in the size of the input and we can hope to achieve linear running time.\n\nAn inverted index is a data structure that will allow us to avoid making quadratically many token comparisons. It maps each token in the data set to the list of documents that contain the token. So, instead of comparing, record by record, each token to every other token to see if they match, we will use inverted indices to look up records that match on a particular token.\n\nNote on terminology: In text search, a forward index maps documents in a data set to the tokens they contain. An inverted index supports the inverse mapping.\nExercise 4\nNote: For this section, use the complete Google and Amazon data sets, not the samples\n\nPandas note: If you use DataFrames for the mapping tables, make sure you index them correctly for efficient key look ups\na. To address the overhead of recomputing tokens and token-weights, build a dictionary for each data set that maps record IDs to TF-IDF weighted token vectors (the vectors themselves should be dictionaries). You will need to re-use code from above to recompute IDF weights for the complete combined data set.\n\nIn [ ]:\n# TODO Redo tokenization for full data set\namazon_rec2tok = {}\ngoogle_rec2tok = {}\n\n# TODO Recompute IDFs for full data set\nidfs_full = {}\n\n# TODO Pre-compute TF-IDF weights.  Build mappings from record ID weight vector.\ngoogle_weights = {}\namazon_weights = {}\n\n# TODO Pre-compute norms.  Build mappings from record ID to norm of the weight vector.\ngoogle_norms = {}\namazon_norms = {}\nb. Build inverted indices of both data sources.\n\nIn [ ]:\n# TODO Implement this. Should return a mapping from token to list-of-record-IDs\ndef invert_index(forward_index):\n    pass\n\n# TODO Pre-compute inverted indices\namazon_inv = invert_index(amazon_weights)\ngoogle_inv = invert_index(google_weights)\nc. We are now in position to efficiently perform ER on the full data sets. Implement the following algorithm to build a dictionary that maps a pair of records (as a tuple) to a list of tokens they share in common: Iterate over tokens of one data set, and for each token, if the token appears in the other data set, use the inverted indices to find all pairs of records (one from either set) that contain the token. Add these pairs to the output.\n\nIn [ ]:\nimport itertools\n\n# TODO Implement algorithm to compute this:\ncommon_tokens = {} # Should map a record ID pair to a list of common tokens\n\nprint len(common_tokens) # Should be 2441100\nd. Use the data structures from parts a and c to build a dictionary to map record pairs to cosine similarity scores.\n\nIn [ ]:\n# TODO Implement this. Should take two record IDs and a list of common\n# tokens and return the cosine similarity of the two records.\n# Use results from part *a* for fast look ups.\ndef fast_cosine_similarity(a_rec, g_rec, tokens):\n    pass\n\n# TODO Compute similarities (use fast_cosine_similarity)\nsims = {} # Should map record-ID-pairs to cosine similarity score\nAnalysis\nNow we have an authoritative list of record-pair similarities, but we need a way to use those similarities to decide if two records are duplicates or not. The simplest approach is to pick a threshold. Pairs whose similarity is above the threshold are declared duplicates, and pairs below the threshold are declared distinct.\n\nTo decide where to set the threshold we need to understand what kind of errors result at different levels. If we set the threshold too low, we get more false positives, that is, record-pairs we say are duplicates that in reality are not. If we set the threshold too high, we get more false negatives, that is, record-pairs that really are duplicates but that we miss.\n\nER algorithms are evaluated by the common metrics of information retrieval and search called precision and recall. Precision asks of all the record-pairs marked duplicates, what fraction are true duplicates? Recall asks of all the true duplicates in the data, what fraction did we successfully find? As with false positives and false negatives, there is a trade-off between precision and recall. A third metric, called F-measure, takes the harmonic mean of precision and recall to measure overall goodness in a single value.:\n\nFmeasure=2precision∗recallprecision+recall\nFmeasure=2precision∗recallprecision+recall\nExercise 5\nNote: For this exercise, use the \"gold standard\" mapping from the included file to look up true duplicates, and the results of exercise 4.\na. Implement functions to count true-positives (true duplicates above the threshold), and false-positives and -negatives. HINT: To make your functions efficient, you should bin your counts by similarity range.\n\nIn [ ]:\n# Look up all similarity scores for true duplicates\ntrue_dup_sims = [] # TODO Build this\n\n# TODO Just compute true_dup_sim above\ndef truepos(threshold):\n    return len(true_dup_sims) - falseneg(threshold)\n\n# Pre-bin counts of false positives by threshold range\nnthresholds = 100\ndef bin(similarity):\n    return int(similarity * nthresholds)\n\nfp_counts = {} # TODO Build this.  Should map bin number to count of false-positives\n\n# TODO Implement this\ndef falsepos(threshold):\n    pass\n\n# TODO Implement this\ndef falseneg(threshold):\n    pass\nb. What is the relationship between false-positives and -negatives (and true-positives and -negatives) on the one hand and precision and recall on the other? Use the functions from part a to implement functions to compute precision, recall, F-measure as a function of threshold value.\n\nIn [ ]:\n# TODO Implement this (returns a float)\ndef precision(threshold):\n    pass\n\n# TODO Implement this (returns a float)\ndef recall(threshold):\n    pass\n\n# TODO Implement this (returns a float)\ndef fmeasure(threshold):\n    pass\nc. Make line plots of precision, recall, and F-measure as a function of threshold value, for thresholds between 0.0 and 1.0. You can change nthresholds (above in part a) to change threshold values to plot.\n\nIn [ ]:\n# For the x-axis\nthresholds = [float(n) / nthresholds for n in range(0, nthresholds)]\n\n# TODO Make a plot.  HINT: Use pylab.plot().  Don't forget labels.\nd. Using the plot, pick the optimal threshold value and argue for why it is optimal. If false-positives are considered much worse than false-negatives, how does that change your answer?\n\nTODO Answer question d\ne. State-of-the-art tools can get an F-measure of about 60% on this data set. In this assignment we expect to get an F-measure closer to 40%. Look at some examples of errors (both false-positives and -negatives) and think about what went wrong. What are some ways we might improve our simple classifier? Back up your ideas with examples as much as possible.\n\nTODO Answer question e\nThis website does not host notebooks, it only renders notebooks available on other websites.\n\nDelivered by Fastly, Rendered by Rackspace\n\nnbviewer GitHub repository.\n\nnbviewer version: 89297bd\n\nnbconvert version: 5.3.1\n\nRendered 38 minutes ago","tags":["DataScience","SDU"],"categories":["DataScience"]},{"title":"碎碎念 - 20171025","url":"/2017/10/25/diary171025/","content":"这几天颓的不行，睡过去就醒不过来的那种头疼。\n今天软件工程实验课签到了，我又没去……每次签到必不到体质也是没谁了，但也怨不得谁，自己做的决定，后果自己承担。\n不过也借此对一些人的认识更深了吧……呵呵哒。再忍一年半，早点江湖不见吧。\n全是负能量，真不开心，以后还是不要点开这一天了。\n\n以后再心态爆炸就学习吧。\n背单词\n看微积分\n看CSAPP\n**只有知识不会背叛你。**","tags":["diary"],"categories":["diary"]},{"title":"常用数据集","url":"/2017/10/23/datasets/","content":"### Scikit-learn Toy\nScikit-learn数据集内嵌在Scikit-learn工具包中，可以通过Python导入命令直接加载，不需要从任何外部网络资源中下载。\n加载：\n{% codeblock lang:python %}\nfrom sklearn import datasets\niris=datasets.load_iris()\n{% endcodeblock %}\n基本上，所有的Scikit-learn数据集都提供以下方法：\n.DESCR:提供数据集总体描述（description前五个字母）\n.data:\n.feature_names:\n.target:用数值或类别号表示的分类值\n.target_names:\n.shape:可用于.data和.target方法，描述的是观测数据（第一个值）和特征（第二个值，如有）的数量。\n\n数据集对象包含的主要数据结构是两个数组：date和target\n\n\nToy数据集聚焦一个易于获得又不复杂的数据问题，能帮助你快速建立数据科学的基础。\n\n### mldata.org公共资源库 ###\n可从机器学习数据资源库或LIBSVM数据网站上直接下载，需要访问互联网。\n{% codeblock lang:python %}\nfrom sklearn.datasets import fetch_mldata\nearthquakes=fetch_mldata('global-earthquakes')\nprint(earthquakes.data)\nprint(earthquakes.data.shape)\n{% endcodeblock %}\n\n得到对象结构类似字典，预测变量earthquakes.data,预测目标earthquakes.target\n\n### LIBSVM Data样本 ###\n提供了多种回归、二值化、多标号分类等LIBSVM格式的数据集，如果要用支持向量机的算法进行实验，这个资源库相当有用。\n要加载数据集先要访问数据集所在的网页。\n{% codeblock lang:python %}\n# 注意：在python3中，urllib2库已拆分为urllib.request与urllib.error\nfrom urllib.request import urlopen\ntarget_page='http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary/a1a'\na2a=urlopen(target_page)\nfrom sklearn.datasets import load_svmlight_file\nX_train,y_train=load_svmlight_file(a2a)\nprint(X_train.shape,y_train.shape)\n{% endcodeblock %}\n\n结果将得到两个对象：一组稀疏矩阵格式的训练样本和一个响应数组。\n\n### 直接从CSV或文本文件加载数据 ###\n可以通过网页浏览器或wget命令从资源库直接下载数据集\n加载数据可以用Numpyt的loadtxt函数和pandas的read_csv函数\n{% codeblock lang:python %}\nimport numpy as np\nhousing = np.loadtxt('regression-datasets-housing.csv',delimiter=',')\nprint(type(housing))\nprint(housing.shape)\n{% endcodeblock %}","tags":["DataScience"]},{"title":"等价交换与他","url":"/2017/10/22/diary171022/","content":"过去的一天真是黑暗，仿佛回到了大一那种感觉，沉浸在悲伤里走不出去。\n果然不管过多久，只有他能让我疯疯癫癫完全从生活里抽离吧。\n一步错步步错，我大概是永远都追不上他的脚步了。真可悲。\n说不介意嫁给军人，我是认真的吗。那时候是玩笑话，现在却要认真考虑了啊...完全丧失了喜欢人的能力，忘记了喜欢是什么感觉，大概就是现在这个样子吧。\n好像骂他傻，为什么要选国防生这条路。换做别人，我一定会觉得是活该吧，自己选的路自己受着，这很公平。\n可怎么到他身上就下不去嘴啊。这么优秀的人，我就是希望他顺顺利利平安喜乐的度过一生啊。军人也好，学者也好，身边的人是豆豆也好，是其他人也没有问题，只要他幸福快乐就好了。\n那我呢？我一直以为自己是真的喜欢buaa，可今天听说那个消息，突然觉得buaa和别的学校没什么两样，觉得一直支撑我考研的动力就这么轰然倒塌。\n我就想远远的能看见你就够了，这点愿望都不满足我啊，老天真过分。\n\n人不付出就不会有收获 \n我们所承爱的痛苦 \n一定是为了得到什么而付出的代价\n\n这段话送给你，也送给自己。\n\n16岁时穿着校服心动的感觉，一辈子都不会再有了。\n没有你的buaa，真是无趣。\n我所走的每一步，都是为了更靠近你。","tags":["diary"],"categories":["diary"]},{"title":"diary171018","url":"/2017/10/18/diary171018/","content":"看清前路，然后开始努力\n比你优秀又比你努力的人那么多呢","tags":["diary"],"categories":["diary"]},{"title":"SoftwareEngineerExerciseNote","url":"/2017/10/18/SoftwareEngineerExerciseNote/","content":"需求类 设计类 测试类 \n三选一 也可以多选\n可以用原来的项目\n### 不要源代码\n可以单干\n可以组队，必须写明分工","tags":["SDU","SoftwareEngineer"]},{"title":"碎碎念 2017-10-17","url":"/2017/10/17/diary171017/","content":"做好你自己认定的事就足够了。\n其他人的世界很精彩，但是那也是人家努力的结果，别瞎羡慕了。\n做好自己。\n坚持很难，一定要努力呀。\n\n*人不付出就不会有收获 \n我们所承爱的痛苦 \n一定是为了得到什么而付出的代价 *\n","tags":["diary"],"categories":["diary"]},{"title":"Nachos第六周课堂笔记User-level processes","url":"/2017/10/16/NachosClassNoteW6/","content":"系统调用\n运行时刻的应用程序如何陷入内核\n\ncoff：Unix世界中的格式\nnoff：Nachos专用格式，一个简化版的二进制文件格式\n\tNoff header文件头：从一开始开始，包含了各个段的指针，描述其他部分。unix用magicnumber标识文件类型。\nnachos现版本每次只支持一个进程运行，因为逻辑地址和物理地址是一对一的。若要支持多道程序运行，进程的切换需要写页面blabla，用nachos另外两个内核函数\nNachos的程序跑在虚拟的MIPS架构的Machine上，但Nachos本身不在Machine上跑。Nachos的上下文切换对比真实机器难度下降，因为两者跑的层次不同，对次序没有限制。\n进程可以使用系统调用，因为他其中存在一个系统调用的表\n系统调用号送到指定地方（r2寄存器）后调用syscall,syscall读取来得知需要做什么。要区分调用与syscall。\n每个进程里边有一个表，作为系统调用的存根。每一个名字和一个系统调用号关联（参考start.s,nachos给出13条，现在只有halt是完整的，剩余12条需要自己补足）\nsyscall是一条非特权指令。\nsyscall.h中全都是声明而非实现。","tags":["SDU","Nachos","OperatingSystem"],"categories":["Lesson"]},{"title":"Nachos源码解读笔记","url":"/2017/10/15/NachosGuide/","content":"摘录自网上广泛流传的《山东大学nachos源码解读》\n#### 1.Nachos内核启动\n1.1不同project使用的内核不同\n1.2内核可以通过Kernel.kernel调用\n#### 2.Nachos参数解析、设备创建、Debug方法\n2.1Nachos设备创建：Machine中的createDevices方法，依次创建**中断、定时器**等。\n#### 3.Nachos中断\n#### 4.Nachos内核线程及调度算法\n4.1 TCB：每个nachos线程对应一个TCB，负责处理Nachos线程调度的部分底层细节。\nprivilege对象赋给TCB类执行特权操作的权利。\n4.2每个调度器对应一个ThreadQueue子类。无论是线程的全局就绪队列，还是实现join方法时的等待队列，其队列都应该从调度器中获取。\n入队：waitForAccess,需要人工执行sleep\n出队：nextThread，出队后可执行run方法运行\n4.3对于PriorityScheduler提供了改变优先级的方法。优先级通过ThreadState包装KThread实现，增加优先级这一属性。在题目中实现自己的优先队列PriorityQueue，并选取合适的数据结构，可以让出队时间变为O(logn)","tags":["SDU","Nachos","OperatingSystem"],"categories":["Lesson"]},{"title":"NachosProject1笔记","url":"/2017/10/15/NachosNote1/","content":"\n- **assert:**程序一般分为Debug 版本和Release 版本，Debug 版本用于内部调试，Release 版本发行给用户使用。断言assert 是仅在Debug 版本起作用的宏，它用于检查“不应该”发生的情况。在运行过程中，如果assert 的参数为假，那么程序就会中止（一般地还会出现提示对话，说明在什么地方引发了assert）。\n- **内核线程与用户线程：**\n转自：http://col1.blog.163.com/blog/static/1909775192012719114033352/\n1 .内核级线程:切换由内核控制，当线程进行切换的时候，由用户态转化为内核态。切换完毕要从内核态返回用户态；可以很好的利用smp，即利用多核cpu。windows线程就是这样的。\n\n 2. 用户级线程内核的切换由用户态程序自己控制内核切换,不需要内核干涉，少了进出内核态的消耗，但不能很好的利用多核Cpu,目前Linux pthread大体是这么做的。\n\n线程的实现可以分为两类：用户级线程(User-Level Thread)和内核线线程(Kernel-Level Thread)，后者又称为内核支持的线程或轻量级进程。在多线程操作系统中，各个系统的实现方式并不相同，在有的系统中实现了用户级线程，有的系统中实现了内核级线程。\n\n用户线程指不需要内核支持而在用户程序中实现的线程，其不依赖于操作系统核心，应用进程利用线程库提供创建、同步、调度和管理线程的函数来控制用户线程。不需要用户态/核心态切换，速度快，操作系统内核不知道多线程的存在，因此一个线程阻塞将使得整个进程（包括它的所有线程）阻塞。由于这里的处理器时间片分配是以进程为基本单位，所以每个线程执行的时间相对减少。\n\n内核线程：由操作系统内核创建和撤销。内核维护进程及线程的上下文信息以及线程切换。一个内核线程由于I/O操作而阻塞，不会影响其它线程的运行。Windows NT和2000/XP支持内核线程。\n\n用户线程运行在一个中间系统上面。目前中间系统实现的方式有两种，即运行时系统（Runtime System）和内核控制线程。“运行时系统”实质上是用于管理和控制线程的函数集合，包括创建、撤销、线程的同步和通信的函数以及调度的函数。这些函数都驻留在用户空间作为用户线程和内核之间的接口。用户线程不能使用系统调用，而是当线程需要系统资源时，将请求传送给运行时，由后者通过相应的系统调用来获取系统资源。内核控制线程：系统在分给进程几个轻型进程（LWP），LWP可以通过系统调用来获得内核提供的服务，而进程中的用户线程可通过复用来关联到LWP，从而得到内核的服务。\n\n以下是用户级线程和内核级线程的区别：\n\n（1）内核支持线程是OS内核可感知的，而用户级线程是OS内核不可感知的。\n\n（2）用户级线程的创建、撤消和调度不需要OS内核的支持，是在语言（如Java）这一级处理的；而内核支持线程的创建、撤消和调度都需OS内核提供支持，而且与进程的创建、撤消和调度大体是相同的。\n\n（3）用户级线程执行系统调用指令时将导致其所属进程被中断，而内核支持线程执行系统调用指令时，只导致该线程被中断。\n\n（4）在只有用户级线程的系统内，CPU调度还是以进程为单位，处于运行状态的进程中的多个线程，由用户程序控制线程的轮换运行；在有内核支持线程的系统内，CPU调度则以线程为单位，由OS的线程调度程序负责线程的调度。\n\n（5）用户级线程的程序实体是运行在用户态下的程序，而内核支持线程的程序实体则是可以运行在任何状态下的程序。\n\n内核线程的优点：\n\n（1）当有多个处理机时，一个进程的多个线程可以同时执行。\n\n缺点：\n\n（1）由内核进行调度。\n\n用户进程的优点：\n\n（1） 线程的调度不需要内核直接参与，控制简单。\n\n（2） 可以在不支持线程的操作系统中实现。\n\n（3） 创建和销毁线程、线程切换代价等线程管理的代价比内核线程少得多。\n\n（4） 允许每个进程定制自己的调度算法，线程管理比较灵活。这就是必须自己写管理程序，与内核线程的区别\n\n（5） 线程能够利用的表空间和堆栈空间比内核级线程多。\n\n（6） 同一进程中只能同时有一个线程在运行，如果有一个线程使用了系统调用而阻塞，那么整个进程都会被挂起。另外，页面失效也会产生同样的问题。\n\n缺点：\n\n（1）资源调度按照进程进行，多个处理机下，同一个进程中的线程只能在同一个处理机下分时复用\n- **yield（）：**\n理论上，yield意味着放手，放弃，投降。一个调用yield()方法的线程告诉虚拟机它乐意让其他线程占用自己的位置。这表明该线程没有在做一些紧急的事情。注意，这仅是一个暗示，并不能保证不会产生任何影响。\n- **join():**\n线程实例的方法join()方法可以使得一个线程在另一个线程结束后再执行。如果join()方法在一个线程实例上调用，当前运行着的线程将阻塞直到这个线程实例完成了执行。\n Join()方法的含义：当前线程a在运行，执行b.join(),则a阻塞，直到线程b结束，a继续执行。\n加入就绪队列readyQueue.waitForAccess(this);\n变为就绪状态Machine.autoGrader().readyThread(this);\n**同步机制：**\n锁机制\n信号量机制(Semaphore)：包括无名线程信号量和命名线程信号量\n信号机制(Signal)：（用的Condition类）\n阻塞队列（java）\n一个线程的**有效优先级：**\n即其所属的等待队列中所有线程中最高的一个优先级数值","tags":["SDU","Nachos","OperatingSystem"],"categories":["Lesson"]},{"title":"碎碎念 2017-10-13","url":"/2017/10/14/diary171013/","content":"虽然是14号发的，但说起来应该算是13号的日记吧。\n碎碎念这个名字虽然是一时兴起瞎起的，但形式也的确就是碎碎念...写一篇连贯的文章太费脑子了，想到哪说到哪才比较像我嘛。\n\n今天翻了学长的微博,真是蛮可爱的人，并且还感觉好厉害呀...超级正能量，看我会有种我也会很厉害的错觉怎么破。\n\n说起来，很多大佬在你这个年纪已经非常厉害了，可你看看你呀，还是一条咸鱼啊，什么都不会。所以才更要努力啊，一定一定不要懈怠呀！\n\n*天下女子有情,宁有如杜丽娘者乎!梦其人即病,病即弥连,至手画形容传于世而后死.死三年矣,复能溟莫中求得其所梦者而生.如丽娘者,乃可谓之有情人耳.情不知所起,一往而深,生者可以死,死可以生.生而不可与死,死而不可复生者,皆非情之至也.梦中之情,何必非真,天下岂少梦中之人耶?必因荐枕而成亲,待挂冠而为密者,皆形骸之论也。*\n\n*童话故事虽美，不如这雨夜珍贵。*\n\n我永远喜欢SingleDogUniversity!","tags":["diary"],"categories":["diary"]},{"title":"Nachos课设要求翻译 Ⅰ","url":"/2017/10/13/NachosGuideBookTranslation1/","content":"虽然不是看不懂，但密密麻麻的英语小字实在是辣眼睛，还是翻译一下，顺便逼自己加快进度别懒了。。。_(:з」∠)_\n### Phase 1 为内核进程建立线程系统\n#### Tasks:\n1.（5%，5行，5/100）实现KThread.join()\n\t注意join方法只能被调用一次，即使是不同的线程也禁止二次调用。\n2.（5%，20行，10/100）直接实现条件变量，通过开关中断来提供原子性。\n\t已经用信号量实现了一个例子，你的任务是不用信号量来完成等价的实现。\n\t可以用锁，即使锁也是间接使用了信号量。\n\t你完成的版本在类nachos.threads.Condition2中。\n3.（10%，40行，20/100）实现waitUntil(long x)方法来完成Alarm类。\n\t调用这个函数可以挂起自己本身的执行，直到时间now+x。\n\t（这对实时操作的线程很有用。举例来说，每秒光标闪烁一次。）\n\t线程被唤醒后不需要立即开始执行，只要在它们等待一定时间后，将它们放在定时器中断处理程序的就绪队列中即可。\n\t不要fork多余的线程来实现waitUntil,你只需要修改waitUnti()和定时器中断处理程序。\n\twaitUntil不只限于一个线程，任何数量的线程可以同时调用并被挂起。\n4.（20%，40/100）实现同步收发一个词的消息。用条件变量，不要用信号量！\n**flag:不看代码下边讲解愣是一个字没看懂。。。喵的，待会看完源码回来翻译**\n5.（35%，75/100）通过补全PriorityScheduler类实现优先级调度。\n\t实时调度是建造实时系统block的关键。\n\t为了使用你的优先级调度，你需要更改nachos.conf中的一行来指明要用的调度程序类。ThreadedKernel.scheduler初始值为nachos.threads.RoundRobinScheduler。当你想用优先级调度来运行Nachos时，你需要将它改成nachos.threads.PriorityScheduler。\n\t必须实现方法:getPriority(),getEffectivePriority()与setPriority().\n\t可选实现方法：increasePriority()与decreasePriority()\n\t在选择哪个线程出列时，调度程序总应该选择效率最高的线程。如果有许多相同高优先级的线程在等待，调度程序要选等待最久的那个。\n\t- 优先级转化问题：如果一个高优先级线程需要等待一个低优先级线程，并且另一个高优先级线程已经在就绪列表中，那么这个高优先级线程将永远不能得到CPU,因为低优先级线程将不会获得任何CPU时间。\n\t- 一个部分的修正方案是当等待的线程持有锁时，将它的优先级donate给低优先级线程。\n\t- 实现优先级调度目的是让它在可能的时候捐出自己的优先级。确保实现在考虑了自己收到的所有donations后返回线程优先级的Scheduler.getEffectivePriority()\n\t- 当解决优先级捐赠问题时，你将会发现你可以很容易的计算一个线程的有效优先级，但是计算会占很长时间。为了在设计project的这一方面时得到满分，你应该通过缓存有效优先级并只在可能变化时重新计算线程的有效优先级来加速。\n\t- 注意在实现这一部分时不要破坏抽象的barriers——类Lock不需要修改。优先级捐赠应该通过创建一个实现了此功能的ThreadQueue的子类来完成，并且当使用已经存在的Semaphore与Condition类时应该正常运行。优先级同样应该通过线程join来捐赠。\n\t- 优先级捐赠实现细节：\n\t\t①一个线程的有效优先级通过计算捐赠者与接受者优先级的最大值确定。如果优先级4的线程A捐赠个线程B优先级2，那么线程B的有效优先级现在是4.注意线程A的优先级仍然是4.一个捐赠给其他线程优先级的线程不会损失自己的优先级。因此，“优先级继承”这个术语比“优先级捐赠”更合适。\n\t\t②优先级捐赠是传递性的。如果线程A捐赠给线程B，随后线程B捐赠给线程C，线程B将会捐赠它从A处获得后的新的有效优先级给C。\n\n\n6.（25%，150行，100/100）对这个问题，条件变量是最有效的同步方法。*一些夏威夷大人和小孩想从Oahu去Molokai。但不幸的是，他们只有一艘最多可以同时载两个孩子或者一个大人的船（一个大人一个小孩是不行的），只要有人划，船可以回到Oahu*\n\t- 假定至少有两个孩子，安排一个可以将所有人从Oahu送到Molokai的方案。\n\t- 方法Boat.begin()应该为每个人创建一个进程。我们要将Boat.begin()作为父进程。你的机制不能依赖于事先知道有多少个孩子或大人，尽管你可以随意尝试在线程间定义这些。（也就是说，你不可以在方法*begin()*中传递参数大人与孩子，但如果你想的话，你可以使每个线程增加共享变量来尝试确定这个值）\n\t- 为了展示这趟旅程是正确的同步了的，每次有人穿越河道时都要恰当调用BoatGrader方法。当一个孩子驾船从Oahu到Molokai时，调用ChildRowToMolokai。当一个孩子作为乘客从O到M时，调用ChildRideToMolokai。确保当两个人过河时,先调用...RideTo...方法，再调用...RowTo...方法。\n\t- 你的解决方案不能有忙等，并且最终必须能结束。\n","tags":["SDU","Nachos","OperatingSystem"],"categories":["Lesson"]},{"title":"MPI1","url":"/2017/10/12/MPI1/"},{"title":"机器学习中的范数规则化之（一）L0、L1与L2范数","url":"/2017/10/11/NormRegularization/","content":"转自http://m.blog.csdn.net/zouxy09/article/details/24971995\n\n机器学习中的范数规则化之（一）L0、L1与L2范数\nzouxy09@qq.com\nhttp://blog.csdn.net/zouxy09\n \n       今天我们聊聊机器学习中出现的非常频繁的问题：过拟合与规则化。我们先简单的来理解下常用的L0、L1、L2和核范数规则化。最后聊下规则化项参数的选择问题。这里因为篇幅比较庞大，为了不吓到大家，我将这个五个部分分成两篇博文。知识有限，以下都是我一些浅显的看法，如果理解存在错误，希望大家不吝指正。谢谢。\n \n       监督机器学习问题无非就是“minimizeyour error while regularizing your parameters”，也就是在规则化参数的同时最小化误差。最小化误差是为了让我们的模型拟合我们的训练数据，而规则化参数是防止我们的模型过分拟合我们的训练数据。多么简约的哲学啊！因为参数太多，会导致我们的模型复杂度上升，容易过拟合，也就是我们的训练误差会很小。但训练误差小并不是我们的最终目标，我们的目标是希望模型的测试误差小，也就是能准确的预测新的样本。所以，我们需要保证模型“简单”的基础上最小化训练误差，这样得到的参数才具有好的泛化性能（也就是测试误差也小），而模型“简单”就是通过规则函数来实现的。另外，规则项的使用还可以约束我们的模型的特性。这样就可以将人对这个模型的先验知识融入到模型的学习当中，强行地让学习到的模型具有人想要的特性，例如稀疏、低秩、平滑等等。要知道，有时候人的先验是非常重要的。前人的经验会让你少走很多弯路，这就是为什么我们平时学习最好找个大牛带带的原因。一句点拨可以为我们拨开眼前乌云，还我们一片晴空万里，醍醐灌顶。对机器学习也是一样，如果被我们人稍微点拨一下，它肯定能更快的学习相应的任务。只是由于人和机器的交流目前还没有那么直接的方法，目前这个媒介只能由规则项来担当了。\n       还有几种角度来看待规则化的。规则化符合奥卡姆剃刀(Occam's razor)原理。这名字好霸气，razor！不过它的思想很平易近人：在所有可能选择的模型中，我们应该选择能够很好地解释已知数据并且十分简单的模型。从贝叶斯估计的角度来看，规则化项对应于模型的先验概率。民间还有个说法就是，规则化是结构风险最小化策略的实现，是在经验风险上加一个正则化项(regularizer)或惩罚项(penalty term)。\n       一般来说，监督学习可以看做最小化下面的目标函数：\n\n       其中，第一项L(yi,f(xi;w)) 衡量我们的模型（分类或者回归）对第i个样本的预测值f(xi;w)和真实的标签yi之前的误差。因为我们的模型是要拟合我们的训练样本的嘛，所以我们要求这一项最小，也就是要求我们的模型尽量的拟合我们的训练数据。但正如上面说言，我们不仅要保证训练误差最小，我们更希望我们的模型测试误差小，所以我们需要加上第二项，也就是对参数w的规则化函数Ω(w)去约束我们的模型尽量的简单。\n        OK，到这里，如果你在机器学习浴血奋战多年，你会发现，哎哟哟，机器学习的大部分带参模型都和这个不但形似，而且神似。是的，其实大部分无非就是变换这两项而已。对于第一项Loss函数，如果是Square loss，那就是最小二乘了；如果是Hinge Loss，那就是著名的SVM了；如果是exp-Loss，那就是牛逼的 Boosting了；如果是log-Loss，那就是Logistic Regression了；还有等等。不同的loss函数，具有不同的拟合特性，这个也得就具体问题具体分析的。但这里，我们先不究loss函数的问题，我们把目光转向“规则项Ω(w)”。\n       规则化函数Ω(w)也有很多种选择，一般是模型复杂度的单调递增函数，模型越复杂，规则化值就越大。比如，规则化项可以是模型参数向量的范数。然而，不同的选择对参数w的约束不同，取得的效果也不同，但我们在论文中常见的都聚集在：零范数、一范数、二范数、迹范数、Frobenius范数和核范数等等。这么多范数，到底它们表达啥意思？具有啥能力？什么时候才能用？什么时候需要用呢？不急不急，下面我们挑几个常见的娓娓道来。\n \n一、L0范数与L1范数\n       L0范数是指向量中非0的元素的个数。如果我们用L0范数来规则化一个参数矩阵W的话，就是希望W的大部分元素都是0。这太直观了，太露骨了吧，换句话说，让参数W是稀疏的。OK，看到了“稀疏”二字，大家都应该从当下风风火火的“压缩感知”和“稀疏编码”中醒悟过来，原来用的漫山遍野的“稀疏”就是通过这玩意来实现的。但你又开始怀疑了，是这样吗？看到的papers世界中，稀疏不是都通过L1范数来实现吗？脑海里是不是到处都是||W||1影子呀！几乎是抬头不见低头见。没错，这就是这节的题目把L0和L1放在一起的原因，因为他们有着某种不寻常的关系。那我们再来看看L1范数是什么？它为什么可以实现稀疏？为什么大家都用L1范数去实现稀疏，而不是L0范数呢？\n       L1范数是指向量中各个元素绝对值之和，也有个美称叫“稀疏规则算子”（Lasso regularization）。现在我们来分析下这个价值一个亿的问题：为什么L1范数会使权值稀疏？有人可能会这样给你回答“它是L0范数的最优凸近似”。实际上，还存在一个更美的回答：任何的规则化算子，如果他在Wi=0的地方不可微，并且可以分解为一个“求和”的形式，那么这个规则化算子就可以实现稀疏。这说是这么说，W的L1范数是绝对值，|w|在w=0处是不可微，但这还是不够直观。这里因为我们需要和L2范数进行对比分析。所以关于L1范数的直观理解，请待会看看第二节。\n       对了，上面还有一个问题：既然L0可以实现稀疏，为什么不用L0，而要用L1呢？个人理解一是因为L0范数很难优化求解（NP难问题），二是L1范数是L0范数的最优凸近似，而且它比L0范数要容易优化求解。所以大家才把目光和万千宠爱转于L1范数。\n\n       OK，来个一句话总结：L1范数和L0范数可以实现稀疏，L1因具有比L0更好的优化求解特性而被广泛应用。\n       好，到这里，我们大概知道了L1可以实现稀疏，但我们会想呀，为什么要稀疏？让我们的参数稀疏有什么好处呢？这里扯两点：\n1）特征选择(Feature Selection)：\n       大家对稀疏规则化趋之若鹜的一个关键原因在于它能实现特征的自动选择。一般来说，xi的大部分元素（也就是特征）都是和最终的输出yi没有关系或者不提供任何信息的，在最小化目标函数的时候考虑xi这些额外的特征，虽然可以获得更小的训练误差，但在预测新的样本时，这些没用的信息反而会被考虑，从而干扰了对正确yi的预测。稀疏规则化算子的引入就是为了完成特征自动选择的光荣使命，它会学习地去掉这些没有信息的特征，也就是把这些特征对应的权重置为0。\n2）可解释性(Interpretability)：\n       另一个青睐于稀疏的理由是，模型更容易解释。例如患某种病的概率是y，然后我们收集到的数据x是1000维的，也就是我们需要寻找这1000种因素到底是怎么影响患上这种病的概率的。假设我们这个是个回归模型：y=w1*x1+w2*x2+…+w1000*x1000+b（当然了，为了让y限定在[0,1]的范围，一般还得加个Logistic函数）。通过学习，如果最后学习到的w*就只有很少的非零元素，例如只有5个非零的wi，那么我们就有理由相信，这些对应的特征在患病分析上面提供的信息是巨大的，决策性的。也就是说，患不患这种病只和这5个因素有关，那医生就好分析多了。但如果1000个wi都非0，医生面对这1000种因素，累觉不爱。\n \n二、L2范数\n       除了L1范数，还有一种更受宠幸的规则化范数是L2范数: ||W||2。它也不逊于L1范数，它有两个美称，在回归里面，有人把有它的回归叫“岭回归”（Ridge Regression），有人也叫它“权值衰减weight decay”。这用的很多吧，因为它的强大功效是改善机器学习里面一个非常重要的问题：过拟合。至于过拟合是什么，上面也解释了，就是模型训练时候的误差很小，但在测试的时候误差很大，也就是我们的模型复杂到可以拟合到我们的所有训练样本了，但在实际预测新的样本的时候，糟糕的一塌糊涂。通俗的讲就是应试能力很强，实际应用能力很差。擅长背诵知识，却不懂得灵活利用知识。例如下图所示（来自Ng的course）：\n\n       上面的图是线性回归，下面的图是Logistic回归，也可以说是分类的情况。从左到右分别是欠拟合（underfitting，也称High-bias）、合适的拟合和过拟合（overfitting，也称High variance）三种情况。可以看到，如果模型复杂（可以拟合任意的复杂函数），它可以让我们的模型拟合所有的数据点，也就是基本上没有误差。对于回归来说，就是我们的函数曲线通过了所有的数据点，如上图右。对分类来说，就是我们的函数曲线要把所有的数据点都分类正确，如下图右。这两种情况很明显过拟合了。\n\n       OK，那现在到我们非常关键的问题了，为什么L2范数可以防止过拟合？回答这个问题之前，我们得先看看L2范数是个什么东西。\n       L2范数是指向量各元素的平方和然后求平方根。我们让L2范数的规则项||W||2最小，可以使得W的每个元素都很小，都接近于0，但与L1范数不同，它不会让它等于0，而是接近于0，这里是有很大的区别的哦。而越小的参数说明模型越简单，越简单的模型则越不容易产生过拟合现象。为什么越小的参数说明模型越简单？我也不懂，我的理解是：限制了参数很小，实际上就限制了多项式某些分量的影响很小（看上面线性回归的模型的那个拟合的图），这样就相当于减少参数个数。其实我也不太懂，希望大家可以指点下。\n       这里也一句话总结下：通过L2范数，我们可以实现了对模型空间的限制，从而在一定程度上避免了过拟合。\n       L2范数的好处是什么呢？这里也扯上两点：\n1）学习理论的角度：\n       从学习理论的角度来说，L2范数可以防止过拟合，提升模型的泛化能力。\n2）优化计算的角度：\n       从优化或者数值计算的角度来说，L2范数有助于处理 condition number不好的情况下矩阵求逆很困难的问题。哎，等等，这condition number是啥？我先google一下哈。\n       这里我们也故作高雅的来聊聊优化问题。优化有两大难题，一是：局部最小值，二是：ill-condition病态问题。前者俺就不说了，大家都懂吧，我们要找的是全局最小值，如果局部最小值太多，那我们的优化算法就很容易陷入局部最小而不能自拔，这很明显不是观众愿意看到的剧情。那下面我们来聊聊ill-condition。ill-condition对应的是well-condition。那他们分别代表什么？假设我们有个方程组AX=b，我们需要求解X。如果A或者b稍微的改变，会使得X的解发生很大的改变，那么这个方程组系统就是ill-condition的，反之就是well-condition的。我们具体举个例子吧：\n\n       咱们先看左边的那个。第一行假设是我们的AX=b，第二行我们稍微改变下b，得到的x和没改变前的差别很大，看到吧。第三行我们稍微改变下系数矩阵A，可以看到结果的变化也很大。换句话来说，这个系统的解对系数矩阵A或者b太敏感了。又因为一般我们的系数矩阵A和b是从实验数据里面估计得到的，所以它是存在误差的，如果我们的系统对这个误差是可以容忍的就还好，但系统对这个误差太敏感了，以至于我们的解的误差更大，那这个解就太不靠谱了。所以这个方程组系统就是ill-conditioned病态的，不正常的，不稳定的，有问题的，哈哈。这清楚了吧。右边那个就叫well-condition的系统了。\n       还是再啰嗦一下吧，对于一个ill-condition的系统，我的输入稍微改变下，输出就发生很大的改变，这不好啊，这表明我们的系统不能实用啊。你想想看，例如对于一个回归问题y=f(x)，我们是用训练样本x去训练模型f，使得y尽量输出我们期待的值，例如0。那假如我们遇到一个样本x’，这个样本和训练样本x差别很小，面对他，系统本应该输出和上面的y差不多的值的，例如0.00001，最后却给我输出了一个0.9999，这很明显不对呀。就好像，你很熟悉的一个人脸上长了个青春痘，你就不认识他了，那你大脑就太差劲了，哈哈。所以如果一个系统是ill-conditioned病态的，我们就会对它的结果产生怀疑。那到底要相信它多少呢？我们得找个标准来衡量吧，因为有些系统的病没那么重，它的结果还是可以相信的，不能一刀切吧。终于回来了，上面的condition number就是拿来衡量ill-condition系统的可信度的。condition number衡量的是输入发生微小变化的时候，输出会发生多大的变化。也就是系统对微小变化的敏感度。condition number值小的就是well-conditioned的，大的就是ill-conditioned的。\n       如果方阵A是非奇异的，那么A的conditionnumber定义为：\n\n       也就是矩阵A的norm乘以它的逆的norm。所以具体的值是多少，就要看你选择的norm是什么了。如果方阵A是奇异的，那么A的condition number就是正无穷大了。实际上，每一个可逆方阵都存在一个condition number。但如果要计算它，我们需要先知道这个方阵的norm（范数）和Machine Epsilon（机器的精度）。为什么要范数？范数就相当于衡量一个矩阵的大小，我们知道矩阵是没有大小的，当上面不是要衡量一个矩阵A或者向量b变化的时候，我们的解x变化的大小吗？所以肯定得要有一个东西来度量矩阵和向量的大小吧？对了，他就是范数，表示矩阵大小或者向量长度。OK，经过比较简单的证明，对于AX=b，我们可以得到以下的结论：\n\n       也就是我们的解x的相对变化和A或者b的相对变化是有像上面那样的关系的，其中k(A)的值就相当于倍率，看到了吗？相当于x变化的界。\n       对condition number来个一句话总结：conditionnumber是一个矩阵（或者它所描述的线性系统）的稳定性或者敏感度的度量，如果一个矩阵的condition number在1附近，那么它就是well-conditioned的，如果远大于1，那么它就是ill-conditioned的，如果一个系统是ill-conditioned的，它的输出结果就不要太相信了。\n       好了，对这么一个东西，已经说了好多了。对了，我们为什么聊到这个的了？回到第一句话：从优化或者数值计算的角度来说，L2范数有助于处理 condition number不好的情况下矩阵求逆很困难的问题。因为目标函数如果是二次的，对于线性回归来说，那实际上是有解析解的，求导并令导数等于零即可得到最优解为：\n\n       然而，如果当我们的样本X的数目比每个样本的维度还要小的时候，矩阵XTX将会不是满秩的，也就是XTX会变得不可逆，所以w*就没办法直接计算出来了。或者更确切地说，将会有无穷多个解（因为我们方程组的个数小于未知数的个数）。也就是说，我们的数据不足以确定一个解，如果我们从所有可行解里随机选一个的话，很可能并不是真正好的解，总而言之，我们过拟合了。\n       但如果加上L2规则项，就变成了下面这种情况，就可以直接求逆了：\n\n       这里面，专业点的描述是：要得到这个解，我们通常并不直接求矩阵的逆，而是通过解线性方程组的方式（例如高斯消元法）来计算。考虑没有规则项的时候，也就是λ=0的情况，如果矩阵XTX的 condition number 很大的话，解线性方程组就会在数值上相当不稳定，而这个规则项的引入则可以改善condition number。\n       另外，如果使用迭代优化的算法，condition number 太大仍然会导致问题：它会拖慢迭代的收敛速度，而规则项从优化的角度来看，实际上是将目标函数变成λ-strongly convex（λ强凸）的了。哎哟哟，这里又出现个λ强凸，啥叫λ强凸呢？\n       当f满足：\n\n       时，我们称f为λ-stronglyconvex函数，其中参数λ>0。当λ=0时退回到普通convex 函数的定义。\n       在直观的说明强凸之前，我们先看看普通的凸是怎样的。假设我们让f在x的地方做一阶泰勒近似（一阶泰勒展开忘了吗？f(x)=f(a)+f'(a)(x-a)+o(||x-a||).）：\n\n       直观来讲，convex 性质是指函数曲线位于该点处的切线，也就是线性近似之上，而 strongly convex 则进一步要求位于该处的一个二次函数上方，也就是说要求函数不要太“平坦”而是可以保证有一定的“向上弯曲”的趋势。专业点说，就是convex 可以保证函数在任意一点都处于它的一阶泰勒函数之上，而strongly convex可以保证函数在任意一点都存在一个非常漂亮的二次下界quadratic lower bound。当然这是一个很强的假设，但是同时也是非常重要的假设。可能还不好理解，那我们画个图来形象的理解下。\n\n       大家一看到上面这个图就全明白了吧。不用我啰嗦了吧。还是啰嗦一下吧。我们取我们的最优解w*的地方。如果我们的函数f(w)，见左图，也就是红色那个函数，都会位于蓝色虚线的那根二次函数之上，这样就算wt和w*离的比较近的时候，f(wt)和f(w*)的值差别还是挺大的，也就是会保证在我们的最优解w*附近的时候，还存在较大的梯度值，这样我们才可以在比较少的迭代次数内达到w*。但对于右图，红色的函数f(w)只约束在一个线性的蓝色虚线之上，假设是如右图的很不幸的情况（非常平坦），那在wt还离我们的最优点w*很远的时候，我们的近似梯度(f(wt)-f(w*))/(wt-w*)就已经非常小了，在wt处的近似梯度∂f/∂w就更小了，这样通过梯度下降wt+1=wt-α*(∂f/∂w)，我们得到的结果就是w的变化非常缓慢，像蜗牛一样，非常缓慢的向我们的最优点w*爬动，那在有限的迭代时间内，它离我们的最优点还是很远。\n       所以仅仅靠convex 性质并不能保证在梯度下降和有限的迭代次数的情况下得到的点w会是一个比较好的全局最小点w*的近似点（插个话，有地方说，实际上让迭代在接近最优的地方停止，也是一种规则化或者提高泛化性能的方法）。正如上面分析的那样，如果f(w)在全局最小点w*周围是非常平坦的情况的话，我们有可能会找到一个很远的点。但如果我们有“强凸”的话，就能对情况做一些控制，我们就可以得到一个更好的近似解。至于有多好嘛，这里面有一个bound，这个 bound 的好坏也要取决于strongly convex性质中的常数α的大小。看到这里，不知道大家学聪明了没有。如果要获得strongly convex怎么做？最简单的就是往里面加入一项(α/2)*||w||2。\n       呃，讲个strongly convex花了那么多的篇幅。实际上，在梯度下降中，目标函数收敛速率的上界实际上是和矩阵XTX的 condition number有关，XTX的 condition number 越小，上界就越小，也就是收敛速度会越快。\n这一个优化说了那么多的东西。还是来个一句话总结吧：L2范数不但可以防止过拟合，还可以让我们的优化求解变得稳定和快速。\n       好了，这里兑现上面的承诺，来直观的聊聊L1和L2的差别，为什么一个让绝对值最小，一个让平方最小，会有那么大的差别呢？我看到的有两种几何上直观的解析：\n1）下降速度：\n       我们知道，L1和L2都是规则化的方式，我们将权值参数以L1或者L2的方式放到代价函数里面去。然后模型就会尝试去最小化这些权值参数。而这个最小化就像一个下坡的过程，L1和L2的差别就在于这个“坡”不同，如下图：L1就是按绝对值函数的“坡”下降的，而L2是按二次函数的“坡”下降。所以实际上在0附近，L1的下降速度比L2的下降速度要快。所以会非常快得降到0。不过我觉得这里解释的不太中肯，当然了也不知道是不是自己理解的问题。\n\n       L1在江湖上人称Lasso，L2人称Ridge。不过这两个名字还挺让人迷糊的，看上面的图片，Lasso的图看起来就像ridge，而ridge的图看起来就像lasso。\n2）模型空间的限制：\n       实际上，对于L1和L2规则化的代价函数来说，我们可以写成以下形式：\n\n       也就是说，我们将模型空间限制在w的一个L1-ball 中。为了便于可视化，我们考虑两维的情况，在(w1, w2)平面上可以画出目标函数的等高线，而约束条件则成为平面上半径为C的一个 norm ball 。等高线与 norm ball 首次相交的地方就是最优解：\n\n       可以看到，L1-ball 与L2-ball 的不同就在于L1在和每个坐标轴相交的地方都有“角”出现，而目标函数的测地线除非位置摆得非常好，大部分时候都会在角的地方相交。注意到在角的位置就会产生稀疏性，例如图中的相交点就有w1=0，而更高维的时候（想象一下三维的L1-ball 是什么样的？）除了角点以外，还有很多边的轮廓也是既有很大的概率成为第一次相交的地方，又会产生稀疏性。\n       相比之下，L2-ball 就没有这样的性质，因为没有角，所以第一次相交的地方出现在具有稀疏性的位置的概率就变得非常小了。这就从直观上来解释了为什么L1-regularization 能产生稀疏性，而L2-regularization 不行的原因了。\n       因此，一句话总结就是：L1会趋向于产生少量的特征，而其他的特征都是0，而L2会选择更多的特征，这些特征都会接近于0。Lasso在特征选择时候非常有用，而Ridge就只是一种规则化而已。\n\n       OK，就聊到这里。下一篇博文我们聊聊核范数和规则化项参数选择的问题。全篇的参考资料也请见下一篇博文，这里不重复列出。谢谢。","tags":["MachineLearning"]},{"title":"SSHIntroduction","url":"/2017/10/10/SSHIntroduction/","content":"什么是SSH呢?\n\nSSH的英文全称是Secure SHell。通过使用SSH，你可以把所有传输的数据进行加密，这样\"中间人\"这种攻击方式就不可能实现了，而且也能够防止DNS和IP欺骗。还有一个额外的好处就是传输的数据是经过压缩的，所以可以加快传输的速度。SSH有很多功能，它既可以代替telnet，又可以为ftp、pop、甚至ppp提供一个安全的\"通道\"。SSH客户端与服务器端通讯时，用户名及口令均进行了加密，有效防止了对口令的窃听。最初SSH是由芬兰的一家公司开发的。但是因为受版权和加密算法的限制，现在很多人都转而使用OpenSSH。OpenSSH是SSH的替代软件，而且是免费的，可以预计将来会有越来越多的人使用它而不是SSH。SSH是由客户端和服务端的软件组成的。SSH安装容易、使用简单，而且比较常见，一般的Unix系统、Linux系统、FreeBSD系统都附带有支持SSH的应用程序包。\n","tags":["Linux","SSH","Hadoop","Hbase"],"categories":["Linux"]},{"title":"通俗解释Hadoop","url":"/2017/10/09/HadoopIntroduction/","content":"原文出处：http://os.51cto.com/art/201305/396145.htm\n\n众所周知，Hadoop是Apache软件基金会管理的开源软件平台，但Hadoop到底是什么呢？简单来说，**Hadoop是在分布式服务器集群上存储海量数据并运行分布式分析应用的一种方法。**\nHadoop被设计成一种非常“鲁棒”的系统，即使某台服务器甚至集群宕机了，运行其上的大数据分析应用也不会中断。此外Hadoop的效率也很高，因为它并不需要你在网络间来回捣腾数据。\n以下是Apache的正式定义：\nApache **Hadoop软件库是一个框架**，允许在集群服务器上使用简单的编程模型对大数据集进行分布式处理。Hadoop被设计成能够从单台服务器扩展到数以千计的服务器，每台服务器都有本地的计算和存储资源。Hadoop的高可用性并不依赖硬件，其代码库自身就能在应用层侦测并处理硬件故障，因此能基于服务器集群提供高可用性的服务。\n如果更深入地分析，我们发现Hadoop还有更加精彩的特性。首先，**Hadoop几乎完全是模块化的**，这意味着你们能用其他软件工具抽换掉Hadoop的模块。这使得Hadoop的架构异常灵活，同时又不牺牲其可靠性和高效率。\nHadoop分布式文件系统（HDFS）\n如果提起Hadoop你的大脑一片空白，那么请牢记住这一点：**Hadoop有两个主要部分：一个数据处理框架和一个分布式数据存储文件系统（HDFS）。**\nHDFS就像Hadoop系统的篮子，你把数据整整齐齐码放在里面等待数据分析大厨出手变成性感的大餐端到CEO的桌面上。当然，你可以在Hadoop进行数据分析，也可以见gHadoop中的数据“抽取转换加载”到其他的工具中进行分析。\n数据处理框架和MapReduce\n顾名思义，**数据处理框架是处理数据的工具**。具体来说Hadoop的数据处理框架是基于Jave的系统——MapReduce，你听到MapReduce的次数会比HDFS还要多，这是因为：\n1.MapReduce是真正完成数据处理任务的工具\n2.MapReduce往往会把它的用户逼疯\n在常规意义上的关系型数据库中，数据通过SQL（结构化查询语言）被找到并分析，非关系型数据库也使用查询语句，只是不局限于SQL而已，于是有了一个新名词NoSQL。\n有一点容易搞混的是，**Hadoop并不是一个真正意义上的数据库：它能存储和抽取数据，但并没有查询语言介入。Hadoop更多是一个数据仓库系统，所以需要MapReduce这样的系统来进行真正的数据处理。**\nMapReduce运行一系列任务，其中每项任务都是单独的Java应用，能够访问数据并抽取有用信息。使用MapReduce而不是查询语言让Hadoop数据分析的功能更加强大和灵活，但同时也导致技术复杂性大幅增加。\n目前有很多工具能够让Hadoop更容易使用，例如Hive，可以将查询语句转换成MapReduce任务。但是MapReduce的复杂性和局限性（单任务批处理）使得Hadoop在更多情况下都被作为数据仓库使用而非数据分析工具。\nHadoop的另外一个独特之处是：所有的功能都是分布式的，而不是传统数据库的集中式系统。\n\n----------\nhadoop是一个平台，是一个适合大数据的分布式存储和计算的平台。\n分布式存储~HDFS\n分布式计算~MapReduce","tags":["DataScience"]},{"title":"《集体智慧编程》读书笔记 - Chapter2","url":"/2017/10/08/ProgrammingCollectiveIntelligenceChapter2/","content":"**评价相似度**\n1.欧几里得距离\n{% codeblock lang:python %}\ndef sim_distance(prefs,person1,person2):\n    si={}\n    for item in prefs[person1]:\n        if item in prefs[person2]:\n            si[item]=1\n\n        if len(si)==0:return 0\n        \n        sum_of_squares=sum([pow(prefs[person1][item]-prefs[person2][item],2) \n        for item in prefs[person1] if item in prefs[person2]])\n    \n        return 1/(1+sqrt(sum_of_squares))\n{% endcodeblock %}\n2.皮尔逊相关系数：判断两组数据与某一直线拟合程度的一种度量。对应的公示比欧几里得距离评价的计算公式要复杂，但是他在数据不是很规范的时候，会倾向于给出更好的结果。\n","tags":["DataScience","集体智慧编程"]},{"title":"Anaconda学习之路 Ⅰ - IPython","url":"/2017/10/08/IPythonIntroduction/","content":"IPython是交互式任务的专用工具，它的特殊命令能够帮助开发人人员更好的理解正在编写的代码。\n**踩坑1：IPython是交互式命令行！魔术函数在控制台里输入，不要像我一样傻不拉几写到.py里，不报错才怪【掩面**\n* <object>?与<object>??\n\t输出<object>的详细描述。??比?更详细（书上这么写的，我自己暂时没看出来...但的确是都能用的）\n> 举个栗子\n> t?\n> Type:        int\n> String form: 1\n> Docstring:  \n> int(x=0) -integer\n> int(x, base=10) -integer\n> \n> Convert a number or string to an integer, or return 0 if no arguments\n> are given.  If x is a number, return x.__int__().  For floating point\n> numbers, this truncates towards zero.\n> \n> If x is not a number or if base is given, then x must be a string,\n> bytes, or bytearray instance representing an integer literal in the\n> given base.  The literal can be preceded by '+' or '-' and be surrounded\n> by whitespace.  The base defaults to 10.  Valid bases are 0 and 2-36.\n> Base 0 means to interpret the base from the string as an integer literal.\n> int('0b100', base=0)\n4\n","tags":["Anaconda","IPython"],"categories":["Anaconda"]},{"title":"记Spyder与Dell那个不得不说的坑","url":"/2017/10/08/AnacondaInstallation/","content":"进来先大喊三声：Anaconda超好用！Anaconda超好用！Anaconda超好用！\n### 这个坑长啥样 ##\n先说电脑配置：Dell Inspiron5548,Win10.\nemmm喊完了我们开始说正事。。。第一次用Anaconda集成好的环境和Spyder写程序实在是非常好的体验，但最后写完了却遇到很严重的问题——不能保存不能打开文件，打开两秒钟就弹出python.exe已停止工作，然后Spyder就退出鸟。。。可怜我第一次辛辛苦苦写了那么久的程序啊喂！完全没保存啊！！！\n调试报错：(python.exe 中)处有未经处理的异常: 0xC0000374: 堆已损坏。\n### 那些徒劳的尝试 ###\n解决这个问题的时候完全是懵逼的，因为不知道是python的问题还是anaconda的问题还是spyder的问题还是windows的问题...所以折腾了很久【怀疑了这么多都没怀疑到Dell!\n**1.升级Spyder **\n\t当时出问题的时候我用的版本是3.1.3,用pip升到了写文章时候最新版3.2.3,果然没有用QwQ\n**2.卸载重装Anaconda**\n\t怎么折腾的按下不表，结论：还是没用。。。\n**3.卸载自己下载的Python**\n\t我最早是自己下载了Python3.5，后来才装了anaconda，担心是二者有冲突。emmmm总之卸载之后看来并不是这样的。。。\n**4.替换ntdll.dll**\n\t对为了这个问题我生平第一次改动了system文件夹...可惜结果完全对不起我的勇气QAQ\n### 怎么爬出来 ###\n百度/google/StackOverflow\n最后跌跌撞撞终于不知道咋地终于找到啊了亲爱的github!\n问题链接在此↓\nhttps://github.com/spyder-ide/spyder/issues/3903\n从头到尾看下来，帖子重点提取如下\n**1.回退Spyder2.3.9，问题解决**：亲测有效！能用了！但是python也变2.7噜\n**2.与Qt冲突**：反正我卸载了Qt5之后没啥卵用...\n**3.与DellBackupAndRecovery冲突**：开始完全没敢卸载，毕竟是系统出场自带的软件诶...但最后发现卸载了也没啥影响的样子【笑\n### 总结 ###\n亲测有用的解决方案如下：\n1.退回Spyder2.3.9，Python会默认退回2.7，能不能用Python3我还没试唔...\n2.卸载Dell Backup and Recovery\n\n最后为我冤死的Qt5掬一把同情泪_(:з」∠)_\n总之问题解决！希望可以遇到同样问题的人一点参考~\n","tags":["Anaconda"],"categories":["Anaconda"]},{"title":"Anaconda清华镜像站","url":"/2017/10/07/AnacondaMirror/","content":"### Anaconda清华镜像站 ##\nhttps://mirrors.tuna.tsinghua.edu.cn/anaconda/archive/\n","tags":["Anaconda","Python","DataScience"]},{"title":"Hexo","url":"/2017/10/04/Hexo/","content":"this is the first test.","tags":["Hexo"]},{"title":"Hexo备忘录","url":"/2017/10/04/Hexo备忘录/","content":"### 关于Hexo的一切 #","tags":["Hexo"],"categories":["Hexo"]}]